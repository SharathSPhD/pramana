model:
  name: "unsloth/Llama-3.2-3B-Instruct"
  revision: "main"

lora:
  rank: 32
  alpha: 32
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

training:
  learning_rate: 2.0e-5
  batch_size: 2
  gradient_accumulation_steps: 4
  epochs: 3
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0

data:
  max_length: 4096

evaluation:
  tier1_threshold: 0.9
  tier2_threshold: 0.7
