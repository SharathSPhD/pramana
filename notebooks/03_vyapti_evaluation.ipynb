{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vyapti Probe Benchmark — Interactive Evaluation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SharathSPhD/pramana/blob/main/notebooks/03_vyapti_evaluation.ipynb)\n",
    "\n",
    "This notebook provides an interactive interface for running the **Vyapti Probe Benchmark** — a 100-problem evaluation suite testing whether LLMs can distinguish statistical regularity from *vyapti* (invariable concomitance).\n",
    "\n",
    "**What's inside:**\n",
    "- Browse all 100 problems (50 probes + 50 controls) across 5 Hetvabhasa categories\n",
    "- Run evaluation with local models (Ollama/llama.cpp) or Hugging Face models\n",
    "- View 5-tier scoring results (outcome, structure, vyapti, Z3, hetvabhasa)\n",
    "- Statistical analysis with bootstrap confidence intervals\n",
    "- Visualizations: probe-vs-control heatmaps, failure distributions, CI plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title 1. Environment Setup\n",
    "import os, sys, subprocess, importlib\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules or os.path.exists('/content')\n",
    "\n",
    "if IN_COLAB:\n",
    "    REPO_URL = 'https://github.com/SharathSPhD/pramana.git'\n",
    "    REPO_DIR = '/content/pramana'\n",
    "    if not os.path.exists(REPO_DIR):\n",
    "        print('Cloning repository...')\n",
    "        subprocess.run(['git', 'clone', '--depth', '1', REPO_URL, REPO_DIR],\n",
    "                       check=True, capture_output=True)\n",
    "        print('Done.')\n",
    "    os.chdir(REPO_DIR)\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-r', 'notebooks/requirements.txt'],\n",
    "                   check=True, capture_output=True)\n",
    "    sys.path.insert(0, os.path.join(REPO_DIR, 'src'))\n",
    "else:\n",
    "    # Local: assume running from repo root or notebooks/\n",
    "    root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    if os.path.exists(os.path.join(root, 'data', 'vyapti_probe')):\n",
    "        os.chdir(root)\n",
    "    sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "# GPU detection\n",
    "try:\n",
    "    import torch\n",
    "    GPU_AVAILABLE = torch.cuda.is_available()\n",
    "    if GPU_AVAILABLE:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        print(f'GPU detected: {gpu_name}')\n",
    "    else:\n",
    "        print('No GPU detected. CPU mode.')\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print('PyTorch not available. Using CPU-only backends.')\n",
    "\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title 2. Load Benchmark Data\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('data/vyapti_probe')\n",
    "\n",
    "with open(DATA_DIR / 'problems.json') as f:\n",
    "    problems = json.load(f)\n",
    "with open(DATA_DIR / 'solutions.json') as f:\n",
    "    solutions_list = json.load(f)\n",
    "    solutions = {s['id']: s for s in solutions_list}\n",
    "\n",
    "probes = [p for p in problems if p['type'] == 'probe']\n",
    "controls = [p for p in problems if p['type'] == 'control']\n",
    "\n",
    "print(f'Loaded {len(problems)} problems ({len(probes)} probes + {len(controls)} controls)')\n",
    "print(f'Loaded {len(solutions)} solutions')\n",
    "print()\n",
    "\n",
    "# Summary by category\n",
    "from collections import Counter\n",
    "cat_counts = Counter(p['category'] for p in probes)\n",
    "print('Probes by category:')\n",
    "for cat, count in sorted(cat_counts.items()):\n",
    "    print(f'  {cat}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Browse Problems\n",
    "\n",
    "Explore the benchmark problems by category and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title 3. Problem Browser\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "#@markdown **Select category and problem:**\n",
    "category = 'savyabhichara' #@param ['savyabhichara', 'viruddha', 'prakaranasama', 'sadhyasama', 'kalatita']\n",
    "problem_type = 'probe' #@param ['probe', 'control']\n",
    "problem_index = 0 #@param {type: \"slider\", min: 0, max: 14}\n",
    "\n",
    "filtered = [p for p in problems if p['category'] == category and p['type'] == problem_type]\n",
    "if problem_index >= len(filtered):\n",
    "    problem_index = len(filtered) - 1\n",
    "\n",
    "p = filtered[problem_index]\n",
    "sol = solutions.get(p['id'], {})\n",
    "\n",
    "md = f\"\"\"### {p['id']} | {p['logic_type']} | Difficulty: {p['difficulty']}\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "{p['problem_text']}\n",
    "\n",
    "---\n",
    "\n",
    "**Correct Answer:** {p['correct_answer']}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if p['type'] == 'probe':\n",
    "    md += f\"\"\"**Trap Answer:** {p['trap_answer']}\n",
    "\n",
    "**Vyapti Under Test:** {p['vyapti_under_test']}\n",
    "\n",
    "**Why It Fails:** {p['why_it_fails']}\n",
    "\"\"\"\n",
    "\n",
    "if sol:\n",
    "    md += f\"\"\"\\n---\\n\\n**Solution Details:**\n",
    "- Vyapti Status: {sol.get('vyapti_status', 'N/A')}\n",
    "- Counterexample: {sol.get('counterexample', 'N/A')}\n",
    "- Hetvabhasa: {sol.get('hetvabhasa_type', 'N/A')}\n",
    "- Z3 Note: {sol.get('z3_verification', 'N/A')}\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Evaluation\n",
    "\n",
    "Evaluate a model on the full benchmark or a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title 4a. Configure Model Backend\n",
    "\n",
    "backend = 'simulation' #@param ['simulation', 'ollama', 'transformers']\n",
    "model_name = 'simulated_base' #@param {type: 'string'}\n",
    "\n",
    "#@markdown **Generation parameters:**\n",
    "max_new_tokens = 2048 #@param {type: 'integer'}\n",
    "temperature = 0.5 #@param {type: 'number'}\n",
    "top_p = 0.75 #@param {type: 'number'}\n",
    "top_k = 5 #@param {type: 'integer'}\n",
    "\n",
    "def get_model_fn(backend, model_name):\n",
    "    if backend == 'simulation':\n",
    "        import random\n",
    "        rng = random.Random(42)\n",
    "        def sim_fn(prompt):\n",
    "            if rng.random() < 0.4:\n",
    "                return 'No, the conclusion cannot be drawn. There is a counterexample that falsifies the universal.'\n",
    "            return 'Yes, based on the pattern observed in the premises, we can conclude the answer is affirmative.'\n",
    "        return sim_fn\n",
    "    elif backend == 'ollama':\n",
    "        import requests\n",
    "        def ollama_fn(prompt):\n",
    "            resp = requests.post('http://localhost:11434/api/generate', json={\n",
    "                'model': model_name,\n",
    "                'prompt': prompt,\n",
    "                'options': {'num_predict': max_new_tokens, 'temperature': temperature, 'top_p': top_p, 'top_k': top_k},\n",
    "                'stream': False,\n",
    "            }, timeout=300)\n",
    "            return resp.json().get('response', '')\n",
    "        return ollama_fn\n",
    "    elif backend == 'transformers':\n",
    "        import torch\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map='auto')\n",
    "        def hf_fn(prompt):\n",
    "            inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=max_new_tokens,\n",
    "                                         temperature=temperature, top_p=top_p, top_k=top_k, do_sample=True)\n",
    "            return tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        return hf_fn\n",
    "    else:\n",
    "        raise ValueError(f'Unknown backend: {backend}')\n",
    "\n",
    "model_fn = get_model_fn(backend, model_name)\n",
    "print(f'Backend: {backend}, Model: {model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title 4b. Run Evaluation\n",
    "from pramana.benchmarks.vyapti_runner import VyaptiEvaluationRunner\n",
    "\n",
    "config = {\n",
    "    'benchmark_path': 'data/vyapti_probe/problems.json',\n",
    "    'solutions_path': 'data/vyapti_probe/solutions.json',\n",
    "}\n",
    "\n",
    "runner = VyaptiEvaluationRunner(config)\n",
    "\n",
    "#@markdown **Evaluate subset or all?**\n",
    "evaluate_all = True #@param {type: 'boolean'}\n",
    "subset_size = 10 #@param {type: 'integer'}\n",
    "\n",
    "if evaluate_all:\n",
    "    results = runner.evaluate_model(model_name, model_fn)\n",
    "else:\n",
    "    subset_ids = [p['id'] for p in problems[:subset_size]]\n",
    "    results = runner.evaluate_model(model_name, model_fn, problem_ids=subset_ids)\n",
    "\n",
    "# Summary\n",
    "correct = sum(1 for r in results if r.final_answer_correct)\n",
    "total = len(results)\n",
    "print(f'\\n=== Results: {model_name} ===')\n",
    "print(f'Accuracy: {correct}/{total} ({correct/total:.1%})')\n",
    "\n",
    "probe_results = [r for r in results if r.problem_type == 'probe']\n",
    "control_results = [r for r in results if r.problem_type == 'control']\n",
    "probe_correct = sum(1 for r in probe_results if r.final_answer_correct)\n",
    "control_correct = sum(1 for r in control_results if r.final_answer_correct)\n",
    "print(f'Probe accuracy: {probe_correct}/{len(probe_results)} ({probe_correct/len(probe_results):.1%})')\n",
    "print(f'Control accuracy: {control_correct}/{len(control_results)} ({control_correct/len(control_results):.1%})')\n",
    "print(f'Vyapti gap: {(control_correct/len(control_results) - probe_correct/len(probe_results)):.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title 5a. Category-wise Breakdown\n",
    "from dataclasses import asdict\n",
    "\n",
    "CATEGORIES = ['savyabhichara', 'viruddha', 'prakaranasama', 'sadhyasama', 'kalatita']\n",
    "\n",
    "print(f'\\n{\"Category\":<20} {\"Probe\":>8} {\"Control\":>8} {\"Gap\":>8}')\n",
    "print('-' * 48)\n",
    "\n",
    "for cat in CATEGORIES:\n",
    "    cat_probes = [r for r in results if r.category == cat and r.problem_type == 'probe']\n",
    "    cat_controls = [r for r in results if r.category == cat and r.problem_type == 'control']\n",
    "    pc = sum(1 for r in cat_probes if r.final_answer_correct)\n",
    "    cc = sum(1 for r in cat_controls if r.final_answer_correct)\n",
    "    pt = len(cat_probes) or 1\n",
    "    ct = len(cat_controls) or 1\n",
    "    gap = cc/ct - pc/pt\n",
    "    print(f'{cat:<20} {pc}/{pt:>3}     {cc}/{ct:>3}     {gap:>+.0%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title 5b. Hetvabhasa Failure Distribution\n",
    "from collections import Counter\n",
    "\n",
    "failures = [r for r in results if not r.final_answer_correct]\n",
    "h_dist = Counter(r.hetvabhasa_classification for r in failures)\n",
    "\n",
    "print(f'\\nTotal failures: {len(failures)}')\n",
    "print(f'\\n{\"Hetvabhasa Type\":<20} {\"Count\":>6} {\"Pct\":>6}')\n",
    "print('-' * 34)\n",
    "for htype, count in h_dist.most_common():\n",
    "    print(f'{htype:<20} {count:>6} {count/len(failures):>6.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title 5c. Visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Probe vs Control by category\n",
    "    cats = ['SAV', 'VIR', 'PRA', 'SAD', 'KAL']\n",
    "    probe_accs = []\n",
    "    control_accs = []\n",
    "    for cat in CATEGORIES:\n",
    "        cp = [r for r in results if r.category == cat and r.problem_type == 'probe']\n",
    "        cc = [r for r in results if r.category == cat and r.problem_type == 'control']\n",
    "        probe_accs.append(sum(1 for r in cp if r.final_answer_correct) / max(len(cp), 1))\n",
    "        control_accs.append(sum(1 for r in cc if r.final_answer_correct) / max(len(cc), 1))\n",
    "\n",
    "    x = np.arange(len(cats))\n",
    "    w = 0.35\n",
    "    ax1.bar(x - w/2, probe_accs, w, label='Probes', color='#e74c3c', alpha=0.8)\n",
    "    ax1.bar(x + w/2, control_accs, w, label='Controls', color='#2ecc71', alpha=0.8)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(cats)\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title(f'Probe vs Control Accuracy ({model_name})')\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(0, 1)\n",
    "\n",
    "    # Hetvabhasa distribution\n",
    "    if h_dist:\n",
    "        labels = list(h_dist.keys())\n",
    "        values = list(h_dist.values())\n",
    "        colors = plt.cm.Set2(range(len(labels)))\n",
    "        ax2.pie(values, labels=labels, autopct='%1.0f%%', colors=colors)\n",
    "        ax2.set_title('Failure Type Distribution')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No failures!', ha='center', va='center', fontsize=14)\n",
    "        ax2.set_title('Failure Type Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print('matplotlib not available. Install with: pip install matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Pre-computed Results\n",
    "\n",
    "If you have previously run the full evaluation campaign, load and analyze those results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title 6. Load and Compare Pre-computed Results\n",
    "results_dir = Path('data/vyapti_probe/results')\n",
    "\n",
    "if (results_dir / 'summary.json').exists():\n",
    "    with open(results_dir / 'summary.json') as f:\n",
    "        summary = json.load(f)\n",
    "    \n",
    "    print('Pre-computed results found!')\n",
    "    print(f'\\n{\"Model\":<28} {\"Accuracy\":>10} {\"Probe\":>8} {\"Control\":>8} {\"Gap\":>8}')\n",
    "    print('-' * 66)\n",
    "    for model, data in summary.items():\n",
    "        gap = data['control_accuracy'] - data['probe_accuracy']\n",
    "        print(f'{model:<28} {data[\"accuracy\"]:>9.1%} {data[\"probe_accuracy\"]:>7.1%} {data[\"control_accuracy\"]:>7.1%} {gap:>+7.1%}')\n",
    "    \n",
    "    # Load comparisons if available\n",
    "    if (results_dir / 'comparisons.json').exists():\n",
    "        with open(results_dir / 'comparisons.json') as f:\n",
    "            comparisons = json.load(f)\n",
    "        print('\\n--- Statistical Comparisons ---')\n",
    "        for c in comparisons:\n",
    "            sig = 'SIGNIFICANT' if c['significant'] else 'not significant'\n",
    "            print(f\"\\n{c['name']}\")\n",
    "            print(f\"  Difference: {c['difference']:+.3f} (95% CI: [{c['ci_lower']:+.3f}, {c['ci_upper']:+.3f}])\")\n",
    "            print(f\"  {sig} (p = {c['p_value_approx']:.4f})\")\n",
    "else:\n",
    "    print('No pre-computed results found.')\n",
    "    print('Run the evaluation first (Section 4) or execute:')\n",
    "    print('  python scripts/run_vyapti_evaluation.py --simulate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook provides the interactive interface for the **Vyapti Probe Benchmark**.\n",
    "\n",
    "**Key scripts:**\n",
    "- `scripts/run_vyapti_evaluation.py` — Full evaluation campaign (real or simulated)\n",
    "- `scripts/run_vyapti_analysis.py` — Statistical analysis and visualization\n",
    "- `scripts/publish_vyapti_dataset.py` — Publish to Hugging Face Hub\n",
    "\n",
    "**Key files:**\n",
    "- `data/vyapti_probe/problems.json` — 100 benchmark problems\n",
    "- `data/vyapti_probe/solutions.json` — Ground truth solutions\n",
    "- `data/vyapti_probe/z3_encodings/` — Formal Z3 verification modules\n",
    "- `docs/paper_vyapti/what_nyaya_reveals.md` — Draft diagnosis paper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
