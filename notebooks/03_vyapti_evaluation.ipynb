{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vyapti Probe Benchmark — Interactive Evaluation\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SharathSPhD/pramana/blob/main/notebooks/03_vyapti_evaluation.ipynb)\n",
        "\n",
        "This notebook provides an interactive interface for running the **Vyapti Probe Benchmark** — a 100-problem evaluation suite testing whether LLMs can distinguish statistical regularity from *vyapti* (invariable concomitance).\n",
        "\n",
        "**What's inside:**\n",
        "- Browse all 100 problems (50 probes + 50 controls) across 5 Hetvabhasa categories\n",
        "- Run evaluation with local models (Ollama/llama.cpp) or Hugging Face models\n",
        "- View 5-tier scoring results (outcome, structure, vyapti, Z3, hetvabhasa)\n",
        "- Statistical analysis with bootstrap confidence intervals\n",
        "- Visualizations: probe-vs-control heatmaps, failure distributions, CI plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 1. Environment Setup\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = \"google.colab\" in sys.modules or os.path.exists(\"/content\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    REPO_URL = \"https://github.com/SharathSPhD/pramana.git\"\n",
        "    REPO_DIR = \"/content/pramana\"\n",
        "    if not os.path.exists(REPO_DIR):\n",
        "        print(\"Cloning repository...\")\n",
        "        subprocess.run(\n",
        "            [\"git\", \"clone\", \"--depth\", \"1\", REPO_URL, REPO_DIR],\n",
        "            check=True,\n",
        "            capture_output=True,\n",
        "        )\n",
        "        print(\"Done.\")\n",
        "    os.chdir(REPO_DIR)\n",
        "    subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", \"notebooks/requirements.txt\"],\n",
        "        check=True,\n",
        "        capture_output=True,\n",
        "    )\n",
        "else:\n",
        "    # Local: support running from repo root or notebooks/\n",
        "    cwd = Path.cwd().resolve()\n",
        "    if (cwd / \"data\" / \"vyapti_probe\").exists():\n",
        "        os.chdir(cwd)\n",
        "    elif (cwd.parent / \"data\" / \"vyapti_probe\").exists():\n",
        "        os.chdir(cwd.parent)\n",
        "\n",
        "PROJECT_ROOT = Path.cwd().resolve()\n",
        "DATA_DIR = PROJECT_ROOT / \"data\" / \"vyapti_probe\"\n",
        "RESULTS_REAL_DIR = DATA_DIR / \"results_real\"\n",
        "\n",
        "if not DATA_DIR.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not find benchmark data directory at {DATA_DIR}. \"\n",
        "        \"Run this notebook from the pramana repo root (or Colab setup).\"\n",
        "    )\n",
        "\n",
        "# Add project paths for imports\n",
        "sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
        "sys.path.insert(0, str(PROJECT_ROOT / \"notebooks\"))\n",
        "\n",
        "# GPU detection\n",
        "try:\n",
        "    import torch\n",
        "\n",
        "    GPU_AVAILABLE = torch.cuda.is_available()\n",
        "    if GPU_AVAILABLE:\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"GPU detected: {gpu_name}\")\n",
        "    else:\n",
        "        print(\"No GPU detected. CPU mode.\")\n",
        "except ImportError:\n",
        "    GPU_AVAILABLE = False\n",
        "    print(\"PyTorch not available. Using CPU-only backends.\")\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Results path: {RESULTS_REAL_DIR}\")\n",
        "print(\"Setup complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 2. Load Benchmark Data\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "with open(DATA_DIR / \"problems.json\") as f:\n",
        "    problems = json.load(f)\n",
        "\n",
        "with open(DATA_DIR / \"solutions.json\") as f:\n",
        "    solutions_list = json.load(f)\n",
        "\n",
        "solutions = {s[\"id\"]: s for s in solutions_list}\n",
        "problems_by_id = {p[\"id\"]: p for p in problems}\n",
        "\n",
        "probes = [p for p in problems if p[\"type\"] == \"probe\"]\n",
        "controls = [p for p in problems if p[\"type\"] == \"control\"]\n",
        "\n",
        "print(f\"Loaded {len(problems)} problems ({len(probes)} probes + {len(controls)} controls)\")\n",
        "print(f\"Loaded {len(solutions)} solutions\")\n",
        "print()\n",
        "\n",
        "cat_counts = Counter(p[\"category\"] for p in probes)\n",
        "print(\"Probes by category:\")\n",
        "for cat, count in sorted(cat_counts.items()):\n",
        "    print(f\"  {cat}: {count}\")\n",
        "\n",
        "missing_solutions = [p[\"id\"] for p in problems if p[\"id\"] not in solutions]\n",
        "if missing_solutions:\n",
        "    print(f\"\\nWARNING: {len(missing_solutions)} problems missing solutions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Browse Problems\n",
        "\n",
        "Explore benchmark content by category and type (independent of model execution)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 3. Problem Browser\n",
        "from IPython.display import display, Markdown, HTML\n",
        "\n",
        "#@markdown **Select category and problem:**\n",
        "category = 'savyabhichara' #@param ['savyabhichara', 'viruddha', 'prakaranasama', 'sadhyasama', 'kalatita']\n",
        "problem_type = 'probe' #@param ['probe', 'control']\n",
        "problem_index = 0 #@param {type: \"slider\", min: 0, max: 14}\n",
        "\n",
        "filtered = [p for p in problems if p['category'] == category and p['type'] == problem_type]\n",
        "if problem_index >= len(filtered):\n",
        "    problem_index = len(filtered) - 1\n",
        "\n",
        "p = filtered[problem_index]\n",
        "sol = solutions.get(p['id'], {})\n",
        "\n",
        "md = f\"\"\"### {p['id']} | {p['logic_type']} | Difficulty: {p['difficulty']}\n",
        "\n",
        "**Problem:**\n",
        "\n",
        "{p['problem_text']}\n",
        "\n",
        "---\n",
        "\n",
        "**Correct Answer:** {p['correct_answer']}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "if p['type'] == 'probe':\n",
        "    md += f\"\"\"**Trap Answer:** {p['trap_answer']}\n",
        "\n",
        "**Vyapti Under Test:** {p['vyapti_under_test']}\n",
        "\n",
        "**Why It Fails:** {p['why_it_fails']}\n",
        "\"\"\"\n",
        "\n",
        "if sol:\n",
        "    md += f\"\"\"\\n---\\n\\n**Solution Details:**\n",
        "- Vyapti Status: {sol.get('vyapti_status', 'N/A')}\n",
        "- Counterexample: {sol.get('counterexample', 'N/A')}\n",
        "- Hetvabhasa: {sol.get('hetvabhasa_type', 'N/A')}\n",
        "- Z3 Note: {sol.get('z3_verification', 'N/A')}\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(md))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Pre-computed Evaluation Results (Primary Mode)\n",
        "\n",
        "This is the default workflow. It reads real evaluation outputs from `data/vyapti_probe/results_real/` and does **not** require loading any model backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 4a. Load Results Artifacts\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "RESULTS_DIR = RESULTS_REAL_DIR\n",
        "summary_path = RESULTS_DIR / \"summary.json\"\n",
        "comparisons_path = RESULTS_DIR / \"comparisons.json\"\n",
        "taxonomy_path = RESULTS_DIR / \"taxonomy_coverage.json\"\n",
        "report_path = RESULTS_DIR / \"report.md\"\n",
        "\n",
        "if not summary_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Missing {summary_path}. Run scripts/run_vyapti_real.py and scripts/run_vyapti_analysis.py first.\"\n",
        "    )\n",
        "\n",
        "with open(summary_path) as f:\n",
        "    summary = json.load(f)\n",
        "\n",
        "comparisons = []\n",
        "if comparisons_path.exists():\n",
        "    with open(comparisons_path) as f:\n",
        "        comparisons = json.load(f)\n",
        "\n",
        "taxonomy = None\n",
        "if taxonomy_path.exists():\n",
        "    with open(taxonomy_path) as f:\n",
        "        taxonomy = json.load(f)\n",
        "\n",
        "MODEL_RESULTS = defaultdict(dict)\n",
        "PER_MODEL_COUNTS = {}\n",
        "\n",
        "for model in summary.keys():\n",
        "    model_dir = RESULTS_DIR / model.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
        "    if not model_dir.exists():\n",
        "        PER_MODEL_COUNTS[model] = 0\n",
        "        continue\n",
        "\n",
        "    for fp in sorted(model_dir.glob(\"*.json\")):\n",
        "        if fp.name == \"summary.json\":\n",
        "            continue\n",
        "        with open(fp) as f:\n",
        "            row = json.load(f)\n",
        "        pid = row.get(\"problem_id\", fp.stem)\n",
        "        MODEL_RESULTS[model][pid] = row\n",
        "\n",
        "    PER_MODEL_COUNTS[model] = len(MODEL_RESULTS[model])\n",
        "\n",
        "AVAILABLE_MODELS = list(summary.keys())\n",
        "\n",
        "print(f\"Loaded summary for {len(summary)} models from {RESULTS_DIR}\")\n",
        "for model in AVAILABLE_MODELS:\n",
        "    count = PER_MODEL_COUNTS.get(model, 0)\n",
        "    flag = \"OK\" if count >= 100 else \"WARN\"\n",
        "    print(f\"  [{flag}] {model}: {count} per-problem records\")\n",
        "\n",
        "if comparisons:\n",
        "    print(f\"Loaded {len(comparisons)} statistical comparisons\")\n",
        "if taxonomy:\n",
        "    print(\"Loaded taxonomy coverage metrics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 4b. Overview Table (All Models)\n",
        "rows = []\n",
        "for model, data in summary.items():\n",
        "    rows.append(\n",
        "        {\n",
        "            \"model\": model,\n",
        "            \"accuracy\": data.get(\"accuracy\", 0.0),\n",
        "            \"probe_accuracy\": data.get(\"probe_accuracy\", 0.0),\n",
        "            \"control_accuracy\": data.get(\"control_accuracy\", 0.0),\n",
        "            \"gap_control_minus_probe\": data.get(\"control_accuracy\", 0.0) - data.get(\"probe_accuracy\", 0.0),\n",
        "            \"records_loaded\": PER_MODEL_COUNTS.get(model, 0),\n",
        "        }\n",
        "    )\n",
        "\n",
        "rows = sorted(rows, key=lambda r: r[\"accuracy\"], reverse=True)\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "    from IPython.display import display\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    display(df.style.format({\n",
        "        \"accuracy\": \"{:.1%}\",\n",
        "        \"probe_accuracy\": \"{:.1%}\",\n",
        "        \"control_accuracy\": \"{:.1%}\",\n",
        "        \"gap_control_minus_probe\": \"{:+.1%}\",\n",
        "    }))\n",
        "except Exception:\n",
        "    print(f'\\n{\"Model\":<28} {\"Accuracy\":>10} {\"Probe\":>8} {\"Control\":>8} {\"Gap\":>8} {\"Rows\":>6}')\n",
        "    print('-' * 80)\n",
        "    for row in rows:\n",
        "        print(\n",
        "            f\"{row['model']:<28} \"\n",
        "            f\"{row['accuracy']:>9.1%} \"\n",
        "            f\"{row['probe_accuracy']:>7.1%} \"\n",
        "            f\"{row['control_accuracy']:>7.1%} \"\n",
        "            f\"{row['gap_control_minus_probe']:>+7.1%} \"\n",
        "            f\"{row['records_loaded']:>6}\"\n",
        "        )\n",
        "\n",
        "print(\"\\nPrimary mode note: this analysis uses pre-computed JSON artifacts only; no model is loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Browse Model Responses (Input/Output Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 5a. Response Browser\n",
        "from IPython.display import Markdown, display\n",
        "import html\n",
        "\n",
        "#@markdown Select a model and problem id to inspect the saved model response and tier scores.\n",
        "selected_model = \"deepseek_8b_base\" #@param [\"llama_3b_base\", \"deepseek_8b_base\", \"stage0_pramana\", \"stage1_pramana\", \"base_with_cot\", \"base_with_nyaya_template\"]\n",
        "selected_problem_id = \"SAV-01\" #@param {type:\"string\"}\n",
        "\n",
        "model_records = MODEL_RESULTS.get(selected_model, {})\n",
        "record = model_records.get(selected_problem_id.strip())\n",
        "problem = problems_by_id.get(selected_problem_id.strip())\n",
        "solution = solutions.get(selected_problem_id.strip(), {})\n",
        "\n",
        "if problem is None:\n",
        "    print(f\"Problem '{selected_problem_id}' not found in benchmark.\")\n",
        "elif record is None:\n",
        "    print(f\"No saved record found for model='{selected_model}', problem='{selected_problem_id}'.\")\n",
        "    sample_ids = sorted(list(model_records.keys()))[:15]\n",
        "    if sample_ids:\n",
        "        print(f\"Sample available ids for {selected_model}: {', '.join(sample_ids)}\")\n",
        "else:\n",
        "    tiers = record.get(\"tiers\", [])\n",
        "    tier_lines = []\n",
        "    for t in tiers:\n",
        "        status = \"PASS\" if t.get(\"passed\", False) else \"FAIL\"\n",
        "        tier_lines.append(f\"- Tier {t.get('tier', '?')} ({t.get('name', 'unknown')}): {status} | score={t.get('score', 0):.2f}\")\n",
        "\n",
        "    md = f\"\"\"\n",
        "### {selected_problem_id} | {problem.get('category', 'N/A')} | {problem.get('type', 'N/A')}\n",
        "\n",
        "**Model:** `{selected_model}`  \n",
        "**Final answer correct:** `{record.get('final_answer_correct', False)}`  \n",
        "**Hetvabhasa classification:** `{record.get('hetvabhasa_classification', 'N/A')}`\n",
        "\n",
        "**Problem text**\n",
        "\n",
        "{problem.get('problem_text', '')}\n",
        "\n",
        "---\n",
        "\n",
        "**Ground-truth answer**\n",
        "\n",
        "{solution.get('answer', problem.get('correct_answer', 'N/A'))}\n",
        "\n",
        "---\n",
        "\n",
        "**Tier breakdown**\n",
        "\n",
        "{chr(10).join(tier_lines)}\n",
        "\n",
        "---\n",
        "\n",
        "**Model raw response**\n",
        "\n",
        "```text\n",
        "{record.get('raw_response', '')}\n",
        "```\n",
        "\"\"\"\n",
        "    display(Markdown(md))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 6. Statistical Analysis (from saved artifacts)\n",
        "CATEGORIES = [\"savyabhichara\", \"viruddha\", \"prakaranasama\", \"sadhyasama\", \"kalatita\"]\n",
        "\n",
        "print(\"=== C1-C4 Comparisons ===\")\n",
        "if comparisons:\n",
        "    for c in comparisons:\n",
        "        print(f\"\\n{c.get('name', 'Unnamed comparison')}\")\n",
        "        print(f\"  {c.get('description', '')}\")\n",
        "        print(f\"  Difference: {c.get('difference', 0):+.3f}\")\n",
        "        if c.get(\"p_value_approx\", -1) >= 0:\n",
        "            print(f\"  95% CI: [{c.get('ci_lower', 0):+.3f}, {c.get('ci_upper', 0):+.3f}]\")\n",
        "            print(f\"  p-value: {c.get('p_value_approx', 1.0):.4f}\")\n",
        "            print(f\"  Significant: {c.get('significant', False)}\")\n",
        "        else:\n",
        "            print(\"  Descriptive metric (no inferential p-value)\")\n",
        "        print(f\"  N: {c.get('n_samples', 0)}\")\n",
        "else:\n",
        "    print(\"No comparisons.json loaded.\")\n",
        "\n",
        "print(\"\\n=== Hetvabhasa Taxonomy ===\")\n",
        "if taxonomy:\n",
        "    print(f\"Total failures: {taxonomy.get('total_failures', 0)}\")\n",
        "    print(f\"Coverage: {taxonomy.get('coverage_pct', 0):.1f}%\")\n",
        "    print(f\"Assisted predictive accuracy: {taxonomy.get('predictive_accuracy', 0):.1f}%\")\n",
        "    print(f\"Strict predictive accuracy: {taxonomy.get('strict_predictive_accuracy', 0):.1f}%\")\n",
        "    print(f\"Fallback classifications: {taxonomy.get('fallback_count', 0)}\")\n",
        "    print(\"\\nDistribution:\")\n",
        "    dist = taxonomy.get(\"distribution\", {})\n",
        "    total_failures = max(1, taxonomy.get(\"total_failures\", 1))\n",
        "    for htype, count in sorted(dist.items(), key=lambda kv: kv[1], reverse=True):\n",
        "        print(f\"  {htype:<16} {count:>4} ({count/total_failures:>5.1%})\")\n",
        "else:\n",
        "    print(\"No taxonomy_coverage.json loaded.\")\n",
        "\n",
        "print(\"\\n=== Category-wise Probe/Control by Model ===\")\n",
        "for model, data in summary.items():\n",
        "    print(f\"\\n{model}\")\n",
        "    print(f\"  {'Category':<16} {'Probe':>9} {'Control':>9} {'Gap':>8}\")\n",
        "    for cat in CATEGORIES:\n",
        "        cat_data = data.get(\"by_category\", {}).get(cat, {})\n",
        "        pc = cat_data.get(\"probe_correct\", 0)\n",
        "        pt = max(1, cat_data.get(\"probe_total\", 0))\n",
        "        cc = cat_data.get(\"control_correct\", 0)\n",
        "        ct = max(1, cat_data.get(\"control_total\", 0))\n",
        "        gap = (cc / ct) - (pc / pt)\n",
        "        print(f\"  {cat:<16} {pc:>2}/{pt:<5} {cc:>2}/{ct:<5} {gap:>+7.1%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 7. Visualization (from pre-computed data)\n",
        "#@markdown Choose a model for category-level probe/control visualization.\n",
        "viz_model = \"deepseek_8b_base\" #@param [\"llama_3b_base\", \"deepseek_8b_base\", \"stage0_pramana\", \"stage1_pramana\", \"base_with_cot\", \"base_with_nyaya_template\"]\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    ax1, ax2, ax3 = axes\n",
        "\n",
        "    # Panel 1: Probe vs control by category for selected model\n",
        "    cat_short = {\n",
        "        \"savyabhichara\": \"SAV\",\n",
        "        \"viruddha\": \"VIR\",\n",
        "        \"prakaranasama\": \"PRA\",\n",
        "        \"sadhyasama\": \"SAD\",\n",
        "        \"kalatita\": \"KAL\",\n",
        "    }\n",
        "    model_data = summary.get(viz_model, {})\n",
        "    by_cat = model_data.get(\"by_category\", {})\n",
        "\n",
        "    cats = [\"savyabhichara\", \"viruddha\", \"prakaranasama\", \"sadhyasama\", \"kalatita\"]\n",
        "    probe_accs = []\n",
        "    control_accs = []\n",
        "    labels = []\n",
        "\n",
        "    for cat in cats:\n",
        "        c = by_cat.get(cat, {})\n",
        "        pt = max(1, c.get(\"probe_total\", 0))\n",
        "        ct = max(1, c.get(\"control_total\", 0))\n",
        "        probe_accs.append(c.get(\"probe_correct\", 0) / pt)\n",
        "        control_accs.append(c.get(\"control_correct\", 0) / ct)\n",
        "        labels.append(cat_short[cat])\n",
        "\n",
        "    x = np.arange(len(labels))\n",
        "    w = 0.36\n",
        "    ax1.bar(x - w / 2, probe_accs, w, label=\"Probes\", alpha=0.85)\n",
        "    ax1.bar(x + w / 2, control_accs, w, label=\"Controls\", alpha=0.85)\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(labels)\n",
        "    ax1.set_ylim(0, 1)\n",
        "    ax1.set_ylabel(\"Accuracy\")\n",
        "    ax1.set_title(f\"{viz_model}: Probe vs Control by category\")\n",
        "    ax1.legend()\n",
        "\n",
        "    # Panel 2: Probe and control accuracy across all models\n",
        "    models = list(summary.keys())\n",
        "    probe_all = [summary[m].get(\"probe_accuracy\", 0.0) for m in models]\n",
        "    control_all = [summary[m].get(\"control_accuracy\", 0.0) for m in models]\n",
        "    x2 = np.arange(len(models))\n",
        "\n",
        "    ax2.bar(x2 - w / 2, probe_all, w, label=\"Probe\", alpha=0.85)\n",
        "    ax2.bar(x2 + w / 2, control_all, w, label=\"Control\", alpha=0.85)\n",
        "    ax2.set_xticks(x2)\n",
        "    ax2.set_xticklabels(models, rotation=40, ha=\"right\")\n",
        "    ax2.set_ylim(0, 1)\n",
        "    ax2.set_title(\"All models: Probe vs Control\")\n",
        "    ax2.legend()\n",
        "\n",
        "    # Panel 3: Hetvabhasa failure distribution from taxonomy_coverage.json\n",
        "    if taxonomy and taxonomy.get(\"distribution\"):\n",
        "        dist = taxonomy[\"distribution\"]\n",
        "        labels3 = list(dist.keys())\n",
        "        values3 = [dist[k] for k in labels3]\n",
        "        ax3.bar(labels3, values3, alpha=0.9)\n",
        "        ax3.set_title(\"Failure distribution by hetvabhasa\")\n",
        "        ax3.set_ylabel(\"Count\")\n",
        "        ax3.tick_params(axis=\"x\", labelrotation=30)\n",
        "    else:\n",
        "        ax3.text(0.5, 0.5, \"No taxonomy data\", ha=\"center\", va=\"center\")\n",
        "        ax3.set_title(\"Failure distribution by hetvabhasa\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except ImportError:\n",
        "    print(\"matplotlib not available. Install with: pip install matplotlib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Optional Live Evaluation (Advanced)\n",
        "\n",
        "Use this only if you explicitly want to run fresh inference. This section loads a real backend (`ollama` or `transformers`) and runs the benchmark runner on a subset or full set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 8a. Run Live Evaluation (Optional)\n",
        "#@markdown This is optional and requires real model loading. Keep `run_live_eval` unchecked for viewer-only mode.\n",
        "\n",
        "live_backend = \"ollama\" #@param [\"ollama\", \"transformers\"]\n",
        "live_model_name = \"llama3.2:3b-instruct-q4_K_M\" #@param {type:\"string\"}\n",
        "ollama_base_url = \"http://localhost:11434\" #@param {type:\"string\"}\n",
        "hf_token = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown **Generation parameters:**\n",
        "live_max_new_tokens = 1024 #@param {type:\"integer\"}\n",
        "live_temperature = 0.5 #@param {type:\"number\"}\n",
        "live_top_p = 0.75 #@param {type:\"number\"}\n",
        "live_top_k = 5 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown **Execution controls:**\n",
        "run_live_eval = False #@param {type:\"boolean\"}\n",
        "live_evaluate_all = False #@param {type:\"boolean\"}\n",
        "live_subset_size = 5 #@param {type:\"integer\"}\n",
        "\n",
        "if not run_live_eval:\n",
        "    print(\"Live evaluation is disabled.\")\n",
        "    print(\"Set run_live_eval=True only when you want fresh inference.\")\n",
        "else:\n",
        "    try:\n",
        "        from pramana_backend import create_backend\n",
        "        from pramana.benchmarks.vyapti_runner import VyaptiEvaluationRunner\n",
        "\n",
        "        if live_backend == \"ollama\":\n",
        "            backend_obj = create_backend(\n",
        "                \"ollama\",\n",
        "                model_name=live_model_name,\n",
        "                base_url=ollama_base_url,\n",
        "            )\n",
        "        elif live_backend == \"transformers\":\n",
        "            backend_obj = create_backend(\n",
        "                \"transformers\",\n",
        "                model_id=live_model_name,\n",
        "                hf_token=hf_token or None,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backend: {live_backend}\")\n",
        "\n",
        "        def model_fn(prompt: str) -> str:\n",
        "            return backend_obj.generate(\n",
        "                prompt,\n",
        "                max_new_tokens=live_max_new_tokens,\n",
        "                temperature=live_temperature,\n",
        "                top_p=live_top_p,\n",
        "                top_k=live_top_k,\n",
        "            )\n",
        "\n",
        "        runner = VyaptiEvaluationRunner(\n",
        "            {\n",
        "                \"benchmark_path\": \"data/vyapti_probe/problems.json\",\n",
        "                \"solutions_path\": \"data/vyapti_probe/solutions.json\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "        if live_evaluate_all:\n",
        "            live_results = runner.evaluate_model(live_model_name, model_fn)\n",
        "        else:\n",
        "            n = max(1, min(int(live_subset_size), len(problems)))\n",
        "            subset_ids = [p[\"id\"] for p in problems[:n]]\n",
        "            live_results = runner.evaluate_model(live_model_name, model_fn, problem_ids=subset_ids)\n",
        "\n",
        "        total = len(live_results)\n",
        "        correct = sum(1 for r in live_results if r.final_answer_correct)\n",
        "        probes_live = [r for r in live_results if r.problem_type == \"probe\"]\n",
        "        controls_live = [r for r in live_results if r.problem_type == \"control\"]\n",
        "        probe_acc = (sum(1 for r in probes_live if r.final_answer_correct) / max(1, len(probes_live)))\n",
        "        control_acc = (sum(1 for r in controls_live if r.final_answer_correct) / max(1, len(controls_live)))\n",
        "\n",
        "        print(f\"\\n=== Live Results: {live_model_name} ({live_backend}) ===\")\n",
        "        print(f\"Accuracy: {correct}/{total} ({correct / max(1, total):.1%})\")\n",
        "        print(f\"Probe accuracy: {probe_acc:.1%}\")\n",
        "        print(f\"Control accuracy: {control_acc:.1%}\")\n",
        "        print(f\"Vyapti gap (control - probe): {(control_acc - probe_acc):+.1%}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Live evaluation failed: {e}\")\n",
        "        print(\"\\nTips:\")\n",
        "        print(\"- For ollama: ensure local server is running and model exists\")\n",
        "        print(\"- For transformers: provide a valid HF model id and token (if gated)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook is now **viewer-first**:\n",
        "\n",
        "1. Primary workflow analyzes pre-computed real campaign artifacts from `data/vyapti_probe/results_real/`.\n",
        "2. You can browse benchmark content and inspect model input/output behavior per problem.\n",
        "3. You can run optional fresh inference with real backends only (`ollama` or `transformers`).\n",
        "\n",
        "**Core data files:**\n",
        "- `data/vyapti_probe/problems.json`\n",
        "- `data/vyapti_probe/solutions.json`\n",
        "- `data/vyapti_probe/results_real/summary.json`\n",
        "- `data/vyapti_probe/results_real/comparisons.json`\n",
        "- `data/vyapti_probe/results_real/taxonomy_coverage.json`\n",
        "\n",
        "**Core scripts:**\n",
        "- `scripts/run_vyapti_real.py` — Full real evaluation campaign\n",
        "- `scripts/run_vyapti_analysis.py` — Statistical analysis + plots\n",
        "\n",
        "Use the optional live section only when you intentionally want to load models and run inference."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
