{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pramana Evaluation Notebook\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SharathSPhD/pramana/blob/main/notebooks/02_pramana_evaluation.ipynb)\n",
    "\n",
    "Comprehensive evaluation of Pramana models across multiple tiers:\n",
    "- Tier 1: Structural evaluation (format adherence)\n",
    "- Tier 2: Content quality evaluation\n",
    "- Answer correctness (exact/normalized/semantic matching)\n",
    "- Cross-stage comparison\n",
    "- Failure analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form"
   },
   "source": [
    "#@title Setup: Clone repo, install deps, detect GPU (double-click to show code)\n",
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# ---- Clone repo (skip if already cloned or running locally) ----\n",
    "REPO_URL = \"https://github.com/SharathSPhD/pramana.git\"\n",
    "REPO_DIR = \"pramana\"\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not Path(REPO_DIR).exists():\n",
    "        print(\"Cloning repository...\")\n",
    "        subprocess.run([\"git\", \"clone\", REPO_URL], check=True, capture_output=True)\n",
    "        print(\"  Done.\")\n",
    "    os.chdir(REPO_DIR)\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", \"notebooks/requirements.txt\"],\n",
    "                   check=True, capture_output=True)\n",
    "    sys.path.insert(0, \"notebooks\")\n",
    "else:\n",
    "    # Local: pramana_backend.py should be in the same dir or parent\n",
    "    if Path(\"pramana_backend.py\").exists():\n",
    "        sys.path.insert(0, \".\")\n",
    "    elif Path(\"notebooks/pramana_backend.py\").exists():\n",
    "        sys.path.insert(0, \"notebooks\")\n",
    "    else:\n",
    "        for p in [Path(\"..\"), Path(\"../notebooks\")]:\n",
    "            if (p / \"pramana_backend.py\").exists():\n",
    "                sys.path.insert(0, str(p.resolve()))\n",
    "                break\n",
    "\n",
    "# ---- GPU Detection ----\n",
    "def check_backend():\n",
    "    \"\"\"Detect GPU/CPU and display status banner.\"\"\"\n",
    "    gpu_name, gpu_mem = None, None\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader,nounits\"],\n",
    "            capture_output=True, text=True, timeout=5,\n",
    "        )\n",
    "        if result.returncode == 0 and result.stdout.strip():\n",
    "            parts = result.stdout.strip().split(\", \")\n",
    "            gpu_name = parts[0]\n",
    "            gpu_mem = parts[1] if len(parts) > 1 else \"?\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if gpu_name:\n",
    "        hw = f\"<b>GPU: {gpu_name} ({gpu_mem} MB)</b>\"\n",
    "        color = \"green\"\n",
    "    else:\n",
    "        hw = \"<b>CPU</b>\"\n",
    "        color = \"orange\"\n",
    "\n",
    "    env = \"Google Colab\" if IN_COLAB else \"Local\"\n",
    "    banner = (\n",
    "        f\"<div style='padding:10px;border-radius:8px;border:2px solid {color};margin:8px 0'>\"\n",
    "        f\"<span style='font-size:1.2em'>Runtime: <span style='color:{color}'>{hw}</span></span>\"\n",
    "        f\"<br>Environment: {env}\"\n",
    "    )\n",
    "    if not gpu_name:\n",
    "        banner += \"<br><i>For better performance: Runtime -> Change runtime type -> GPU</i>\"\n",
    "    banner += \"</div>\"\n",
    "    display(HTML(banner))\n",
    "    return gpu_name is not None\n",
    "\n",
    "GPU_AVAILABLE = check_backend()\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form"
   },
   "source": [
    "#@title Imports (double-click to show code)\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from pramana_backend import (\n",
    "    STAGE_CONFIGS,\n",
    "    OLLAMA_MODEL_MAP,\n",
    "    build_user_prompt,\n",
    "    create_backend,\n",
    "    EXAMPLE_PROBLEMS,\n",
    "    load_test_problems,\n",
    "    parse_nyaya_phases,\n",
    "    validate_structure,\n",
    "    score_content_quality,\n",
    "    extract_final_answer,\n",
    "    normalize_text,\n",
    "    token_overlap_ratio,\n",
    "    score_answers,\n",
    "    wilson_interval,\n",
    "    setup_ollama,\n",
    "    setup_ollama_stage,\n",
    "    download_gguf,\n",
    ")\n",
    "\n",
    "print(\"\\u2713 All modules imported\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Evaluation configuration (double-click to show code)\n",
    "# Evaluation configuration\n",
    "# Data source: \"embedded\" (built-in examples), \"huggingface\" (download from HF), or a local directory path\n",
    "DATA_SOURCE = \"embedded\"\n",
    "RESULTS_DIR = Path(\"./results\")\n",
    "OUTPUT_DIR = Path(\"./results/evaluation\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Evaluation tiers to run (1=structural, 2=content quality, 3=Z3 verification)\n",
    "EVAL_TIERS = [1, 2]  # Skip Tier 3 (Z3) unless needed\n",
    "\n",
    "# ---- Generation hyperparameters ----\n",
    "# These match the HF Space app controls. On local hardware (not ZeroGPU),\n",
    "# you can use much larger token lengths to avoid truncating Nyaya output.\n",
    "BATCH_SIZE = 4        # Process examples in batches\n",
    "MAX_TOKENS = 2048     # Maximum generation length (raised from 1024 for local hardware)\n",
    "TEMPERATURE = 0.5     # Sampling temperature (0 = deterministic)\n",
    "TOP_P = 0.75          # Nucleus sampling parameter\n",
    "TOP_K = 5             # Top-k sampling parameter (0 = disabled)\n",
    "TIMEOUT_SECONDS = 120 # Timeout per generation (raised for longer outputs)\n",
    "\n",
    "# Stages to evaluate\n",
    "STAGES_TO_EVALUATE = [\"Stage 0\", \"Stage 1\"]\n",
    "\n",
    "print(f\"✓ Data source: {DATA_SOURCE}\")\n",
    "print(f\"✓ Results directory: {OUTPUT_DIR}\")\n",
    "print(f\"✓ Evaluation tiers: {EVAL_TIERS}\")\n",
    "print(f\"✓ Generation hyperparameters: max_tokens={MAX_TOKENS}, temperature={TEMPERATURE}, top_p={TOP_P}, top_k={TOP_K}\")\n",
    "print(f\"✓ Stages to evaluate: {STAGES_TO_EVALUATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form"
   },
   "source": [
    "#@title Initialize backend (double-click to show code)\n",
    "# Change BACKEND_TYPE to switch inference backends.\n",
    "#   \"ollama\"       - Ollama server (recommended, auto-setup available)\n",
    "#   \"llamacpp\"     - llama-cpp-python (direct GGUF loading)\n",
    "#   \"transformers\" - HuggingFace Transformers (requires GPU + pip install transformers torch)\n",
    "\n",
    "BACKEND_TYPE = \"ollama\"  # <-- Change this to your preferred backend\n",
    "\n",
    "# Backend-specific configuration\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "GGUF_PATH = \"/tmp/nyaya-llama-3b-stage0.gguf\"\n",
    "\n",
    "# Auto-setup: install and configure the selected backend\n",
    "if BACKEND_TYPE == \"ollama\":\n",
    "    for _stage in STAGES_TO_EVALUATE:\n",
    "        print(f\"\\n--- Setting up Ollama for {_stage} ---\")\n",
    "        try:\n",
    "            _result = setup_ollama_stage(_stage, base_url=OLLAMA_URL)\n",
    "        except Exception as e:\n",
    "            print(f\"  Setup error for {_stage}: {e}\")\n",
    "\n",
    "elif BACKEND_TYPE == \"llamacpp\":\n",
    "    if not Path(GGUF_PATH).exists():\n",
    "        print(\"GGUF not found. Auto-downloading...\")\n",
    "        try:\n",
    "            GGUF_PATH = download_gguf()\n",
    "            print(f\"GGUF ready at: {GGUF_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Auto-download failed: {e}\")\n",
    "    else:\n",
    "        print(f\"GGUF found at: {GGUF_PATH}\")\n",
    "\n",
    "elif BACKEND_TYPE == \"transformers\":\n",
    "    import subprocess as _sp\n",
    "    _sp.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\", \"torch\", \"accelerate\", \"peft\"])\n",
    "\n",
    "backend_cache = {}\n",
    "\n",
    "def get_backend(stage_name: str, role: str = \"tuned\"):\n",
    "    \"\"\"Get or create backend for a stage.\"\"\"\n",
    "    cache_key = f\"{stage_name}:{role}\"\n",
    "    if cache_key not in backend_cache:\n",
    "        try:\n",
    "            stage_config = STAGE_CONFIGS[stage_name]\n",
    "            if BACKEND_TYPE == \"transformers\":\n",
    "                model_id = stage_config.base_model_id if role == \"base\" else stage_config.tuned_model_id\n",
    "                backend_cache[cache_key] = create_backend(\"transformers\", model_id=model_id)\n",
    "            elif BACKEND_TYPE == \"ollama\":\n",
    "                model_map = OLLAMA_MODEL_MAP.get(stage_name, {})\n",
    "                model_name = model_map.get(role)\n",
    "                if model_name is None:\n",
    "                    print(f\"  Warning: No Ollama model for {stage_name} role={role}\")\n",
    "                    backend_cache[cache_key] = None\n",
    "                else:\n",
    "                    backend_cache[cache_key] = create_backend(\"ollama\", model_name=model_name, base_url=OLLAMA_URL)\n",
    "            elif BACKEND_TYPE == \"llamacpp\":\n",
    "                backend_cache[cache_key] = create_backend(\"llamacpp\", model_path=GGUF_PATH)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown backend: {BACKEND_TYPE}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not create backend for {stage_name} ({role}): {e}\")\n",
    "            backend_cache[cache_key] = None\n",
    "    return backend_cache[cache_key]\n",
    "\n",
    "def get_model_label(stage_name: str, role: str = \"tuned\") -> str:\n",
    "    if BACKEND_TYPE == \"ollama\":\n",
    "        return OLLAMA_MODEL_MAP.get(stage_name, {}).get(role, \"unknown\")\n",
    "    stage_config = STAGE_CONFIGS[stage_name]\n",
    "    return stage_config.base_model_id if role == \"base\" else stage_config.tuned_model_id\n",
    "\n",
    "_env = \"Colab VM\" if IN_COLAB else \"Local\"\n",
    "print(f\"\\nBackend: {BACKEND_TYPE} (running on {_env})\")\n",
    "print(\"Backend initialization ready\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Load validation examples (self-contained, no external files needed) (double-click to show code)\n",
    "# Load validation examples (self-contained, no external files needed)\n",
    "validation_examples = load_test_problems(DATA_SOURCE)\n",
    "print(f\"✓ Loaded {len(validation_examples)} validation examples from '{DATA_SOURCE}'\")\n",
    "\n",
    "# Display example summary\n",
    "for i, ex in enumerate(validation_examples):\n",
    "    print(f\"  {i+1}. [{ex['id']}] {ex['problem_type']} ({ex['difficulty']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Optionally load from HuggingFace datasets (double-click to show code)\n",
    "# Optionally load from HuggingFace datasets\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    # Load Stage 0 dataset\n",
    "    stage0_dataset = load_dataset(\n",
    "        STAGE_CONFIGS[\"Stage 0\"].dataset_repo_id,\n",
    "        split=\"test\",\n",
    "        token=HF_TOKEN\n",
    "    )\n",
    "    print(f\"✓ Loaded Stage 0 dataset: {len(stage0_dataset)} examples\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not load HF dataset: {e}\")\n",
    "    stage0_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Display problem summary table (double-click to show code)\n",
    "# Display problem summary table\n",
    "import pandas as pd\n",
    "\n",
    "problem_summary = pd.DataFrame([\n",
    "    {\n",
    "        \"ID\": ex[\"id\"],\n",
    "        \"Problem Type\": ex[\"problem_type\"],\n",
    "        \"Difficulty\": ex.get(\"difficulty\", \"unknown\"),\n",
    "        \"Has Ground Truth\": bool(ex.get(\"ground_truth\")),\n",
    "        \"Problem Preview\": ex[\"problem\"][:60] + \"...\" if len(ex[\"problem\"]) > 60 else ex[\"problem\"]\n",
    "    }\n",
    "    for ex in validation_examples\n",
    "])\n",
    "\n",
    "display(problem_summary)\n",
    "print(f\"\\nTotal problems: {len(validation_examples)}\")\n",
    "print(f\"Problem types: {problem_summary['Problem Type'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Run inference over test suite (double-click to show code)\n",
    "# Run inference over test suite\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import signal\n",
    "\n",
    "def generate_with_timeout(backend, prompt: str, system_prompt: str, timeout: int) -> str:\n",
    "    \"\"\"Generate with timeout handling.\"\"\"\n",
    "    def timeout_handler(signum, frame):\n",
    "        raise TimeoutError(f\"Generation exceeded {timeout}s\")\n",
    "    \n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    signal.alarm(timeout)\n",
    "    \n",
    "    try:\n",
    "        if backend:\n",
    "            output = backend.generate(\n",
    "                prompt,\n",
    "                system_prompt=system_prompt,\n",
    "                max_new_tokens=MAX_TOKENS,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p=TOP_P,\n",
    "                top_k=TOP_K,\n",
    "            )\n",
    "        else:\n",
    "            # Fallback: direct model loading\n",
    "            output = \"[Backend not available]\"\n",
    "        signal.alarm(0)\n",
    "        return output\n",
    "    except TimeoutError:\n",
    "        signal.alarm(0)\n",
    "        return \"[TIMEOUT]\"\n",
    "    except Exception as e:\n",
    "        signal.alarm(0)\n",
    "        return f\"[ERROR: {str(e)[:50]}]\"\n",
    "\n",
    "def run_batch_generation(stage_name: str, examples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Run batch generation for a stage.\"\"\"\n",
    "    stage_config = STAGE_CONFIGS[stage_name]\n",
    "    backend = get_backend(stage_name)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for ex in tqdm(examples, desc=f\"Generating ({stage_name})\"):\n",
    "        problem = ex[\"problem\"]\n",
    "        user_prompt = build_user_prompt(problem)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        generated_output = generate_with_timeout(\n",
    "            backend,\n",
    "            user_prompt,\n",
    "            stage_config.system_prompt,\n",
    "            TIMEOUT_SECONDS\n",
    "        )\n",
    "        generation_time = time.time() - start_time\n",
    "        \n",
    "        results.append({\n",
    "            \"example_id\": ex[\"id\"],\n",
    "            \"stage\": stage_name,\n",
    "            \"problem\": problem,\n",
    "            \"generated_output\": generated_output,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"ground_truth\": ex.get(\"ground_truth\", \"\"),\n",
    "            \"problem_type\": ex[\"problem_type\"],\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Batch generation functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Generate outputs for all stages (double-click to show code)\n",
    "# Generate outputs for all stages\n",
    "all_generation_results = {}\n",
    "\n",
    "for stage_name in STAGES_TO_EVALUATE:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Generating outputs for {stage_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    stage_results = run_batch_generation(stage_name, validation_examples)\n",
    "    all_generation_results[stage_name] = stage_results\n",
    "    \n",
    "    # Cache to JSON\n",
    "    cache_file = OUTPUT_DIR / f\"generations_{stage_name.lower().replace(' ', '_')}.json\"\n",
    "    with open(cache_file, \"w\") as f:\n",
    "        json.dump(stage_results, f, indent=2, default=str)\n",
    "    print(f\"✓ Cached to {cache_file}\")\n",
    "\n",
    "print(f\"\\n✓ Completed generation for {len(STAGES_TO_EVALUATE)} stages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Load cached results if available (for re-running evaluation without regeneration) (double-click to show code)\n",
    "# Load cached results if available (for re-running evaluation without regeneration)\n",
    "def load_cached_generations() -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"Load cached generation results.\"\"\"\n",
    "    cached = {}\n",
    "    for stage_name in STAGES_TO_EVALUATE:\n",
    "        cache_file = OUTPUT_DIR / f\"generations_{stage_name.lower().replace(' ', '_')}.json\"\n",
    "        if cache_file.exists():\n",
    "            with open(cache_file) as f:\n",
    "                cached[stage_name] = json.load(f)\n",
    "            print(f\"✓ Loaded cached results for {stage_name}\")\n",
    "    return cached\n",
    "\n",
    "# Uncomment to use cached results:\n",
    "# all_generation_results = load_cached_generations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tier 1: Structural Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Structural evaluation using self-contained validate_structure() (double-click to show code)\n",
    "# Structural evaluation using self-contained validate_structure()\n",
    "\n",
    "def evaluate_structural(stage_name: str, generation_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Run Tier 1 structural evaluation (self-contained regex-based).\"\"\"\n",
    "    evaluated = []\n",
    "    \n",
    "    for result in generation_results:\n",
    "        example_id = result[\"example_id\"]\n",
    "        generated_output = result[\"generated_output\"]\n",
    "        \n",
    "        eval_result = {\n",
    "            \"example_id\": example_id,\n",
    "            \"stage\": stage_name,\n",
    "            \"parse_success\": False,\n",
    "            \"format_metrics\": {},\n",
    "            \"tier1_result\": None,\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Run self-contained structural validation\n",
    "            validation = validate_structure(generated_output)\n",
    "            \n",
    "            eval_result[\"parse_success\"] = validation[\"phases_present\"] >= 3  # At least half the phases\n",
    "            eval_result[\"format_metrics\"] = {\n",
    "                \"phase_completeness\": validation[\"phase_details\"],\n",
    "                \"num_phases_present\": validation[\"phases_present\"],\n",
    "                \"pramana_sources\": validation[\"pramana_sources\"],\n",
    "                \"num_syllogisms\": validation[\"syllogism_count\"],\n",
    "            }\n",
    "            eval_result[\"tier1_result\"] = {\n",
    "                \"passed\": validation[\"passed\"],\n",
    "                \"score\": validation[\"phases_present\"] / 6.0,\n",
    "                \"errors\": validation[\"errors\"],\n",
    "                \"details\": validation[\"phase_details\"],\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            eval_result[\"parse_error\"] = f\"Unexpected error: {e}\"\n",
    "        \n",
    "        evaluated.append(eval_result)\n",
    "    \n",
    "    return evaluated\n",
    "\n",
    "print(\"✓ Structural evaluation functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Run Tier 1 evaluation for all stages (double-click to show code)\n",
    "# Run Tier 1 evaluation for all stages\n",
    "tier1_results = {}\n",
    "\n",
    "for stage_name, generation_results in all_generation_results.items():\n",
    "    print(f\"\\nEvaluating {stage_name}...\")\n",
    "    tier1_results[stage_name] = evaluate_structural(stage_name, generation_results)\n",
    "\n",
    "print(f\"\\n✓ Completed Tier 1 evaluation for {len(tier1_results)} stages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Format adherence summary table (double-click to show code)\n",
    "# Format adherence summary table\n",
    "format_summary_rows = []\n",
    "\n",
    "for stage_name, results in tier1_results.items():\n",
    "    total = len(results)\n",
    "    parse_success = sum(1 for r in results if r.get(\"parse_success\"))\n",
    "    tier1_passed = sum(1 for r in results if r.get(\"tier1_result\", {}).get(\"passed\"))\n",
    "    \n",
    "    if parse_success > 0:\n",
    "        avg_phases = sum(\n",
    "            r.get(\"format_metrics\", {}).get(\"num_phases_present\", 0)\n",
    "            for r in results if r.get(\"parse_success\")\n",
    "        ) / parse_success\n",
    "        avg_pramana = sum(\n",
    "            r.get(\"format_metrics\", {}).get(\"num_pramana_sources\", 0)\n",
    "            for r in results if r.get(\"parse_success\")\n",
    "        ) / parse_success\n",
    "    else:\n",
    "        avg_phases = 0\n",
    "        avg_pramana = 0\n",
    "    \n",
    "    format_summary_rows.append({\n",
    "        \"Stage\": stage_name,\n",
    "        \"Total Examples\": total,\n",
    "        \"Parse Success\": parse_success,\n",
    "        \"Parse Rate\": f\"{parse_success/total*100:.1f}%\" if total > 0 else \"0%\",\n",
    "        \"Tier 1 Passed\": tier1_passed,\n",
    "        \"Tier 1 Pass Rate\": f\"{tier1_passed/total*100:.1f}%\" if total > 0 else \"0%\",\n",
    "        \"Avg Phases Present\": f\"{avg_phases:.1f}/6\",\n",
    "        \"Avg Pramana Sources\": f\"{avg_pramana:.1f}\",\n",
    "    })\n",
    "\n",
    "format_summary_df = pd.DataFrame(format_summary_rows)\n",
    "display(format_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Format adherence bar chart (double-click to show code)\n",
    "# Format adherence bar chart\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Parse success rate\n",
    "stages = format_summary_df[\"Stage\"].tolist()\n",
    "parse_rates = [float(r.replace(\"%\", \"\")) for r in format_summary_df[\"Parse Rate\"]]\n",
    "tier1_rates = [float(r.replace(\"%\", \"\")) for r in format_summary_df[\"Tier 1 Pass Rate\"]]\n",
    "\n",
    "axes[0].bar(stages, parse_rates, color=\"skyblue\", alpha=0.7)\n",
    "axes[0].set_ylabel(\"Rate (%)\")\n",
    "axes[0].set_title(\"Parse Success Rate\")\n",
    "axes[0].set_ylim(0, 100)\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "axes[1].bar(stages, tier1_rates, color=\"lightgreen\", alpha=0.7)\n",
    "axes[1].set_ylabel(\"Rate (%)\")\n",
    "axes[1].set_title(\"Tier 1 Pass Rate\")\n",
    "axes[1].set_ylim(0, 100)\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tier 2: Content Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Content quality evaluation using self-contained score_content_quality() (double-click to show code)\n",
    "# Content quality evaluation using self-contained score_content_quality()\n",
    "\n",
    "def evaluate_content_quality(stage_name: str, generation_results: List[Dict[str, Any]], tier1_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Run Tier 2 content quality evaluation (self-contained regex-based).\"\"\"\n",
    "    evaluated = []\n",
    "    \n",
    "    # Create mapping from example_id to tier1 result\n",
    "    tier1_map = {r[\"example_id\"]: r for r in tier1_results}\n",
    "    \n",
    "    for result in generation_results:\n",
    "        example_id = result[\"example_id\"]\n",
    "        tier1_result = tier1_map.get(example_id, {})\n",
    "        \n",
    "        eval_result = {\n",
    "            \"example_id\": example_id,\n",
    "            \"stage\": stage_name,\n",
    "            \"content_quality\": None,\n",
    "        }\n",
    "        \n",
    "        # Only evaluate if parsing succeeded\n",
    "        if tier1_result.get(\"parse_success\"):\n",
    "            try:\n",
    "                generated_output = result[\"generated_output\"]\n",
    "                problem_text = result.get(\"problem\", \"\")\n",
    "                \n",
    "                # Run self-contained content quality scoring\n",
    "                quality_scores = score_content_quality(generated_output, problem_text)\n",
    "                \n",
    "                eval_result[\"content_quality\"] = {\n",
    "                    \"pratyaksha_score\": quality_scores[\"pratyaksha_grounding\"],\n",
    "                    \"udaharana_valid\": quality_scores[\"udaharana_valid\"] > 0,\n",
    "                    \"tarka_meaningful\": quality_scores[\"tarka_meaningful\"] > 0,\n",
    "                    \"hetvabhasa_completeness\": quality_scores[\"hetvabhasa_completeness\"],\n",
    "                    \"overall_score\": quality_scores[\"overall\"],\n",
    "                }\n",
    "            except Exception as e:\n",
    "                eval_result[\"content_quality_error\"] = str(e)\n",
    "        \n",
    "        evaluated.append(eval_result)\n",
    "    \n",
    "    return evaluated\n",
    "\n",
    "print(\"✓ Content quality evaluation functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Run Tier 2 evaluation (double-click to show code)\n",
    "# Run Tier 2 evaluation\n",
    "tier2_results = {}\n",
    "\n",
    "for stage_name in STAGES_TO_EVALUATE:\n",
    "    generation_results = all_generation_results[stage_name]\n",
    "    stage_tier1_results = tier1_results[stage_name]\n",
    "    tier2_results[stage_name] = evaluate_content_quality(\n",
    "        stage_name, generation_results, stage_tier1_results\n",
    "    )\n",
    "\n",
    "print(f\"✓ Completed Tier 2 evaluation for {len(tier2_results)} stages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Content quality radar chart (double-click to show code)\n",
    "# Content quality radar chart\n",
    "import numpy as np\n",
    "\n",
    "def plot_content_quality_radar(stage_results: Dict[str, List[Dict[str, Any]]]):\n",
    "    \"\"\"Create radar chart comparing content quality across stages.\"\"\"\n",
    "    categories = [\n",
    "        \"Pratyaksha Score\",\n",
    "        \"Udaharana Valid\",\n",
    "        \"Tarka Meaningful\",\n",
    "        \"Hetvabhasa Completeness\",\n",
    "        \"Overall Score\"\n",
    "    ]\n",
    "    \n",
    "    num_categories = len(categories)\n",
    "    angles = np.linspace(0, 2 * np.pi, num_categories, endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection=\"polar\"))\n",
    "    \n",
    "    for stage_name, results in stage_results.items():\n",
    "        # Calculate averages\n",
    "        quality_scores = [r.get(\"content_quality\") for r in results if r.get(\"content_quality\")]\n",
    "        \n",
    "        if not quality_scores:\n",
    "            continue\n",
    "        \n",
    "        avg_pratyaksha = np.mean([q[\"pratyaksha_score\"] for q in quality_scores])\n",
    "        avg_udaharana = np.mean([1.0 if q[\"udaharana_valid\"] else 0.0 for q in quality_scores])\n",
    "        avg_tarka = np.mean([1.0 if q[\"tarka_meaningful\"] else 0.0 for q in quality_scores])\n",
    "        avg_hetvabhasa = np.mean([q[\"hetvabhasa_completeness\"] for q in quality_scores])\n",
    "        avg_overall = np.mean([q[\"overall_score\"] for q in quality_scores])\n",
    "        \n",
    "        values = [avg_pratyaksha, avg_udaharana, avg_tarka, avg_hetvabhasa, avg_overall]\n",
    "        values += values[:1]  # Complete the circle\n",
    "        \n",
    "        ax.plot(angles, values, \"o-\", linewidth=2, label=stage_name)\n",
    "        ax.fill(angles, values, alpha=0.25)\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(\"Content Quality Comparison\", size=16, fontweight=\"bold\", pad=20)\n",
    "    ax.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_content_quality_radar(tier2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Answer Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Answer correctness evaluation using self-contained extract_final_answer() + score_answers() (double-click to show code)\n",
    "# Answer correctness evaluation using self-contained extract_final_answer() + score_answers()\n",
    "\n",
    "def evaluate_answer_correctness(\n",
    "    stage_name: str,\n",
    "    generation_results: List[Dict[str, Any]],\n",
    "    tier1_results: List[Dict[str, Any]]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Evaluate answer correctness using self-contained scoring functions.\"\"\"\n",
    "    evaluated = []\n",
    "    \n",
    "    # Create mapping from example_id to tier1 result\n",
    "    tier1_map = {r[\"example_id\"]: r for r in tier1_results}\n",
    "    \n",
    "    for result in generation_results:\n",
    "        example_id = result[\"example_id\"]\n",
    "        ground_truth = result.get(\"ground_truth\", \"\")\n",
    "        tier1_result = tier1_map.get(example_id, {})\n",
    "        \n",
    "        eval_result = {\n",
    "            \"example_id\": example_id,\n",
    "            \"stage\": stage_name,\n",
    "            \"answer_scores\": None,\n",
    "        }\n",
    "        \n",
    "        # Only evaluate if we have ground truth and parsing succeeded\n",
    "        if ground_truth and tier1_result.get(\"parse_success\"):\n",
    "            try:\n",
    "                generated_output = result[\"generated_output\"]\n",
    "                \n",
    "                # Extract final answer using self-contained function\n",
    "                predicted_answer = extract_final_answer(generated_output) or \"\"\n",
    "                \n",
    "                # Score answers\n",
    "                scores = score_answers(predicted_answer, ground_truth)\n",
    "                eval_result[\"answer_scores\"] = scores\n",
    "                \n",
    "            except Exception as e:\n",
    "                eval_result[\"answer_error\"] = str(e)\n",
    "        \n",
    "        evaluated.append(eval_result)\n",
    "    \n",
    "    return evaluated\n",
    "\n",
    "print(\"✓ Answer correctness evaluation functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Run answer correctness evaluation (double-click to show code)\n",
    "# Run answer correctness evaluation\n",
    "answer_results = {}\n",
    "\n",
    "for stage_name in STAGES_TO_EVALUATE:\n",
    "    generation_results = all_generation_results[stage_name]\n",
    "    stage_tier1_results = tier1_results[stage_name]\n",
    "    answer_results[stage_name] = evaluate_answer_correctness(\n",
    "        stage_name, generation_results, stage_tier1_results\n",
    "    )\n",
    "\n",
    "print(f\"✓ Completed answer correctness evaluation for {len(answer_results)} stages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Calculate Wilson confidence intervals (double-click to show code)\n",
    "# Calculate Wilson confidence intervals\n",
    "def calculate_accuracy_metrics(answer_results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate accuracy metrics with Wilson confidence intervals.\"\"\"\n",
    "    total = len(answer_results)\n",
    "    \n",
    "    exact_matches = sum(\n",
    "        1 for r in answer_results\n",
    "        if r.get(\"answer_scores\", {}).get(\"exact_match\")\n",
    "    )\n",
    "    normalized_matches = sum(\n",
    "        1 for r in answer_results\n",
    "        if r.get(\"answer_scores\", {}).get(\"normalized_match\")\n",
    "    )\n",
    "    semantic_matches = sum(\n",
    "        1 for r in answer_results\n",
    "        if r.get(\"answer_scores\", {}).get(\"semantic_match\")\n",
    "    )\n",
    "    \n",
    "    exact_ci = wilson_interval(successes=exact_matches, total=total)\n",
    "    normalized_ci = wilson_interval(successes=normalized_matches, total=total)\n",
    "    semantic_ci = wilson_interval(successes=semantic_matches, total=total)\n",
    "    \n",
    "    avg_similarity = np.mean([\n",
    "        r.get(\"answer_scores\", {}).get(\"semantic_similarity\", 0.0)\n",
    "        for r in answer_results\n",
    "        if r.get(\"answer_scores\")\n",
    "    ]) if any(r.get(\"answer_scores\") for r in answer_results) else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"exact_matches\": exact_matches,\n",
    "        \"exact_rate\": exact_matches / total if total > 0 else 0.0,\n",
    "        \"exact_ci\": exact_ci,\n",
    "        \"normalized_matches\": normalized_matches,\n",
    "        \"normalized_rate\": normalized_matches / total if total > 0 else 0.0,\n",
    "        \"normalized_ci\": normalized_ci,\n",
    "        \"semantic_matches\": semantic_matches,\n",
    "        \"semantic_rate\": semantic_matches / total if total > 0 else 0.0,\n",
    "        \"semantic_ci\": semantic_ci,\n",
    "        \"avg_semantic_similarity\": avg_similarity,\n",
    "    }\n",
    "\n",
    "# Calculate metrics for each stage\n",
    "accuracy_metrics = {}\n",
    "for stage_name, results in answer_results.items():\n",
    "    accuracy_metrics[stage_name] = calculate_accuracy_metrics(results)\n",
    "\n",
    "# Display summary table\n",
    "accuracy_summary_rows = []\n",
    "for stage_name, metrics in accuracy_metrics.items():\n",
    "    accuracy_summary_rows.append({\n",
    "        \"Stage\": stage_name,\n",
    "        \"Total\": metrics[\"total\"],\n",
    "        \"Exact Match\": f\"{metrics['exact_matches']} ({metrics['exact_rate']*100:.1f}%)\",\n",
    "        \"Exact CI (95%)\": f\"[{metrics['exact_ci'][0]:.3f}, {metrics['exact_ci'][1]:.3f}]\",\n",
    "        \"Normalized Match\": f\"{metrics['normalized_matches']} ({metrics['normalized_rate']*100:.1f}%)\",\n",
    "        \"Normalized CI (95%)\": f\"[{metrics['normalized_ci'][0]:.3f}, {metrics['normalized_ci'][1]:.3f}]\",\n",
    "        \"Semantic Match\": f\"{metrics['semantic_matches']} ({metrics['semantic_rate']*100:.1f}%)\",\n",
    "        \"Semantic CI (95%)\": f\"[{metrics['semantic_ci'][0]:.3f}, {metrics['semantic_ci'][1]:.3f}]\",\n",
    "        \"Avg Similarity\": f\"{metrics['avg_semantic_similarity']:.3f}\",\n",
    "    })\n",
    "\n",
    "accuracy_summary_df = pd.DataFrame(accuracy_summary_rows)\n",
    "display(accuracy_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Stage Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Side-by-side metrics comparison (double-click to show code)\n",
    "# Side-by-side metrics comparison\n",
    "comparison_data = []\n",
    "\n",
    "for stage_name in STAGES_TO_EVALUATE:\n",
    "    stage_tier1 = tier1_results[stage_name]\n",
    "    stage_tier2 = tier2_results[stage_name]\n",
    "    stage_answers = answer_results[stage_name]\n",
    "    \n",
    "    # Tier 1 metrics\n",
    "    parse_rate = sum(1 for r in stage_tier1 if r.get(\"parse_success\")) / len(stage_tier1)\n",
    "    tier1_pass_rate = sum(1 for r in stage_tier1 if r.get(\"tier1_result\", {}).get(\"passed\")) / len(stage_tier1)\n",
    "    \n",
    "    # Tier 2 metrics\n",
    "    quality_scores = [r.get(\"content_quality\") for r in stage_tier2 if r.get(\"content_quality\")]\n",
    "    avg_quality = np.mean([q[\"overall_score\"] for q in quality_scores]) if quality_scores else 0.0\n",
    "    \n",
    "    # Answer correctness\n",
    "    exact_rate = accuracy_metrics[stage_name][\"exact_rate\"]\n",
    "    semantic_rate = accuracy_metrics[stage_name][\"semantic_rate\"]\n",
    "    \n",
    "    comparison_data.append({\n",
    "        \"Stage\": stage_name,\n",
    "        \"Parse Rate\": f\"{parse_rate*100:.1f}%\",\n",
    "        \"Tier 1 Pass Rate\": f\"{tier1_pass_rate*100:.1f}%\",\n",
    "        \"Avg Content Quality\": f\"{avg_quality:.3f}\",\n",
    "        \"Exact Match Rate\": f\"{exact_rate*100:.1f}%\",\n",
    "        \"Semantic Match Rate\": f\"{semantic_rate*100:.1f}%\",\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Visual comparison chart (double-click to show code)\n",
    "# Visual comparison chart\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "stages = comparison_df[\"Stage\"].tolist()\n",
    "\n",
    "# Parse rates\n",
    "parse_rates = [float(r.replace(\"%\", \"\")) for r in comparison_df[\"Parse Rate\"]]\n",
    "axes[0, 0].bar(stages, parse_rates, color=\"skyblue\", alpha=0.7)\n",
    "axes[0, 0].set_ylabel(\"Rate (%)\")\n",
    "axes[0, 0].set_title(\"Parse Success Rate\")\n",
    "axes[0, 0].set_ylim(0, 100)\n",
    "axes[0, 0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Tier 1 pass rates\n",
    "tier1_rates = [float(r.replace(\"%\", \"\")) for r in comparison_df[\"Tier 1 Pass Rate\"]]\n",
    "axes[0, 1].bar(stages, tier1_rates, color=\"lightgreen\", alpha=0.7)\n",
    "axes[0, 1].set_ylabel(\"Rate (%)\")\n",
    "axes[0, 1].set_title(\"Tier 1 Pass Rate\")\n",
    "axes[0, 1].set_ylim(0, 100)\n",
    "axes[0, 1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Content quality\n",
    "quality_scores = [float(r) for r in comparison_df[\"Avg Content Quality\"]]\n",
    "axes[1, 0].bar(stages, quality_scores, color=\"orange\", alpha=0.7)\n",
    "axes[1, 0].set_ylabel(\"Score\")\n",
    "axes[1, 0].set_title(\"Average Content Quality\")\n",
    "axes[1, 0].set_ylim(0, 1)\n",
    "axes[1, 0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Answer correctness\n",
    "exact_rates = [float(r.replace(\"%\", \"\")) for r in comparison_df[\"Exact Match Rate\"]]\n",
    "semantic_rates = [float(r.replace(\"%\", \"\")) for r in comparison_df[\"Semantic Match Rate\"]]\n",
    "x = np.arange(len(stages))\n",
    "width = 0.35\n",
    "axes[1, 1].bar(x - width/2, exact_rates, width, label=\"Exact Match\", color=\"coral\", alpha=0.7)\n",
    "axes[1, 1].bar(x + width/2, semantic_rates, width, label=\"Semantic Match\", color=\"lightblue\", alpha=0.7)\n",
    "axes[1, 1].set_ylabel(\"Rate (%)\")\n",
    "axes[1, 1].set_title(\"Answer Correctness\")\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(stages)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_ylim(0, 100)\n",
    "axes[1, 1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Failure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Identify worst examples (double-click to show code)\n",
    "# Identify worst examples\n",
    "def classify_failure_mode(result: Dict[str, Any]) -> str:\n",
    "    \"\"\"Classify the type of failure.\"\"\"\n",
    "    if not result.get(\"parse_success\"):\n",
    "        return \"Parse Failure\"\n",
    "    \n",
    "    tier1_passed = result.get(\"tier1_result\", {}).get(\"passed\", False)\n",
    "    if not tier1_passed:\n",
    "        return \"Tier 1 Failure (Structure)\"\n",
    "    \n",
    "    answer_scores = result.get(\"answer_scores\", {})\n",
    "    if answer_scores:\n",
    "        if not answer_scores.get(\"exact_match\") and not answer_scores.get(\"semantic_match\"):\n",
    "            return \"Answer Incorrect\"\n",
    "    \n",
    "    quality = result.get(\"content_quality\")\n",
    "    if quality and quality.get(\"overall_score\", 1.0) < 0.5:\n",
    "        return \"Low Content Quality\"\n",
    "    \n",
    "    return \"Success\"\n",
    "\n",
    "# Classify failures for all stages\n",
    "failure_analysis = {}\n",
    "\n",
    "for stage_name in STAGES_TO_EVALUATE:\n",
    "    stage_tier1 = tier1_results[stage_name]\n",
    "    stage_tier2 = tier2_results[stage_name]\n",
    "    stage_answers = answer_results[stage_name]\n",
    "    \n",
    "    # Merge results\n",
    "    merged_results = []\n",
    "    tier1_map = {r[\"example_id\"]: r for r in stage_tier1}\n",
    "    tier2_map = {r[\"example_id\"]: r for r in stage_tier2}\n",
    "    answer_map = {r[\"example_id\"]: r for r in stage_answers}\n",
    "    \n",
    "    for example_id in tier1_map.keys():\n",
    "        merged = {\n",
    "            \"example_id\": example_id,\n",
    "            **tier1_map.get(example_id, {}),\n",
    "            **tier2_map.get(example_id, {}),\n",
    "            **answer_map.get(example_id, {}),\n",
    "        }\n",
    "        merged[\"failure_mode\"] = classify_failure_mode(merged)\n",
    "        merged_results.append(merged)\n",
    "    \n",
    "    failure_analysis[stage_name] = merged_results\n",
    "\n",
    "print(\"✓ Failure analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Display worst examples with annotations (double-click to show code)\n",
    "# Display worst examples with annotations\n",
    "def display_worst_examples(stage_name: str, failure_results: List[Dict[str, Any]], n: int = 5):\n",
    "    \"\"\"Display the worst performing examples.\"\"\"\n",
    "    # Sort by failure severity\n",
    "    failure_order = {\"Parse Failure\": 0, \"Tier 1 Failure (Structure)\": 1, \"Answer Incorrect\": 2, \"Low Content Quality\": 3, \"Success\": 4}\n",
    "    \n",
    "    sorted_results = sorted(\n",
    "        failure_results,\n",
    "        key=lambda r: (\n",
    "            failure_order.get(r.get(\"failure_mode\", \"Success\"), 4),\n",
    "            -r.get(\"content_quality\", {}).get(\"overall_score\", 1.0) if r.get(\"content_quality\") else 0.0,\n",
    "            -r.get(\"answer_scores\", {}).get(\"semantic_similarity\", 1.0) if r.get(\"answer_scores\") else 0.0,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    worst = sorted_results[:n]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Worst {n} Examples for {stage_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for i, result in enumerate(worst, 1):\n",
    "        print(f\"\\n{i}. Example: {result['example_id']}\")\n",
    "        print(f\"   Failure Mode: {result.get('failure_mode', 'Unknown')}\")\n",
    "        \n",
    "        if not result.get(\"parse_success\"):\n",
    "            print(f\"   Parse Error: {result.get('parse_error', 'Unknown')}\")\n",
    "        \n",
    "        tier1_result = result.get(\"tier1_result\", {})\n",
    "        if tier1_result:\n",
    "            print(f\"   Tier 1 Passed: {tier1_result.get('passed', False)}\")\n",
    "            if tier1_result.get(\"errors\"):\n",
    "                print(f\"   Tier 1 Errors: {', '.join(tier1_result['errors'][:3])}\")\n",
    "        \n",
    "        quality = result.get(\"content_quality\")\n",
    "        if quality:\n",
    "            print(f\"   Content Quality: {quality.get('overall_score', 0.0):.3f}\")\n",
    "        \n",
    "        answer_scores = result.get(\"answer_scores\")\n",
    "        if answer_scores:\n",
    "            print(f\"   Exact Match: {answer_scores.get('exact_match', False)}\")\n",
    "            print(f\"   Semantic Similarity: {answer_scores.get('semantic_similarity', 0.0):.3f}\")\n",
    "        \n",
    "        # Find original problem\n",
    "        for ex in validation_examples:\n",
    "            if ex[\"id\"] == result[\"example_id\"]:\n",
    "                print(f\"   Problem Preview: {ex['problem'][:100]}...\")\n",
    "                break\n",
    "\n",
    "for stage_name, results in failure_analysis.items():\n",
    "    display_worst_examples(stage_name, results, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Failure mode distribution (double-click to show code)\n",
    "# Failure mode distribution\n",
    "failure_summary_rows = []\n",
    "\n",
    "for stage_name, results in failure_analysis.items():\n",
    "    failure_modes = [r.get(\"failure_mode\", \"Unknown\") for r in results]\n",
    "    mode_counts = pd.Series(failure_modes).value_counts()\n",
    "    \n",
    "    for mode, count in mode_counts.items():\n",
    "        failure_summary_rows.append({\n",
    "            \"Stage\": stage_name,\n",
    "            \"Failure Mode\": mode,\n",
    "            \"Count\": count,\n",
    "            \"Percentage\": f\"{count/len(results)*100:.1f}%\"\n",
    "        })\n",
    "\n",
    "failure_summary_df = pd.DataFrame(failure_summary_rows)\n",
    "display(failure_summary_df.pivot_table(\n",
    "    index=\"Failure Mode\",\n",
    "    columns=\"Stage\",\n",
    "    values=\"Count\",\n",
    "    fill_value=0\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Save comprehensive results as JSON (compatible with results/ format) (double-click to show code)\n",
    "# Save comprehensive results as JSON (compatible with results/ format)\n",
    "def compile_final_results() -> Dict[str, Any]:\n",
    "    \"\"\"Compile all evaluation results into final format.\"\"\"\n",
    "    return {\n",
    "        \"evaluation_metadata\": {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data_source\": DATA_SOURCE,\n",
    "            \"stages_evaluated\": STAGES_TO_EVALUATE,\n",
    "            \"eval_tiers\": EVAL_TIERS,\n",
    "            \"num_examples\": len(validation_examples),\n",
    "        },\n",
    "        \"tier1_results\": {\n",
    "            stage: [\n",
    "                {\n",
    "                    \"example_id\": r[\"example_id\"],\n",
    "                    \"parse_success\": r.get(\"parse_success\", False),\n",
    "                    \"format_metrics\": r.get(\"format_metrics\", {}),\n",
    "                    \"tier1_passed\": r.get(\"tier1_result\", {}).get(\"passed\", False),\n",
    "                    \"tier1_score\": r.get(\"tier1_result\", {}).get(\"score\", 0.0),\n",
    "                    \"tier1_errors\": r.get(\"tier1_result\", {}).get(\"errors\", []),\n",
    "                }\n",
    "                for r in results\n",
    "            ]\n",
    "            for stage, results in tier1_results.items()\n",
    "        },\n",
    "        \"tier2_results\": {\n",
    "            stage: [\n",
    "                {\n",
    "                    \"example_id\": r[\"example_id\"],\n",
    "                    \"content_quality\": r.get(\"content_quality\"),\n",
    "                }\n",
    "                for r in results\n",
    "            ]\n",
    "            for stage, results in tier2_results.items()\n",
    "        },\n",
    "        \"answer_results\": {\n",
    "            stage: [\n",
    "                {\n",
    "                    \"example_id\": r[\"example_id\"],\n",
    "                    \"answer_scores\": r.get(\"answer_scores\"),\n",
    "                }\n",
    "                for r in results\n",
    "            ]\n",
    "            for stage, results in answer_results.items()\n",
    "        },\n",
    "        \"summary_metrics\": {\n",
    "            stage: {\n",
    "                \"format_adherence\": {\n",
    "                    \"parse_rate\": sum(1 for r in tier1_results[stage] if r.get(\"parse_success\")) / len(tier1_results[stage]),\n",
    "                    \"tier1_pass_rate\": sum(1 for r in tier1_results[stage] if r.get(\"tier1_result\", {}).get(\"passed\")) / len(tier1_results[stage]),\n",
    "                },\n",
    "                \"content_quality\": {\n",
    "                    \"avg_overall_score\": np.mean([\n",
    "                        r.get(\"content_quality\", {}).get(\"overall_score\", 0.0)\n",
    "                        for r in tier2_results[stage]\n",
    "                        if r.get(\"content_quality\")\n",
    "                    ]) if any(r.get(\"content_quality\") for r in tier2_results[stage]) else 0.0,\n",
    "                },\n",
    "                \"answer_correctness\": accuracy_metrics[stage],\n",
    "            }\n",
    "            for stage in STAGES_TO_EVALUATE\n",
    "        },\n",
    "    }\n",
    "\n",
    "final_results = compile_final_results()\n",
    "\n",
    "# Save to JSON\n",
    "results_file = OUTPUT_DIR / f\"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(results_file, \"w\") as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✓ Results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Generate Markdown report (double-click to show code)\n",
    "# Generate Markdown report\n",
    "def generate_markdown_report(results: Dict[str, Any]) -> str:\n",
    "    \"\"\"Generate a comprehensive Markdown evaluation report.\"\"\"\n",
    "    md_lines = [\n",
    "        \"# Pramana Evaluation Report\",\n",
    "        \"\",\n",
    "        f\"**Generated:** {results['evaluation_metadata']['timestamp']}\",\n",
    "        f\"**Validation Directory:** {results['evaluation_metadata']['validation_dir']}\",\n",
    "        f\"**Stages Evaluated:** {', '.join(results['evaluation_metadata']['stages_evaluated'])}\",\n",
    "        f\"**Number of Examples:** {results['evaluation_metadata']['num_examples']}\",\n",
    "        \"\",\n",
    "        \"## Summary Metrics\",\n",
    "        \"\",\n",
    "    ]\n",
    "    \n",
    "    # Summary table\n",
    "    md_lines.extend([\n",
    "        \"| Stage | Parse Rate | Tier 1 Pass Rate | Avg Content Quality | Exact Match Rate | Semantic Match Rate |\",\n",
    "        \"|-------|------------|------------------|---------------------|------------------|---------------------|\",\n",
    "    ])\n",
    "    \n",
    "    for stage in results['evaluation_metadata']['stages_evaluated']:\n",
    "        summary = results['summary_metrics'][stage]\n",
    "        format_metrics = summary['format_adherence']\n",
    "        quality_metrics = summary['content_quality']\n",
    "        answer_metrics = summary['answer_correctness']\n",
    "        \n",
    "        md_lines.append(\n",
    "            f\"| {stage} | \"\n",
    "            f\"{format_metrics['parse_rate']*100:.1f}% | \"\n",
    "            f\"{format_metrics['tier1_pass_rate']*100:.1f}% | \"\n",
    "            f\"{quality_metrics['avg_overall_score']:.3f} | \"\n",
    "            f\"{answer_metrics['exact_rate']*100:.1f}% | \"\n",
    "            f\"{answer_metrics['semantic_rate']*100:.1f}% |\"\n",
    "        )\n",
    "    \n",
    "    md_lines.extend([\"\", \"## Detailed Results\", \"\",])\n",
    "    \n",
    "    for stage in results['evaluation_metadata']['stages_evaluated']:\n",
    "        md_lines.extend([\n",
    "            f\"### {stage}\",\n",
    "            \"\",\n",
    "            f\"**Format Adherence:**\",\n",
    "            f\"- Parse Success Rate: {results['summary_metrics'][stage]['format_adherence']['parse_rate']*100:.1f}%\",\n",
    "            f\"- Tier 1 Pass Rate: {results['summary_metrics'][stage]['format_adherence']['tier1_pass_rate']*100:.1f}%\",\n",
    "            \"\",\n",
    "            f\"**Content Quality:**\",\n",
    "            f\"- Average Overall Score: {results['summary_metrics'][stage]['content_quality']['avg_overall_score']:.3f}\",\n",
    "            \"\",\n",
    "            f\"**Answer Correctness:**\",\n",
    "            f\"- Exact Match Rate: {results['summary_metrics'][stage]['answer_correctness']['exact_rate']*100:.1f}%\",\n",
    "            f\"- Semantic Match Rate: {results['summary_metrics'][stage]['answer_correctness']['semantic_rate']*100:.1f}%\",\n",
    "            f\"- Average Semantic Similarity: {results['summary_metrics'][stage]['answer_correctness']['avg_semantic_similarity']:.3f}\",\n",
    "            \"\",\n",
    "        ])\n",
    "    \n",
    "    return \"\\n\".join(md_lines)\n",
    "\n",
    "markdown_report = generate_markdown_report(final_results)\n",
    "\n",
    "# Save Markdown report\n",
    "report_file = OUTPUT_DIR / f\"evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "report_file.write_text(markdown_report, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✓ Markdown report saved to {report_file}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Report Preview:\")\n",
    "print(\"=\"*80)\n",
    "print(markdown_report[:1000] + \"...\" if len(markdown_report) > 1000 else markdown_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}