{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pramana Explorer: Interactive Navya-Nyaya Reasoning Demo\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SharathSPhD/pramana/blob/main/notebooks/01_pramana_explorer.ipynb#slideshowMode=true)\n",
        "\n",
        "This notebook provides an interactive exploration of the Pramana epistemic reasoning engine, demonstrating the 6-phase Navya-Nyaya methodology for systematic logical problem-solving.\n",
        "\n",
        "**Presentation:** Most code cells are collapsed by default (double-click a cell title to show code). Interactive controls (configuration, generation, exercises) remain visible for easy use.\n",
        "\n",
        "> **Tip:** If any controls appear as `VBox(...)` or a thin line, click **View → Exit slideshow**, then **Runtime → Run all** to re-render widgets in normal view."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning repository...\n",
            "  Done.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style='padding:10px;border-radius:8px;border:2px solid green;margin:8px 0'><span style='font-size:1.2em'>Runtime: <span style='color:green'><b>GPU: Tesla T4 (15360 MB)</b></span></span><br>Environment: Google Colab</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working directory: /content/pramana\n"
          ]
        }
      ],
      "source": [
        "#@title Setup: Clone repo, install deps, detect GPU\n",
        "import os, sys, subprocess, importlib\n",
        "from pathlib import Path\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "REPO_URL = \"https://github.com/SharathSPhD/pramana.git\"\n",
        "REPO_DIR = \"pramana\"\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    if not Path(REPO_DIR).exists():\n",
        "        print(\"Cloning repository...\")\n",
        "        subprocess.run([\"git\", \"clone\", REPO_URL], check=True, capture_output=True)\n",
        "        print(\"  Done.\")\n",
        "    os.chdir(REPO_DIR)\n",
        "\n",
        "    # Check ipywidgets version BEFORE installing deps\n",
        "    import importlib.metadata as _meta\n",
        "    try:\n",
        "        _old_ver = _meta.version(\"ipywidgets\")\n",
        "    except _meta.PackageNotFoundError:\n",
        "        _old_ver = \"0\"\n",
        "\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "                    \"-r\", \"notebooks/requirements.txt\"],\n",
        "                   check=True, capture_output=True)\n",
        "\n",
        "    # Re-read version after install (invalidate cache)\n",
        "    importlib.invalidate_caches()\n",
        "    try:\n",
        "        _new_ver = _meta.version(\"ipywidgets\")\n",
        "    except Exception:\n",
        "        _new_ver = _old_ver\n",
        "\n",
        "    # If ipywidgets was downgraded (8.x -> 7.x), kernel restart is required\n",
        "    # for Colab to load the correct JS widget bundles.\n",
        "    if _old_ver.startswith(\"8\") and not _new_ver.startswith(\"8\"):\n",
        "        print(\"\\u2699 ipywidgets downgraded for Colab compatibility. Restarting runtime...\")\n",
        "        print(\"After restart, click Runtime \\u2192 Run all to continue.\")\n",
        "        import signal\n",
        "        os.kill(os.getpid(), signal.SIGKILL)\n",
        "\n",
        "    sys.path.insert(0, \"notebooks\")\n",
        "\n",
        "    # Enable third-party widget rendering in Colab\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "else:\n",
        "    # Local: pramana_backend.py should be in the same dir or parent\n",
        "    if Path(\"pramana_backend.py\").exists():\n",
        "        sys.path.insert(0, \".\")\n",
        "    elif Path(\"notebooks/pramana_backend.py\").exists():\n",
        "        sys.path.insert(0, \"notebooks\")\n",
        "    else:\n",
        "        for p in [Path(\"..\"), Path(\"../notebooks\")]:\n",
        "            if (p / \"pramana_backend.py\").exists():\n",
        "                sys.path.insert(0, str(p.resolve()))\n",
        "                break\n",
        "\n",
        "# ---- GPU Detection ----\n",
        "def check_backend():\n",
        "    \"\"\"Detect GPU/CPU and display status banner.\"\"\"\n",
        "    gpu_name, gpu_mem = None, None\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader,nounits\"],\n",
        "            capture_output=True, text=True, timeout=5,\n",
        "        )\n",
        "        if result.returncode == 0 and result.stdout.strip():\n",
        "            parts = result.stdout.strip().split(\", \")\n",
        "            gpu_name = parts[0]\n",
        "            gpu_mem = parts[1] if len(parts) > 1 else \"?\"\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if gpu_name:\n",
        "        hw = f\"<b>GPU: {gpu_name} ({gpu_mem} MB)</b>\"\n",
        "        color = \"green\"\n",
        "    else:\n",
        "        hw = \"<b>CPU</b>\"\n",
        "        color = \"orange\"\n",
        "\n",
        "    env = \"Google Colab\" if IN_COLAB else \"Local\"\n",
        "    banner = (\n",
        "        f\"<div style='padding:10px;border-radius:8px;border:2px solid {color};margin:8px 0'>\"\n",
        "        f\"<span style='font-size:1.2em'>Runtime: <span style='color:{color}'>{hw}</span></span>\"\n",
        "        f\"<br>Environment: {env}\"\n",
        "    )\n",
        "    if not gpu_name:\n",
        "        banner += \"<br><i>For better performance: Runtime \\u2192 Change runtime type \\u2192 GPU</i>\"\n",
        "    banner += \"</div>\"\n",
        "    display(HTML(banner))\n",
        "    return gpu_name is not None\n",
        "\n",
        "GPU_AVAILABLE = check_backend()\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ All modules imported\n"
          ]
        }
      ],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interactive, fixed, HBox, VBox, Output\n",
        "\n",
        "from pramana_backend import (\n",
        "    create_backend,\n",
        "    STAGE_CONFIGS,\n",
        "    OLLAMA_MODEL_MAP,\n",
        "    build_user_prompt,\n",
        "    EXAMPLE_PROBLEMS,\n",
        "    load_test_problems,\n",
        "    parse_nyaya_phases,\n",
        "    validate_structure as backend_validate_structure,\n",
        "    score_content_quality as backend_score_content_quality,\n",
        "    extract_final_answer,\n",
        "    score_answers,\n",
        "    setup_ollama,\n",
        "    setup_ollama_stage,\n",
        "    download_gguf,\n",
        ")\n",
        "\n",
        "print(\"\\u2713 All modules imported\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e754b70842384891aef7778f7f7cb0dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<h3>Configuration</h3>'), Dropdown(description='Backend:', index=1, options=(('Tran…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "_gpu_label = \"Colab GPU\" if IN_COLAB else \"Local GPU/CPU\"\n",
        "backend_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        (f\"Transformers ({_gpu_label})\", \"transformers\"),\n",
        "        (f\"Ollama ({_gpu_label})\", \"ollama\"),\n",
        "        (f\"llama.cpp GGUF ({_gpu_label})\", \"llamacpp\"),\n",
        "    ],\n",
        "    value=\"ollama\" if IN_COLAB else \"transformers\",\n",
        "    description=\"Backend:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "stage_dropdown = widgets.Dropdown(\n",
        "    options=list(STAGE_CONFIGS.keys()),\n",
        "    value=\"Stage 0\",\n",
        "    description=\"Stage:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "model_variant_dropdown = widgets.Dropdown(\n",
        "    options=[(\"Tuned model\", \"tuned\"), (\"Base model\", \"base\"), (\"Both (comparison)\", \"both\")],\n",
        "    value=\"both\",\n",
        "    description=\"Model:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "# System prompt -- editable, auto-populated from stage config\n",
        "_initial_stage = STAGE_CONFIGS[\"Stage 0\"]\n",
        "system_prompt_textarea = widgets.Textarea(\n",
        "    value=_initial_stage.system_prompt,\n",
        "    description=\"System Prompt:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        "    layout=widgets.Layout(width=\"95%\", height=\"120px\"),\n",
        ")\n",
        "\n",
        "# ---------- Ollama backend-specific settings ----------\n",
        "_stage0_map = OLLAMA_MODEL_MAP[\"Stage 0\"]\n",
        "ollama_base_model_input = widgets.Text(\n",
        "    value=_stage0_map[\"base\"],\n",
        "    placeholder=\"Ollama base model tag\",\n",
        "    description=\"Base model:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        "    layout=widgets.Layout(width=\"95%\"),\n",
        ")\n",
        "ollama_tuned_model_input = widgets.Text(\n",
        "    value=_stage0_map[\"tuned\"],\n",
        "    placeholder=\"Ollama tuned model tag\",\n",
        "    description=\"Tuned model:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        "    layout=widgets.Layout(width=\"95%\"),\n",
        ")\n",
        "ollama_url_input = widgets.Text(\n",
        "    value=\"http://localhost:11434\",\n",
        "    description=\"Ollama URL:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        "    layout=widgets.Layout(width=\"95%\"),\n",
        ")\n",
        "\n",
        "# llama.cpp\n",
        "gguf_path_input = widgets.Text(\n",
        "    value=\"\",\n",
        "    placeholder=\"Path to .gguf file (or llama.cpp server URL)\",\n",
        "    description=\"GGUF path/URL:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        "    layout=widgets.Layout(width=\"95%\"),\n",
        ")\n",
        "\n",
        "# ---------- Stage observer ----------\n",
        "def _on_stage_change(change):\n",
        "    stage = change[\"new\"]\n",
        "    stage_cfg = STAGE_CONFIGS[stage]\n",
        "    system_prompt_textarea.value = stage_cfg.system_prompt\n",
        "    if stage in OLLAMA_MODEL_MAP:\n",
        "        ollama_base_model_input.value = OLLAMA_MODEL_MAP[stage][\"base\"]\n",
        "        ollama_tuned_model_input.value = OLLAMA_MODEL_MAP[stage][\"tuned\"]\n",
        "\n",
        "stage_dropdown.observe(_on_stage_change, names=\"value\")\n",
        "\n",
        "# ---------- Backend settings panel ----------\n",
        "backend_settings_output = Output()\n",
        "\n",
        "def update_backend_settings(change=None):\n",
        "    with backend_settings_output:\n",
        "        backend_settings_output.clear_output()\n",
        "        bt = backend_dropdown.value\n",
        "        if bt == \"transformers\":\n",
        "            display(widgets.HTML(\"<i>Uses HuggingFace model IDs from stage config. Requires GPU.</i>\"))\n",
        "        elif bt == \"ollama\":\n",
        "            ollama_setup_btn = widgets.Button(description=\"Auto-Setup Ollama (base + tuned)\", button_style=\"info\", icon=\"magic\")\n",
        "            ollama_setup_out = Output()\n",
        "            def _ollama_auto(b):\n",
        "                with ollama_setup_out:\n",
        "                    ollama_setup_out.clear_output()\n",
        "                    try:\n",
        "                        stage = stage_dropdown.value\n",
        "                        print(f\"Setting up Ollama models for {stage}...\")\n",
        "                        result = setup_ollama_stage(stage)\n",
        "                        ollama_base_model_input.value = result[\"base\"] or ollama_base_model_input.value\n",
        "                        if result[\"tuned\"]:\n",
        "                            ollama_tuned_model_input.value = result[\"tuned\"]\n",
        "                        print(f\"\\n\\u2713 Setup complete for {stage}\")\n",
        "                        print(f\"  Base:  {result['base']}\")\n",
        "                        print(f\"  Tuned: {result['tuned'] or '(not available)'}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"\\u2717 Setup failed: {e}\")\n",
        "                        import traceback; traceback.print_exc()\n",
        "            ollama_setup_btn.on_click(_ollama_auto)\n",
        "            display(VBox([\n",
        "                widgets.HTML(\n",
        "                    f\"<b>Ollama Settings</b> \\u2014 \"\n",
        "                    \"Click <b>Auto-Setup</b> to install Ollama + pull/create models for the selected stage.\"\n",
        "                ),\n",
        "                ollama_base_model_input,\n",
        "                ollama_tuned_model_input,\n",
        "                ollama_url_input,\n",
        "                ollama_setup_btn,\n",
        "                ollama_setup_out,\n",
        "            ]))\n",
        "        elif bt == \"llamacpp\":\n",
        "            gguf_setup_btn = widgets.Button(description=\"Auto-Download GGUF\", button_style=\"info\", icon=\"download\")\n",
        "            gguf_setup_out = Output()\n",
        "            def _gguf_auto(b):\n",
        "                with gguf_setup_out:\n",
        "                    gguf_setup_out.clear_output()\n",
        "                    try:\n",
        "                        path = download_gguf()\n",
        "                        gguf_path_input.value = path\n",
        "                        print(f\"\\u2713 GGUF downloaded: {path}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"\\u2717 Download failed: {e}\")\n",
        "            gguf_setup_btn.on_click(_gguf_auto)\n",
        "            display(VBox([\n",
        "                widgets.HTML(\"<b>llama.cpp Settings</b> \\u2014 Click Auto-Download or provide path to GGUF\"),\n",
        "                gguf_path_input,\n",
        "                gguf_setup_btn,\n",
        "                gguf_setup_out,\n",
        "            ]))\n",
        "\n",
        "backend_dropdown.observe(update_backend_settings, names='value')\n",
        "\n",
        "# ---------- Configuration apply ----------\n",
        "def setup_config():\n",
        "    global config\n",
        "    backend_type = backend_dropdown.value\n",
        "    stage_name = stage_dropdown.value\n",
        "    variant = model_variant_dropdown.value\n",
        "    stage_config = STAGE_CONFIGS[stage_name]\n",
        "\n",
        "    def _make_backend(model_id, role=\"tuned\"):\n",
        "        if backend_type == \"transformers\":\n",
        "            return create_backend(\"transformers\", model_id=model_id)\n",
        "        elif backend_type == \"ollama\":\n",
        "            if role == \"base\":\n",
        "                model_name = ollama_base_model_input.value.strip()\n",
        "            else:\n",
        "                model_name = ollama_tuned_model_input.value.strip()\n",
        "            return create_backend(\"ollama\", model_name=model_name,\n",
        "                                  base_url=ollama_url_input.value.strip())\n",
        "        elif backend_type == \"llamacpp\":\n",
        "            path_or_url = gguf_path_input.value.strip()\n",
        "            if path_or_url.startswith(\"http\"):\n",
        "                return create_backend(\"llamacpp\", server_url=path_or_url)\n",
        "            else:\n",
        "                return create_backend(\"llamacpp\", model_path=path_or_url)\n",
        "\n",
        "    try:\n",
        "        base_backend = _make_backend(stage_config.base_model_id, role=\"base\") if variant in (\"base\", \"both\") else None\n",
        "        tuned_backend = _make_backend(stage_config.tuned_model_id, role=\"tuned\") if variant in (\"tuned\", \"both\") else None\n",
        "\n",
        "        if backend_type == \"ollama\":\n",
        "            base_label = ollama_base_model_input.value.strip() if base_backend else None\n",
        "            tuned_label = ollama_tuned_model_input.value.strip() if tuned_backend else None\n",
        "        else:\n",
        "            base_label = stage_config.base_model_id if base_backend else None\n",
        "            tuned_label = stage_config.tuned_model_id if tuned_backend else None\n",
        "\n",
        "        active_system_prompt = system_prompt_textarea.value.strip() or stage_config.system_prompt\n",
        "\n",
        "        config = {\n",
        "            \"backend_type\": backend_type,\n",
        "            \"stage_config\": stage_config,\n",
        "            \"base_backend\": base_backend,\n",
        "            \"tuned_backend\": tuned_backend,\n",
        "            \"base_label\": base_label,\n",
        "            \"tuned_label\": tuned_label,\n",
        "            \"system_prompt\": active_system_prompt,\n",
        "        }\n",
        "        print(f\"\\u2713 Configuration set: {stage_name} with {backend_type} backend\")\n",
        "        if base_backend:\n",
        "            print(f\"  Base model ready: {base_label}\")\n",
        "        if tuned_backend:\n",
        "            print(f\"  Tuned model ready: {tuned_label}\")\n",
        "        return config\n",
        "    except Exception as e:\n",
        "        print(f\"\\u2717 Configuration failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "setup_button = widgets.Button(description=\"Apply Configuration\", button_style=\"primary\", icon=\"check\")\n",
        "config_output = Output()\n",
        "\n",
        "def on_setup_click(button):\n",
        "    with config_output:\n",
        "        config_output.clear_output()\n",
        "        setup_config()\n",
        "\n",
        "setup_button.on_click(on_setup_click)\n",
        "update_backend_settings()\n",
        "\n",
        "display(VBox([\n",
        "    widgets.HTML(\"<h3>Configuration</h3>\"),\n",
        "    backend_dropdown,\n",
        "    stage_dropdown,\n",
        "    model_variant_dropdown,\n",
        "    widgets.HTML(\"<b>System Prompt</b> (auto-populated from stage; editable):\"),\n",
        "    system_prompt_textarea,\n",
        "    backend_settings_output,\n",
        "    setup_button,\n",
        "    config_output,\n",
        "]))\n",
        "\n",
        "config = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Introduction to Navya-Nyaya Reasoning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What is Navya-Nyaya?\n",
        "\n",
        "Navya-Nyaya (\"New Logic\") is a 2,500-year-old Indian epistemological system that provides a structured methodology for systematic reasoning. Unlike Western formal logic, Navya-Nyaya integrates logic and epistemology, requiring explicit grounding in concrete examples and universal rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The 6-Phase Framework\n",
        "\n",
        "Pramana enforces a structured 6-phase Nyaya methodology:\n",
        "\n",
        "1. **Samshaya (Doubt Analysis)** - Classify the type of uncertainty/ambiguity\n",
        "2. **Pramana (Evidence Sources)** - Identify valid knowledge sources:\n",
        "   - Pratyaksha (Direct Perception)\n",
        "   - Anumana (Inference)\n",
        "   - Upamana (Comparison)\n",
        "   - Shabda (Testimony)\n",
        "3. **Pancha Avayava (5-Member Syllogism)** - Construct formal argument with:\n",
        "   - Pratijna (Thesis)\n",
        "   - Hetu (Reason)\n",
        "   - Udaharana (Universal Example)\n",
        "   - Upanaya (Application)\n",
        "   - Nigamana (Conclusion)\n",
        "4. **Tarka (Counterfactual Testing)** - Use reductio ad absurdum to verify conclusions\n",
        "5. **Hetvabhasa (Fallacy Detection)** - Check for reasoning errors\n",
        "6. **Nirnaya (Ascertainment)** - Reach definitive conclusion or explicitly state insufficient evidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Example Problem: pramana-001\n",
              "\n",
              "Three people (Alice, Bob, Carol) each have one pet: a cat, a dog, or a fish.\n",
              "\n",
              "**Constraints**:\n",
              "1. Alice does not have the cat\n",
              "2. Bob has the dog\n",
              "3. Carol does not have the fish\n",
              "\n",
              "**Question**: Who has which pet?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Ground Truth:** Alice has the fish, Bob has the dog, Carol has the cat"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Problem Type:** constraint_satisfaction | **Difficulty:** medium"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Load example from embedded data (no external files needed) (double-click to show code)\n",
        "# Load example from embedded data (no external files needed)\n",
        "example = EXAMPLE_PROBLEMS[0]  # pramana-001: constraint satisfaction\n",
        "\n",
        "display(Markdown(f\"## Example Problem: {example['id']}\\n\\n{example['problem']}\"))\n",
        "display(Markdown(f\"**Ground Truth:** {example['ground_truth']}\"))\n",
        "display(Markdown(f\"**Problem Type:** {example['problem_type']} | **Difficulty:** {example['difficulty']}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phase 1: Samshaya (Doubt Analysis)\n",
        "\n",
        "The example demonstrates **Samana Dharma Upapatti** (Multiple possibilities share similar properties):\n",
        "\n",
        "- There are three people and three pets, creating multiple possible assignments\n",
        "- Without systematic reasoning, we cannot determine which person has which pet\n",
        "- The doubt arises because multiple arrangements are conceivable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phase 2: Pramana (Sources of Knowledge)\n",
        "\n",
        "The example identifies:\n",
        "\n",
        "- **Pratyaksha**: Directly stated constraints (\"Alice does not have the cat\", \"Bob has the dog\", etc.)\n",
        "- **Anumana**: Inferences like \"If Bob has the dog, then neither Alice nor Carol has the dog\"\n",
        "- **Upamana**: Comparison to constraint satisfaction problems with mutual exclusivity\n",
        "- **Shabda**: Logical principles (Law of Excluded Middle, Non-Contradiction, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phase 3: Pancha Avayava (5-Member Syllogism)\n",
        "\n",
        "The example constructs three syllogisms:\n",
        "\n",
        "1. **Establishing Bob's Pet**: Pratijna=\"Bob has the dog\", supported by direct constraint\n",
        "2. **Establishing Alice's Pet**: Pratijna=\"Alice has the fish\", via elimination (cannot have cat or dog)\n",
        "3. **Establishing Carol's Pet**: Pratijna=\"Carol has the cat\", via completeness principle\n",
        "\n",
        "Each syllogism includes all 5 members with explicit Udaharana (universal rules) and Upanaya (application)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phase 4: Tarka (Counterfactual Testing)\n",
        "\n",
        "The example uses reductio ad absurdum:\n",
        "\n",
        "- **Hypothesis**: \"Suppose Carol does not have the cat\"\n",
        "- **Consequence**: This leads to Carol having no pet, violating completeness\n",
        "- **Analysis**: This is absurd given the problem constraints\n",
        "- **Resolution**: Therefore, Carol must have the cat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phase 5: Hetvabhasa (Fallacy Check)\n",
        "\n",
        "The example checks for:\n",
        "\n",
        "- **Savyabhichara** (Erratic reasoning): None detected\n",
        "- **Viruddha** (Contradictory reasoning): None detected\n",
        "- **Prakaranasama** (Circular reasoning): None detected\n",
        "- **Sadhyasama** (Begging the question): None detected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phase 6: Nirnaya (Ascertainment)\n",
        "\n",
        "**Status**: Definitive Knowledge\n",
        "\n",
        "**Final Answer**: Alice has the fish, Bob has the dog, and Carol has the cat.\n",
        "\n",
        "**Confidence**: High - The solution is logically necessary given the constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Interactive Comparison: Base vs Tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Loaded 5 embedded examples\n",
            "  1. [pramana-001] constraint_satisfaction (medium)\n",
            "  2. [pramana-003] transitive_reasoning (medium)\n",
            "  3. [pramana-005] multi_step_deduction (medium)\n",
            "  4. [test-001] constraint_satisfaction (easy)\n",
            "  5. [test-004] boolean_sat (medium)\n"
          ]
        }
      ],
      "source": [
        "#@title Load example problems (self-contained, no external files needed) (double-click to show code)\n",
        "# Load example problems (self-contained, no external files needed)\n",
        "examples = load_test_problems(\"embedded\")\n",
        "print(f\"✓ Loaded {len(examples)} embedded examples\")\n",
        "\n",
        "# Display problem list\n",
        "for i, ex in enumerate(examples):\n",
        "    print(f\"  {i+1}. [{ex['id']}] {ex['problem_type']} ({ex['difficulty']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fa90b749c0141348e90cf508044ed51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<h3>Generation Controls</h3>'), Dropdown(description='Example:', options=(('pramana…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "example_selector = widgets.Dropdown(\n",
        "    options=[(ex[\"id\"], idx) for idx, ex in enumerate(examples)],\n",
        "    description=\"Example:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "# --- Example preview area ---\n",
        "example_preview_output = Output()\n",
        "\n",
        "def _on_example_select(change):\n",
        "    \"\"\"Display the selected problem text and ground truth.\"\"\"\n",
        "    with example_preview_output:\n",
        "        example_preview_output.clear_output()\n",
        "        idx = change[\"new\"]\n",
        "        ex = examples[idx]\n",
        "        display(HTML(f\"\"\"\n",
        "        <div style=\"border:1px solid #ddd; padding:12px; border-radius:6px; background:#f9f9f9; margin:4px 0;\">\n",
        "            <b>Problem:</b>\n",
        "            <pre style=\"white-space: pre-wrap; margin:6px 0;\">{ex[\"problem\"]}</pre>\n",
        "            <b>Expected Answer:</b> <code>{ex.get(\"expected_answer\", \"N/A\")}</code>\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "\n",
        "example_selector.observe(_on_example_select, names=\"value\")\n",
        "# Show initial selection immediately\n",
        "_on_example_select({\"new\": example_selector.value})\n",
        "\n",
        "# --- Hyperparameter controls (matching HF Space app) ---\n",
        "max_tokens_slider = widgets.IntSlider(\n",
        "    value=2048,\n",
        "    min=64,\n",
        "    max=4096,\n",
        "    step=32,\n",
        "    description=\"Max new tokens:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        "    layout=widgets.Layout(width=\"95%\"),\n",
        ")\n",
        "\n",
        "temperature_slider = widgets.FloatSlider(\n",
        "    value=0.5,\n",
        "    min=0.0,\n",
        "    max=1.5,\n",
        "    step=0.05,\n",
        "    description=\"Temperature:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        "    layout=widgets.Layout(width=\"95%\"),\n",
        ")\n",
        "\n",
        "top_p_slider = widgets.FloatSlider(\n",
        "    value=0.75,\n",
        "    min=0.0,\n",
        "    max=1.0,\n",
        "    step=0.05,\n",
        "    description=\"Top-p:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        "    layout=widgets.Layout(width=\"95%\"),\n",
        ")\n",
        "\n",
        "top_k_slider = widgets.IntSlider(\n",
        "    value=5,\n",
        "    min=0,\n",
        "    max=200,\n",
        "    step=5,\n",
        "    description=\"Top-k:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        "    layout=widgets.Layout(width=\"95%\"),\n",
        ")\n",
        "\n",
        "generate_button = widgets.Button(\n",
        "    description=\"Generate\",\n",
        "    button_style=\"primary\",\n",
        "    icon=\"play\",\n",
        ")\n",
        "\n",
        "output_area = Output()\n",
        "\n",
        "def generate_comparison(button):\n",
        "    \"\"\"Generate outputs from both models and display side-by-side.\"\"\"\n",
        "    if config is None:\n",
        "        with output_area:\n",
        "            print(\"\\u26a0 Please configure backend and stage first\")\n",
        "        return\n",
        "    \n",
        "    with output_area:\n",
        "        output_area.clear_output()\n",
        "        \n",
        "        idx = example_selector.value\n",
        "        example = examples[idx]\n",
        "        \n",
        "        print(f\"Generating for: {example['id']}\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        # Build prompt\n",
        "        user_prompt = build_user_prompt(example[\"problem\"])\n",
        "        \n",
        "        # Use the user-editable system prompt from config\n",
        "        active_system_prompt = config.get(\"system_prompt\", config[\"stage_config\"].system_prompt)\n",
        "        \n",
        "        # Generation parameters\n",
        "        gen_kwargs = dict(\n",
        "            system_prompt=active_system_prompt,\n",
        "            max_new_tokens=max_tokens_slider.value,\n",
        "            temperature=temperature_slider.value,\n",
        "            top_p=top_p_slider.value,\n",
        "            top_k=top_k_slider.value,\n",
        "        )\n",
        "        \n",
        "        # Resolve display labels from config\n",
        "        base_label = config.get(\"base_label\", config[\"stage_config\"].base_model_id)\n",
        "        tuned_label = config.get(\"tuned_label\", config[\"stage_config\"].tuned_model_id)\n",
        "        \n",
        "        # Generate from base model (if configured)\n",
        "        base_output = None\n",
        "        if config.get(\"base_backend\"):\n",
        "            print(f\"\\n[Base Model: {base_label}] Generating (max_tokens={gen_kwargs['max_new_tokens']}, \"\n",
        "                  f\"temp={gen_kwargs['temperature']}, top_p={gen_kwargs['top_p']}, top_k={gen_kwargs['top_k']})...\")\n",
        "            try:\n",
        "                base_output = config[\"base_backend\"].generate(user_prompt, **gen_kwargs)\n",
        "            except Exception as e:\n",
        "                base_output = f\"Error: {e}\"\n",
        "        \n",
        "        # Generate from tuned model (if configured)\n",
        "        tuned_output = None\n",
        "        if config.get(\"tuned_backend\"):\n",
        "            print(f\"[Tuned Model: {tuned_label}] Generating...\")\n",
        "            try:\n",
        "                tuned_output = config[\"tuned_backend\"].generate(user_prompt, **gen_kwargs)\n",
        "            except Exception as e:\n",
        "                tuned_output = f\"Error: {e}\"\n",
        "        \n",
        "        # Display side-by-side (or single panel if only one model)\n",
        "        display(HTML(\"\"\"\n",
        "        <style>\n",
        "        .comparison-container { display: flex; gap: 20px; }\n",
        "        .model-output { flex: 1; border: 1px solid #ccc; padding: 10px; border-radius: 5px; }\n",
        "        .base-model { background-color: #fff3cd; }\n",
        "        .tuned-model { background-color: #d1ecf1; }\n",
        "        </style>\n",
        "        \"\"\"))\n",
        "        \n",
        "        parts = []\n",
        "        if base_output is not None:\n",
        "            parts.append(f\"\"\"\n",
        "            <div class=\"model-output base-model\">\n",
        "                <h3>Base Model ({base_label})</h3>\n",
        "                <pre style=\"white-space: pre-wrap;\">{base_output}</pre>\n",
        "            </div>\"\"\")\n",
        "        if tuned_output is not None:\n",
        "            parts.append(f\"\"\"\n",
        "            <div class=\"model-output tuned-model\">\n",
        "                <h3>Tuned Model ({tuned_label})</h3>\n",
        "                <pre style=\"white-space: pre-wrap;\">{tuned_output}</pre>\n",
        "            </div>\"\"\")\n",
        "        \n",
        "        if parts:\n",
        "            display(HTML(f'<div class=\"comparison-container\">{\"\".join(parts)}</div>'))\n",
        "        else:\n",
        "            display(HTML(\"<p style='color:orange;'>\\u26a0 No models configured for comparison.</p>\"))\n",
        "        \n",
        "        # Store outputs for analysis (use empty string if model was not run)\n",
        "        global last_base_output, last_tuned_output\n",
        "        last_base_output = base_output or \"\"\n",
        "        last_tuned_output = tuned_output or \"\"\n",
        "\n",
        "generate_button.on_click(generate_comparison)\n",
        "\n",
        "display(VBox([\n",
        "    widgets.HTML(\"<h3>Generation Controls</h3>\"),\n",
        "    example_selector,\n",
        "    example_preview_output,\n",
        "    widgets.HTML(\"<b>Hyperparameters</b> (adjust for your hardware \\u2014 larger tokens = longer output):\"),\n",
        "    max_tokens_slider,\n",
        "    temperature_slider,\n",
        "    top_p_slider,\n",
        "    top_k_slider,\n",
        "    generate_button,\n",
        "    output_area,\n",
        "]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h3>Base Model Output (Highlighted)</h3><pre><span style=\"background-color: #ffeb3b; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Samshaya (Doubt Analysis)\n",
              "</span>**Doubt Type**: Uncertainty about pet ownership\n",
              "**Justification**: Lack of direct information about each person's pet, with only indirect clues and constraints.\n",
              "\n",
              "---\n",
              "\n",
              "<span style=\"background-color: #4caf50; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Pramana (Sources of Knowledge)\n",
              "</span>### Pratyaksha (Direct Perception)\n",
              "- Carol does not have the fish\n",
              "\n",
              "### Anumana (Inference)\n",
              "- From constraint 1: Alice does not have the cat\n",
              "- From constraint 3: Carol does not have the fish\n",
              "- Conclusion: Alice has the cat and Carol has a pet other than the fish.\n",
              "\n",
              "### Upamana (Comparison)\n",
              "- By comparing constraints 2 and 1, we can infer that Bob has the dog and Alice has the cat.\n",
              "\n",
              "### Shabda (Testimony)\n",
              "- No direct testimony provided\n",
              "\n",
              "---\n",
              "\n",
              "<span style=\"background-color: #2196f3; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Pancha Avayava (5-Member Syllogism)\n",
              "</span>### Syllogism 1: \n",
              "**Pratijna (Thesis)**: Carol does not have the fish.\n",
              "**Hetu (Reason)**: If Alice has the cat, then she cannot have the fish.\n",
              "**Udaharana (Universal + Example)**: If A has B, then A cannot have C.\n",
              "**Upanaya (Application)**: Therefore, if Alice has the cat, Carol does not have the fish.\n",
              "**Nigamana (Conclusion)**: Alice has the cat and Carol does not have the fish.\n",
              "\n",
              "---\n",
              "\n",
              "<span style=\"background-color: #ff9800; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Tarka (Counterfactual Reasoning)\n",
              "</span>**Hypothesis**: What if Alice did not have the cat?\n",
              "**Consequence**: Then Carol would have the fish, which contradicts constraint 3.\n",
              "**Analysis**: This analysis confirms that Alice indeed has the cat.\n",
              "**Resolution**: The hypothesis is false.\n",
              "\n",
              "---\n",
              "\n",
              "<span style=\"background-color: #9c27b0; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Hetvabhasa (Fallacy Check)\n",
              "</span>Check for Savyabhichara: No\n",
              "Check for Viruddha: No\n",
              "Check for Asiddha: No\n",
              "Check for Satpratipaksha: No\n",
              "Check for Badhita: No\n",
              "\n",
              "---\n",
              "\n",
              "<span style=\"background-color: #f44336; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Nirnaya (Ascertainment)\n",
              "</span>**Final Answer**: Alice has the cat, Bob has the dog, and Carol has a fish.\n",
              "**Justification**: Based on the logical analysis of constraints and syllogisms.\n",
              "**Confidence**: High</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Tuned Model Output (Highlighted)</h3><pre>Let's start with the doubt analysis.\n",
              "\n",
              "\n",
              "<span style=\"background-color: #ffeb3b; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Samshaya (Doubt Analysis)\n",
              "</span>\n",
              "**Doubt Type**: Savyabhichara\n",
              "**Justification**: We need to figure out who has which pet, but we have conflicting information about Alice and Carol.\n",
              "\n",
              "---\n",
              "\n",
              "<span style=\"background-color: #4caf50; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Pramana (Sources of Knowledge)\n",
              "</span>\n",
              "### Pratyaksha (Direct Perception)\n",
              "- **Pratijna (Thesis)**: I know that Bob has the dog.\n",
              "- **Hetu (Reason)**: This is a direct statement from the problem.\n",
              "- **Udaharana (Universal + Example)**: If Bob says he has the dog, it's true.\n",
              "- **Upanaya (Application)**: In this case, Bob does have the dog.\n",
              "- **Nigamana (Conclusion)**: Therefore, Bob has the dog.\n",
              "\n",
              "### Anumana (Inference)\n",
              "- **Pratijna (Thesis)**: I can infer that Alice does not have the cat.\n",
              "- **Hetu (Reason)**: If Alice had the cat, it would contradict Pratyaksha 1. \n",
              "- **Udaharana (Universal + Example)**: If Alice had the cat, she would be lying about having a pet.\n",
              "- **Upanaya (Application)**: In this case, Alice does not have the cat.\n",
              "- **Nigamana (Conclusion)**: Therefore, Alice does not have the cat.\n",
              "\n",
              "### Upamana (Comparison)\n",
              "- **Pratijna (Thesis)**: I can compare Carol's situation to Alice's and conclude that Carol has a pet different from Alice's.\n",
              "- **Hetu (Reason)**: If Carol had the same pet as Alice, it would be contradictory. \n",
              "- **Udaharana (Universal + Example)**: Just like how Alice does not have the cat, Carol cannot have the cat.\n",
              "- **Upanaya (Application)**: In this case, Carol does not have the cat.\n",
              "- **Nigamana (Conclusion)**: Therefore, Carol does not have the cat.\n",
              "\n",
              "### Shabda (Testimony)\n",
              "- **Pratijna (Thesis)**: I know that Alice says she has a pet.\n",
              "- **Hetu (Reason)**: This is a statement from Alice herself.\n",
              "- **Udaharana (Universal + Example)**: If Alice said she had the cat, it would be true.\n",
              "- **Upanaya (Application)**: In this case, Alice does have a pet.\n",
              "- **Nigamana (Conclusion)**: Therefore, Alice has a pet.\n",
              "\n",
              "---\n",
              "\n",
              "<span style=\"background-color: #2196f3; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Pancha Avayava (5-Member Syllogism)\n",
              "</span>\n",
              "### Syllogism 1:\n",
              "**Pratijna (Thesis)**: Alice has the cat.\n",
              "**Hetu (Reason)**: This is an indirect statement from Alice's testimony. \n",
              "- **Udaharana (Universal + Example)**: If Alice says she has a pet, it must be true because she would know.\n",
              "- **Upanaya (Application)**: In this case, Alice does have the cat.\n",
              "- **Nigamana (Conclusion)**: Therefore, Alice has the cat.\n",
              "\n",
              "### Syllogism 2:\n",
              "**Pratijna (Thesis)**: Carol has the fish.\n",
              "**Hetu (Reason)**: This is a direct statement from the problem. \n",
              "- **Udaharana (Universal + Example)**: If Carol says she has the fish, it's true because she would know.\n",
              "- **Upanaya (Application)**: In this case, Carol does have the fish.\n",
              "- **Nigamana (Conclusion)**: Therefore, Carol has the fish.\n",
              "\n",
              "### Syllogism 3:\n",
              "**Pratijna (Thesis)**: Bob has the dog.\n",
              "**Hetu (Reason)**: This is a direct statement from the problem. \n",
              "- **Udaharana (Universal + Example)**: If Bob says he has the dog, it's true because he would know.\n",
              "- **Upanaya (Application)**: In this case, Bob does have the dog.\n",
              "- **Nigamana (Conclusion)**: Therefore, Bob has the dog.\n",
              "\n",
              "---\n",
              "\n",
              "<span style=\"background-color: #ff9800; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Tarka (Counterfactual Reasoning)\n",
              "</span>\n",
              "**Hypothesis**: What if Alice had the cat?\n",
              "**Consequence**: If Alice had the cat, it would contradict Pratyaksha 1. \n",
              "**Analysis**: This is a counterfactual scenario that tests our hypothesis about Alice's pet.\n",
              "**Resolution**: Our hypothesis must be false; Alice does not have the cat.\n",
              "\n",
              "**Hypothesis**: What if Carol had the fish?\n",
              "**Consequence**: If Carol had the fish, it would contradict Pratyaksha 3. \n",
              "**Analysis**: This is a counterfactual scenario that tests our hypothesis about Carol's pet.\n",
              "**Resolution**: Our hypothesis must be false; Carol does not have the fish.\n",
              "\n",
              "**Hypothesis**: What if Bob did not have the dog?\n",
              "**Consequence**: If Bob did not have the dog, it would contradict Pratyaksha 2. \n",
              "**Analysis**: This is a counterfactual scenario that tests our hypothesis about Bob's pet.\n",
              "**Resolution**: Our hypothesis must be false; Bob does have the dog.\n",
              "\n",
              "---\n",
              "\n",
              "<span style=\"background-color: #9c27b0; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Hetvabhasa (Fallacy Check)\n",
              "</span>\n",
              "Check for Savyabhichara: No\n",
              "- We are not assuming both sides of the doubt simultaneously.\n",
              "\n",
              "Check for Viruddha: Yes\n",
              "- We are using direct statements from the problem to support our conclusions. \n",
              "\n",
              "Check for Asiddha: No\n",
              "- Our reasoning is based on logical rules and principles.\n",
              "\n",
              "Check for Satpratipaksha: No\n",
              "- We are not assuming only one side of the doubt; we have considered multiple possibilities.\n",
              "\n",
              "Check for Badhita: No\n",
              "- We are not using circular reasoning or self-referential arguments.\n",
              "\n",
              "\n",
              "---\n",
              "\n",
              "<span style=\"background-color: #f44336; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Nirnaya (Ascertainment)\n",
              "</span>\n",
              "**Final Answer**: \n",
              "Alice has the cat.\n",
              "Bob has the dog.\n",
              "Carol has the fish.\n",
              "\n",
              "**Justification**: \n",
              "We used direct statements from the problem to support our conclusions, and logical rules of inference. We checked for fallacies in our reasoning.\n",
              "\n",
              "**Confidence**: 100% \n",
              "- Our answer is consistent with all the given information.</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Phase highlighting function (double-click to show code)\n",
        "# Phase highlighting function\n",
        "def highlight_phases(text: str) -> str:\n",
        "    \"\"\"Add HTML highlighting to Nyaya phases.\"\"\"\n",
        "    phases = [\n",
        "        (r\"## Samshaya.*?\\n\", \"#ffeb3b\"),\n",
        "        (r\"## Pramana.*?\\n\", \"#4caf50\"),\n",
        "        (r\"## Pancha Avayava.*?\\n\", \"#2196f3\"),\n",
        "        (r\"## Tarka.*?\\n\", \"#ff9800\"),\n",
        "        (r\"## Hetvabhasa.*?\\n\", \"#9c27b0\"),\n",
        "        (r\"## Nirnaya.*?\\n\", \"#f44336\"),\n",
        "    ]\n",
        "    \n",
        "    for pattern, color in phases:\n",
        "        text = re.sub(\n",
        "            pattern,\n",
        "            f'<span style=\"background-color: {color}; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">\\\\g<0></span>',\n",
        "            text,\n",
        "            flags=re.IGNORECASE,\n",
        "        )\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Display highlighted output\n",
        "if 'last_base_output' in globals():\n",
        "    display(HTML(f\"<h3>Base Model Output (Highlighted)</h3><pre>{highlight_phases(last_base_output)}</pre>\"))\n",
        "if 'last_tuned_output' in globals():\n",
        "    display(HTML(f\"<h3>Tuned Model Output (Highlighted)</h3><pre>{highlight_phases(last_tuned_output)}</pre>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Interactive Learning Exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fd0345418ad4be4b203804653eaec05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='\\n    <h4>Exercise 1: Doubt Type Identification</h4>\\n    <p>What type of doubt (Sa…"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "exercise1_problem = \"\"\"\n",
        "Problem: If it rains, the ground gets wet. The ground is wet. Did it rain?\n",
        "\"\"\"\n",
        "\n",
        "exercise1_question = widgets.HTML(\n",
        "    value=\"\"\"\n",
        "    <h4>Exercise 1: Doubt Type Identification</h4>\n",
        "    <p>What type of doubt (Samshaya) is present in this problem?</p>\n",
        "    <pre>{exercise1_problem}</pre>\n",
        "    \"\"\".format(exercise1_problem=exercise1_problem)\n",
        ")\n",
        "\n",
        "exercise1_answer = widgets.Dropdown(\n",
        "    options=[\n",
        "        (\"Select...\", \"\"),\n",
        "        (\"Samana Dharma Upapatti\", \"samana_dharma_upapatti\"),\n",
        "        (\"Vipratipatti\", \"vipratipatti\"),\n",
        "        (\"Anadhyavasaya\", \"anadhyavasaya\"),\n",
        "    ],\n",
        "    description=\"Answer:\",\n",
        ")\n",
        "\n",
        "exercise1_feedback = Output()\n",
        "\n",
        "def check_exercise1(change):\n",
        "    with exercise1_feedback:\n",
        "        exercise1_feedback.clear_output()\n",
        "        if exercise1_answer.value == \"vipratipatti\":\n",
        "            display(HTML(\"<p style='color: green;'>✓ Correct! This is Vipratipatti (conflicting possibilities) - rain could cause wet ground, but so could other things.</p>\"))\n",
        "        elif exercise1_answer.value:\n",
        "            display(HTML(\"<p style='color: red;'>✗ Not quite. Think about whether there are conflicting possible explanations for the wet ground.</p>\"))\n",
        "\n",
        "exercise1_answer.observe(check_exercise1, names='value')\n",
        "\n",
        "display(VBox([exercise1_question, exercise1_answer, exercise1_feedback]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exercise2_text = \"\"\"\n",
        "In a logic puzzle: \"Alice says she has the red ball. Bob says Alice is lying.\"\n",
        "\n",
        "What type of Pramana is \"Alice says she has the red ball\"?\n",
        "\"\"\"\n",
        "\n",
        "exercise2_question = widgets.HTML(\n",
        "    value=f\"\"\"\n",
        "    <h4>Exercise 2: Pramana Source Identification</h4>\n",
        "    <p>{exercise2_text}</p>\n",
        "    \"\"\".format(exercise2_text=exercise2_text)\n",
        ")\n",
        "\n",
        "exercise2_answer = widgets.Dropdown(\n",
        "    options=[\n",
        "        (\"Select...\", \"\"),\n",
        "        (\"Pratyaksha (Direct Perception)\", \"pratyaksha\"),\n",
        "        (\"Anumana (Inference)\", \"anumana\"),\n",
        "        (\"Upamana (Comparison)\", \"upamana\"),\n",
        "        (\"Shabda (Testimony)\", \"shabda\"),\n",
        "    ],\n",
        "    description=\"Answer:\",\n",
        ")\n",
        "\n",
        "exercise2_feedback = Output()\n",
        "\n",
        "def check_exercise2(change):\n",
        "    with exercise2_feedback:\n",
        "        exercise2_feedback.clear_output()\n",
        "        if exercise2_answer.value == \"shabda\":\n",
        "            display(HTML(\"<p style='color: green;'>✓ Correct! This is Shabda (testimony) - we are told what Alice said, not what we directly observed.</p>\"))\n",
        "        elif exercise2_answer.value:\n",
        "            display(HTML(\"<p style='color: red;'>✗ Not quite. Think about whether this is something we directly observed or something we were told.</p>\"))\n",
        "\n",
        "exercise2_answer.observe(check_exercise2, names='value')\n",
        "\n",
        "display(VBox([exercise2_question, exercise2_answer, exercise2_feedback]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exercise3_text = \"\"\"\n",
        "Complete this Pancha Avayava syllogism:\n",
        "\n",
        "Pratijna (Thesis): All birds can fly.\n",
        "Hetu (Reason): Because they have wings.\n",
        "Udaharana (Universal Example): ?\n",
        "Upanaya (Application): ?\n",
        "Nigamana (Conclusion): ?\n",
        "\"\"\"\n",
        "\n",
        "exercise3_question = widgets.HTML(\n",
        "    value=f\"\"\"\n",
        "    <h4>Exercise 3: Complete the Syllogism</h4>\n",
        "    <pre>{exercise3_text}</pre>\n",
        "    \"\"\".format(exercise3_text=exercise3_text)\n",
        ")\n",
        "\n",
        "exercise3_udaharana = widgets.Textarea(\n",
        "    value=\"\",\n",
        "    placeholder=\"Enter Udaharana (Universal Example)...\",\n",
        "    description=\"Udaharana:\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"80px\"),\n",
        ")\n",
        "\n",
        "exercise3_feedback = Output()\n",
        "\n",
        "def check_exercise3(change):\n",
        "    with exercise3_feedback:\n",
        "        exercise3_feedback.clear_output()\n",
        "        answer = exercise3_udaharana.value.lower()\n",
        "        \n",
        "        # Check for universal rule pattern\n",
        "        has_universal = any(word in answer for word in [\"wherever\", \"whenever\", \"all\", \"any\", \"every\"])\n",
        "        has_example = any(word in answer for word in [\"eagle\", \"sparrow\", \"bird\", \"example\", \"instance\"])\n",
        "        \n",
        "        if has_universal and has_example:\n",
        "            display(HTML(\"<p style='color: green;'>✓ Good! Your Udaharana includes both a universal rule and a concrete example. Example: 'Wherever there is a bird with wings, it can fly. For example, an eagle has wings and can fly.'</p>\"))\n",
        "        elif has_universal:\n",
        "            display(HTML(\"<p style='color: orange;'>⚠ You have a universal rule, but try to include a concrete example too (e.g., 'like an eagle').</p>\"))\n",
        "        else:\n",
        "            display(HTML(\"<p style='color: red;'>✗ Try to include both a universal rule (using words like 'wherever', 'whenever', 'all') and a concrete example.</p>\"))\n",
        "\n",
        "exercise3_udaharana.observe(check_exercise3, names='value')\n",
        "\n",
        "display(VBox([exercise3_question, exercise3_udaharana, exercise3_feedback]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Try Your Own Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "problem_input = widgets.Textarea(\n",
        "    value=\"\",\n",
        "    placeholder=\"Enter your logical problem here...\",\n",
        "    description=\"Problem:\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"150px\"),\n",
        ")\n",
        "\n",
        "custom_generate_button = widgets.Button(\n",
        "    description=\"Generate Nyaya Reasoning\",\n",
        "    button_style=\"success\",\n",
        "    icon=\"rocket\",\n",
        ")\n",
        "\n",
        "custom_output_area = Output()\n",
        "\n",
        "def generate_custom(button):\n",
        "    \"\"\"Generate reasoning for custom problem using tuned model (fallback: base).\"\"\"\n",
        "    if config is None:\n",
        "        with custom_output_area:\n",
        "            print(\"\\u26a0 Please configure backend and stage first\")\n",
        "        return\n",
        "\n",
        "    problem = problem_input.value.strip()\n",
        "    if not problem:\n",
        "        with custom_output_area:\n",
        "            print(\"\\u26a0 Please enter a problem\")\n",
        "        return\n",
        "\n",
        "    with custom_output_area:\n",
        "        custom_output_area.clear_output()\n",
        "\n",
        "        backend = config.get(\"tuned_backend\") or config.get(\"base_backend\")\n",
        "        if backend is None:\n",
        "            print(\"\\u26a0 No model backend available. Please apply configuration first.\")\n",
        "            return\n",
        "\n",
        "        if config.get(\"tuned_backend\"):\n",
        "            model_label = config.get(\"tuned_label\", config[\"stage_config\"].tuned_model_id)\n",
        "            model_role = \"Tuned\"\n",
        "        else:\n",
        "            model_label = config.get(\"base_label\", config[\"stage_config\"].base_model_id)\n",
        "            model_role = \"Base (tuned model not available)\"\n",
        "\n",
        "        print(f\"Generating reasoning with {model_role} model: {model_label}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        user_prompt = build_user_prompt(problem)\n",
        "        active_system_prompt = config.get(\"system_prompt\", config[\"stage_config\"].system_prompt)\n",
        "\n",
        "        try:\n",
        "            output = backend.generate(\n",
        "                user_prompt,\n",
        "                system_prompt=active_system_prompt,\n",
        "                max_new_tokens=max_tokens_slider.value if 'max_tokens_slider' in globals() else 2048,\n",
        "                temperature=temperature_slider.value if 'temperature_slider' in globals() else 0.5,\n",
        "                top_p=top_p_slider.value if 'top_p_slider' in globals() else 0.75,\n",
        "                top_k=top_k_slider.value if 'top_k_slider' in globals() else 5,\n",
        "            )\n",
        "            display(Markdown(f\"## Generated Reasoning ({model_role}: {model_label})\\n\\n{output}\"))\n",
        "        except Exception as e:\n",
        "            display(HTML(f\"<p style='color: red;'>Error: {e}</p>\"))\n",
        "\n",
        "custom_generate_button.on_click(generate_custom)\n",
        "\n",
        "display(VBox([\n",
        "    widgets.HTML(\"<h3>Try Your Own Problem</h3>\"),\n",
        "    problem_input,\n",
        "    custom_generate_button,\n",
        "    custom_output_area,\n",
        "]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook demonstrates:\n",
        "\n",
        "1. **Structured Reasoning**: The 6-phase Navya-Nyaya methodology\n",
        "2. **Model Comparison**: Side-by-side comparison of base vs tuned models\n",
        "3. **Interactive Learning**: Exercises to understand Nyaya concepts\n",
        "4. **Custom Problems**: Generate reasoning for your own logical problems"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
