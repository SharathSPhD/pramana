{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pramana Explorer: Interactive Navya-Nyaya Reasoning Demo\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SharathSPhD/pramana/blob/main/notebooks/01_pramana_explorer.ipynb#slideshowMode=true)\n",
    "\n",
    "This notebook provides an interactive exploration of the Pramana epistemic reasoning engine, demonstrating the 6-phase Navya-Nyaya methodology for systematic logical problem-solving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form"
   },
   "source": [
    "#@title Setup: Clone repo, install deps, detect GPU (double-click to show code)\n",
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# ---- Clone repo (skip if already cloned or running locally) ----\n",
    "REPO_URL = \"https://github.com/SharathSPhD/pramana.git\"\n",
    "REPO_DIR = \"pramana\"\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not Path(REPO_DIR).exists():\n",
    "        print(\"Cloning repository...\")\n",
    "        subprocess.run([\"git\", \"clone\", REPO_URL], check=True, capture_output=True)\n",
    "        print(\"  Done.\")\n",
    "    os.chdir(REPO_DIR)\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", \"notebooks/requirements.txt\"],\n",
    "                   check=True, capture_output=True)\n",
    "    sys.path.insert(0, \"notebooks\")\n",
    "else:\n",
    "    # Local: pramana_backend.py should be in the same dir or parent\n",
    "    if Path(\"pramana_backend.py\").exists():\n",
    "        sys.path.insert(0, \".\")\n",
    "    elif Path(\"notebooks/pramana_backend.py\").exists():\n",
    "        sys.path.insert(0, \"notebooks\")\n",
    "    else:\n",
    "        for p in [Path(\"..\"), Path(\"../notebooks\")]:\n",
    "            if (p / \"pramana_backend.py\").exists():\n",
    "                sys.path.insert(0, str(p.resolve()))\n",
    "                break\n",
    "\n",
    "# ---- GPU Detection ----\n",
    "def check_backend():\n",
    "    \"\"\"Detect GPU/CPU and display status banner.\"\"\"\n",
    "    gpu_name, gpu_mem = None, None\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader,nounits\"],\n",
    "            capture_output=True, text=True, timeout=5,\n",
    "        )\n",
    "        if result.returncode == 0 and result.stdout.strip():\n",
    "            parts = result.stdout.strip().split(\", \")\n",
    "            gpu_name = parts[0]\n",
    "            gpu_mem = parts[1] if len(parts) > 1 else \"?\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if gpu_name:\n",
    "        hw = f\"<b>GPU: {gpu_name} ({gpu_mem} MB)</b>\"\n",
    "        color = \"green\"\n",
    "    else:\n",
    "        hw = \"<b>CPU</b>\"\n",
    "        color = \"orange\"\n",
    "\n",
    "    env = \"Google Colab\" if IN_COLAB else \"Local\"\n",
    "    banner = (\n",
    "        f\"<div style='padding:10px;border-radius:8px;border:2px solid {color};margin:8px 0'>\"\n",
    "        f\"<span style='font-size:1.2em'>Runtime: <span style='color:{color}'>{hw}</span></span>\"\n",
    "        f\"<br>Environment: {env}\"\n",
    "    )\n",
    "    if not gpu_name:\n",
    "        banner += \"<br><i>For better performance: Runtime -> Change runtime type -> GPU</i>\"\n",
    "    banner += \"</div>\"\n",
    "    display(HTML(banner))\n",
    "    return gpu_name is not None\n",
    "\n",
    "GPU_AVAILABLE = check_backend()\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form"
   },
   "source": [
    "#@title Imports (double-click to show code)\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, HBox, VBox, Output\n",
    "\n",
    "from pramana_backend import (\n",
    "    create_backend,\n",
    "    STAGE_CONFIGS,\n",
    "    OLLAMA_MODEL_MAP,\n",
    "    build_user_prompt,\n",
    "    EXAMPLE_PROBLEMS,\n",
    "    load_test_problems,\n",
    "    parse_nyaya_phases,\n",
    "    validate_structure as backend_validate_structure,\n",
    "    score_content_quality as backend_score_content_quality,\n",
    "    extract_final_answer,\n",
    "    score_answers,\n",
    "    setup_ollama,\n",
    "    setup_ollama_stage,\n",
    "    download_gguf,\n",
    ")\n",
    "\n",
    "print(\"\\u2713 All modules imported\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form"
   },
   "source": [
    "#@title Configuration: Backend selection and model setup (double-click to show code)\n",
    "# Backends: transformers (local GPU), ollama (local server), llama.cpp (GGUF)\n",
    "_gpu_label = \"Colab GPU\" if IN_COLAB else \"Local GPU/CPU\"\n",
    "backend_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        (f\"Transformers ({_gpu_label})\", \"transformers\"),\n",
    "        (f\"Ollama ({_gpu_label})\", \"ollama\"),\n",
    "        (f\"llama.cpp GGUF ({_gpu_label})\", \"llamacpp\"),\n",
    "    ],\n",
    "    value=\"ollama\" if IN_COLAB else \"transformers\",\n",
    "    description=\"Backend:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "stage_dropdown = widgets.Dropdown(\n",
    "    options=list(STAGE_CONFIGS.keys()),\n",
    "    value=\"Stage 0\",\n",
    "    description=\"Stage:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "model_variant_dropdown = widgets.Dropdown(\n",
    "    options=[(\"Tuned model\", \"tuned\"), (\"Base model\", \"base\"), (\"Both (comparison)\", \"both\")],\n",
    "    value=\"both\",\n",
    "    description=\"Model:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "# System prompt -- editable, auto-populated from stage config\n",
    "_initial_stage = STAGE_CONFIGS[\"Stage 0\"]\n",
    "system_prompt_textarea = widgets.Textarea(\n",
    "    value=_initial_stage.system_prompt,\n",
    "    description=\"System Prompt:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"95%\", height=\"120px\"),\n",
    ")\n",
    "\n",
    "# ---------- Ollama backend-specific settings ----------\n",
    "_stage0_map = OLLAMA_MODEL_MAP[\"Stage 0\"]\n",
    "ollama_base_model_input = widgets.Text(\n",
    "    value=_stage0_map[\"base\"],\n",
    "    placeholder=\"Ollama base model tag\",\n",
    "    description=\"Base model:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"95%\"),\n",
    ")\n",
    "ollama_tuned_model_input = widgets.Text(\n",
    "    value=_stage0_map[\"tuned\"],\n",
    "    placeholder=\"Ollama tuned model tag\",\n",
    "    description=\"Tuned model:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"95%\"),\n",
    ")\n",
    "ollama_url_input = widgets.Text(\n",
    "    value=\"http://localhost:11434\",\n",
    "    description=\"Ollama URL:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"95%\"),\n",
    ")\n",
    "\n",
    "# llama.cpp\n",
    "gguf_path_input = widgets.Text(\n",
    "    value=\"\",\n",
    "    placeholder=\"Path to .gguf file (or llama.cpp server URL)\",\n",
    "    description=\"GGUF path/URL:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"95%\"),\n",
    ")\n",
    "\n",
    "# ---------- Stage observer ----------\n",
    "def _on_stage_change(change):\n",
    "    stage = change[\"new\"]\n",
    "    stage_cfg = STAGE_CONFIGS[stage]\n",
    "    system_prompt_textarea.value = stage_cfg.system_prompt\n",
    "    if stage in OLLAMA_MODEL_MAP:\n",
    "        ollama_base_model_input.value = OLLAMA_MODEL_MAP[stage][\"base\"]\n",
    "        ollama_tuned_model_input.value = OLLAMA_MODEL_MAP[stage][\"tuned\"]\n",
    "\n",
    "stage_dropdown.observe(_on_stage_change, names=\"value\")\n",
    "\n",
    "# ---------- Backend settings panel ----------\n",
    "backend_settings_output = Output()\n",
    "\n",
    "def update_backend_settings(change=None):\n",
    "    with backend_settings_output:\n",
    "        backend_settings_output.clear_output()\n",
    "        bt = backend_dropdown.value\n",
    "        if bt == \"transformers\":\n",
    "            display(widgets.HTML(\"<i>Uses HuggingFace model IDs from stage config. Requires GPU.</i>\"))\n",
    "        elif bt == \"ollama\":\n",
    "            ollama_setup_btn = widgets.Button(description=\"Auto-Setup Ollama (base + tuned)\", button_style=\"info\", icon=\"magic\")\n",
    "            ollama_setup_out = Output()\n",
    "            def _ollama_auto(b):\n",
    "                with ollama_setup_out:\n",
    "                    ollama_setup_out.clear_output()\n",
    "                    try:\n",
    "                        stage = stage_dropdown.value\n",
    "                        print(f\"Setting up Ollama models for {stage}...\")\n",
    "                        result = setup_ollama_stage(stage)\n",
    "                        ollama_base_model_input.value = result[\"base\"] or ollama_base_model_input.value\n",
    "                        if result[\"tuned\"]:\n",
    "                            ollama_tuned_model_input.value = result[\"tuned\"]\n",
    "                        print(f\"\\n\\u2713 Setup complete for {stage}\")\n",
    "                        print(f\"  Base:  {result['base']}\")\n",
    "                        print(f\"  Tuned: {result['tuned'] or '(not available)'}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\u2717 Setup failed: {e}\")\n",
    "                        import traceback; traceback.print_exc()\n",
    "            ollama_setup_btn.on_click(_ollama_auto)\n",
    "            display(VBox([\n",
    "                widgets.HTML(\n",
    "                    f\"<b>Ollama Settings</b> \\u2014 \"\n",
    "                    \"Click <b>Auto-Setup</b> to install Ollama + pull/create models for the selected stage.\"\n",
    "                ),\n",
    "                ollama_base_model_input,\n",
    "                ollama_tuned_model_input,\n",
    "                ollama_url_input,\n",
    "                ollama_setup_btn,\n",
    "                ollama_setup_out,\n",
    "            ]))\n",
    "        elif bt == \"llamacpp\":\n",
    "            gguf_setup_btn = widgets.Button(description=\"Auto-Download GGUF\", button_style=\"info\", icon=\"download\")\n",
    "            gguf_setup_out = Output()\n",
    "            def _gguf_auto(b):\n",
    "                with gguf_setup_out:\n",
    "                    gguf_setup_out.clear_output()\n",
    "                    try:\n",
    "                        path = download_gguf()\n",
    "                        gguf_path_input.value = path\n",
    "                        print(f\"\\u2713 GGUF downloaded: {path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\u2717 Download failed: {e}\")\n",
    "            gguf_setup_btn.on_click(_gguf_auto)\n",
    "            display(VBox([\n",
    "                widgets.HTML(\"<b>llama.cpp Settings</b> \\u2014 Click Auto-Download or provide path to GGUF\"),\n",
    "                gguf_path_input,\n",
    "                gguf_setup_btn,\n",
    "                gguf_setup_out,\n",
    "            ]))\n",
    "\n",
    "backend_dropdown.observe(update_backend_settings, names='value')\n",
    "\n",
    "# ---------- Configuration apply ----------\n",
    "def setup_config():\n",
    "    global config\n",
    "    backend_type = backend_dropdown.value\n",
    "    stage_name = stage_dropdown.value\n",
    "    variant = model_variant_dropdown.value\n",
    "    stage_config = STAGE_CONFIGS[stage_name]\n",
    "\n",
    "    def _make_backend(model_id, role=\"tuned\"):\n",
    "        if backend_type == \"transformers\":\n",
    "            return create_backend(\"transformers\", model_id=model_id)\n",
    "        elif backend_type == \"ollama\":\n",
    "            if role == \"base\":\n",
    "                model_name = ollama_base_model_input.value.strip()\n",
    "            else:\n",
    "                model_name = ollama_tuned_model_input.value.strip()\n",
    "            return create_backend(\"ollama\", model_name=model_name,\n",
    "                                  base_url=ollama_url_input.value.strip())\n",
    "        elif backend_type == \"llamacpp\":\n",
    "            path_or_url = gguf_path_input.value.strip()\n",
    "            if path_or_url.startswith(\"http\"):\n",
    "                return create_backend(\"llamacpp\", server_url=path_or_url)\n",
    "            else:\n",
    "                return create_backend(\"llamacpp\", model_path=path_or_url)\n",
    "\n",
    "    try:\n",
    "        base_backend = _make_backend(stage_config.base_model_id, role=\"base\") if variant in (\"base\", \"both\") else None\n",
    "        tuned_backend = _make_backend(stage_config.tuned_model_id, role=\"tuned\") if variant in (\"tuned\", \"both\") else None\n",
    "\n",
    "        if backend_type == \"ollama\":\n",
    "            base_label = ollama_base_model_input.value.strip() if base_backend else None\n",
    "            tuned_label = ollama_tuned_model_input.value.strip() if tuned_backend else None\n",
    "        else:\n",
    "            base_label = stage_config.base_model_id if base_backend else None\n",
    "            tuned_label = stage_config.tuned_model_id if tuned_backend else None\n",
    "\n",
    "        active_system_prompt = system_prompt_textarea.value.strip() or stage_config.system_prompt\n",
    "\n",
    "        config = {\n",
    "            \"backend_type\": backend_type,\n",
    "            \"stage_config\": stage_config,\n",
    "            \"base_backend\": base_backend,\n",
    "            \"tuned_backend\": tuned_backend,\n",
    "            \"base_label\": base_label,\n",
    "            \"tuned_label\": tuned_label,\n",
    "            \"system_prompt\": active_system_prompt,\n",
    "        }\n",
    "        print(f\"\\u2713 Configuration set: {stage_name} with {backend_type} backend\")\n",
    "        if base_backend:\n",
    "            print(f\"  Base model ready: {base_label}\")\n",
    "        if tuned_backend:\n",
    "            print(f\"  Tuned model ready: {tuned_label}\")\n",
    "        return config\n",
    "    except Exception as e:\n",
    "        print(f\"\\u2717 Configuration failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "setup_button = widgets.Button(description=\"Apply Configuration\", button_style=\"primary\", icon=\"check\")\n",
    "config_output = Output()\n",
    "\n",
    "def on_setup_click(button):\n",
    "    with config_output:\n",
    "        config_output.clear_output()\n",
    "        setup_config()\n",
    "\n",
    "setup_button.on_click(on_setup_click)\n",
    "update_backend_settings()\n",
    "\n",
    "display(VBox([\n",
    "    widgets.HTML(\"<h3>Configuration</h3>\"),\n",
    "    backend_dropdown,\n",
    "    stage_dropdown,\n",
    "    model_variant_dropdown,\n",
    "    widgets.HTML(\"<b>System Prompt</b> (auto-populated from stage; editable):\"),\n",
    "    system_prompt_textarea,\n",
    "    backend_settings_output,\n",
    "    setup_button,\n",
    "    config_output,\n",
    "]))\n",
    "\n",
    "config = None"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Navya-Nyaya Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Navya-Nyaya?\n",
    "\n",
    "Navya-Nyaya (\"New Logic\") is a 2,500-year-old Indian epistemological system that provides a structured methodology for systematic reasoning. Unlike Western formal logic, Navya-Nyaya integrates logic and epistemology, requiring explicit grounding in concrete examples and universal rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 6-Phase Framework\n",
    "\n",
    "Pramana enforces a structured 6-phase Nyaya methodology:\n",
    "\n",
    "1. **Samshaya (Doubt Analysis)** - Classify the type of uncertainty/ambiguity\n",
    "2. **Pramana (Evidence Sources)** - Identify valid knowledge sources:\n",
    "   - Pratyaksha (Direct Perception)\n",
    "   - Anumana (Inference)\n",
    "   - Upamana (Comparison)\n",
    "   - Shabda (Testimony)\n",
    "3. **Pancha Avayava (5-Member Syllogism)** - Construct formal argument with:\n",
    "   - Pratijna (Thesis)\n",
    "   - Hetu (Reason)\n",
    "   - Udaharana (Universal Example)\n",
    "   - Upanaya (Application)\n",
    "   - Nigamana (Conclusion)\n",
    "4. **Tarka (Counterfactual Testing)** - Use reductio ad absurdum to verify conclusions\n",
    "5. **Hetvabhasa (Fallacy Detection)** - Check for reasoning errors\n",
    "6. **Nirnaya (Ascertainment)** - Reach definitive conclusion or explicitly state insufficient evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Example Problem: pramana-001\n",
       "\n",
       "Three people (Alice, Bob, Carol) each have one pet: a cat, a dog, or a fish.\n",
       "\n",
       "**Constraints**:\n",
       "1. Alice does not have the cat\n",
       "2. Bob has the dog\n",
       "3. Carol does not have the fish\n",
       "\n",
       "**Question**: Who has which pet?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Ground Truth:** Alice has the fish, Bob has the dog, Carol has the cat"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Problem Type:** constraint_satisfaction | **Difficulty:** medium"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Load example from embedded data (no external files needed) (double-click to show code)\n",
    "# Load example from embedded data (no external files needed)\n",
    "example = EXAMPLE_PROBLEMS[0]  # pramana-001: constraint satisfaction\n",
    "\n",
    "display(Markdown(f\"## Example Problem: {example['id']}\\n\\n{example['problem']}\"))\n",
    "display(Markdown(f\"**Ground Truth:** {example['ground_truth']}\"))\n",
    "display(Markdown(f\"**Problem Type:** {example['problem_type']} | **Difficulty:** {example['difficulty']}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Samshaya (Doubt Analysis)\n",
    "\n",
    "The example demonstrates **Samana Dharma Upapatti** (Multiple possibilities share similar properties):\n",
    "\n",
    "- There are three people and three pets, creating multiple possible assignments\n",
    "- Without systematic reasoning, we cannot determine which person has which pet\n",
    "- The doubt arises because multiple arrangements are conceivable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Pramana (Sources of Knowledge)\n",
    "\n",
    "The example identifies:\n",
    "\n",
    "- **Pratyaksha**: Directly stated constraints (\"Alice does not have the cat\", \"Bob has the dog\", etc.)\n",
    "- **Anumana**: Inferences like \"If Bob has the dog, then neither Alice nor Carol has the dog\"\n",
    "- **Upamana**: Comparison to constraint satisfaction problems with mutual exclusivity\n",
    "- **Shabda**: Logical principles (Law of Excluded Middle, Non-Contradiction, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: Pancha Avayava (5-Member Syllogism)\n",
    "\n",
    "The example constructs three syllogisms:\n",
    "\n",
    "1. **Establishing Bob's Pet**: Pratijna=\"Bob has the dog\", supported by direct constraint\n",
    "2. **Establishing Alice's Pet**: Pratijna=\"Alice has the fish\", via elimination (cannot have cat or dog)\n",
    "3. **Establishing Carol's Pet**: Pratijna=\"Carol has the cat\", via completeness principle\n",
    "\n",
    "Each syllogism includes all 5 members with explicit Udaharana (universal rules) and Upanaya (application)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4: Tarka (Counterfactual Testing)\n",
    "\n",
    "The example uses reductio ad absurdum:\n",
    "\n",
    "- **Hypothesis**: \"Suppose Carol does not have the cat\"\n",
    "- **Consequence**: This leads to Carol having no pet, violating completeness\n",
    "- **Analysis**: This is absurd given the problem constraints\n",
    "- **Resolution**: Therefore, Carol must have the cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 5: Hetvabhasa (Fallacy Check)\n",
    "\n",
    "The example checks for:\n",
    "\n",
    "- **Savyabhichara** (Erratic reasoning): None detected\n",
    "- **Viruddha** (Contradictory reasoning): None detected\n",
    "- **Prakaranasama** (Circular reasoning): None detected\n",
    "- **Sadhyasama** (Begging the question): None detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 6: Nirnaya (Ascertainment)\n",
    "\n",
    "**Status**: Definitive Knowledge\n",
    "\n",
    "**Final Answer**: Alice has the fish, Bob has the dog, and Carol has the cat.\n",
    "\n",
    "**Confidence**: High - The solution is logically necessary given the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactive Comparison: Base vs Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 5 embedded examples\n",
      "  1. [pramana-001] constraint_satisfaction (medium)\n",
      "  2. [pramana-003] transitive_reasoning (medium)\n",
      "  3. [pramana-005] multi_step_deduction (medium)\n",
      "  4. [test-001] constraint_satisfaction (easy)\n",
      "  5. [test-004] boolean_sat (medium)\n"
     ]
    }
   ],
   "source": [
    "#@title Load example problems (self-contained, no external files needed) (double-click to show code)\n",
    "# Load example problems (self-contained, no external files needed)\n",
    "examples = load_test_problems(\"embedded\")\n",
    "print(f\"✓ Loaded {len(examples)} embedded examples\")\n",
    "\n",
    "# Display problem list\n",
    "for i, ex in enumerate(examples):\n",
    "    print(f\"  {i+1}. [{ex['id']}] {ex['problem_type']} ({ex['difficulty']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b605c5556c1416986ee5257f9c3a2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Generation Controls</h3>'), Dropdown(description='Example:', options=(('pramana…"
      ]
     },
     "metadata": {
      "application/vnd.jupyter.widget-view+json": {
       "colab": {
        "custom_widget_manager": {
         "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
        }
       }
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Example selector and generation controls (double-click to show code)\n",
    "# Example selector and generation controls\n",
    "example_selector = widgets.Dropdown(\n",
    "    options=[(ex[\"id\"], idx) for idx, ex in enumerate(examples)],\n",
    "    description=\"Example:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "# --- Example preview area ---\n",
    "example_preview_output = Output()\n",
    "\n",
    "def _on_example_select(change):\n",
    "    \"\"\"Display the selected problem text and ground truth.\"\"\"\n",
    "    with example_preview_output:\n",
    "        example_preview_output.clear_output()\n",
    "        idx = change[\"new\"]\n",
    "        ex = examples[idx]\n",
    "        display(HTML(f\"\"\"\n",
    "        <div style=\"border:1px solid #ddd; padding:12px; border-radius:6px; background:#f9f9f9; margin:4px 0;\">\n",
    "            <b>Problem:</b>\n",
    "            <pre style=\"white-space: pre-wrap; margin:6px 0;\">{ex[\"problem\"]}</pre>\n",
    "            <b>Expected Answer:</b> <code>{ex.get(\"expected_answer\", \"N/A\")}</code>\n",
    "        </div>\n",
    "        \"\"\"))\n",
    "\n",
    "example_selector.observe(_on_example_select, names=\"value\")\n",
    "# Show initial selection immediately\n",
    "_on_example_select({\"new\": example_selector.value})\n",
    "\n",
    "# --- Hyperparameter controls (matching HF Space app) ---\n",
    "max_tokens_slider = widgets.IntSlider(\n",
    "    value=2048,\n",
    "    min=64,\n",
    "    max=4096,\n",
    "    step=32,\n",
    "    description=\"Max new tokens:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"95%\"),\n",
    ")\n",
    "\n",
    "temperature_slider = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.0,\n",
    "    max=1.5,\n",
    "    step=0.05,\n",
    "    description=\"Temperature:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"95%\"),\n",
    ")\n",
    "\n",
    "top_p_slider = widgets.FloatSlider(\n",
    "    value=0.75,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.05,\n",
    "    description=\"Top-p:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"95%\"),\n",
    ")\n",
    "\n",
    "top_k_slider = widgets.IntSlider(\n",
    "    value=5,\n",
    "    min=0,\n",
    "    max=200,\n",
    "    step=5,\n",
    "    description=\"Top-k:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"95%\"),\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(\n",
    "    description=\"Generate\",\n",
    "    button_style=\"primary\",\n",
    "    icon=\"play\",\n",
    ")\n",
    "\n",
    "output_area = Output()\n",
    "\n",
    "def generate_comparison(button):\n",
    "    \"\"\"Generate outputs from both models and display side-by-side.\"\"\"\n",
    "    if config is None:\n",
    "        with output_area:\n",
    "            print(\"\\u26a0 Please configure backend and stage first\")\n",
    "        return\n",
    "    \n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        \n",
    "        idx = example_selector.value\n",
    "        example = examples[idx]\n",
    "        \n",
    "        print(f\"Generating for: {example['id']}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Build prompt\n",
    "        user_prompt = build_user_prompt(example[\"problem\"])\n",
    "        \n",
    "        # Use the user-editable system prompt from config\n",
    "        active_system_prompt = config.get(\"system_prompt\", config[\"stage_config\"].system_prompt)\n",
    "        \n",
    "        # Generation parameters\n",
    "        gen_kwargs = dict(\n",
    "            system_prompt=active_system_prompt,\n",
    "            max_new_tokens=max_tokens_slider.value,\n",
    "            temperature=temperature_slider.value,\n",
    "            top_p=top_p_slider.value,\n",
    "            top_k=top_k_slider.value,\n",
    "        )\n",
    "        \n",
    "        # Resolve display labels from config\n",
    "        base_label = config.get(\"base_label\", config[\"stage_config\"].base_model_id)\n",
    "        tuned_label = config.get(\"tuned_label\", config[\"stage_config\"].tuned_model_id)\n",
    "        \n",
    "        # Generate from base model (if configured)\n",
    "        base_output = None\n",
    "        if config.get(\"base_backend\"):\n",
    "            print(f\"\\n[Base Model: {base_label}] Generating (max_tokens={gen_kwargs['max_new_tokens']}, \"\n",
    "                  f\"temp={gen_kwargs['temperature']}, top_p={gen_kwargs['top_p']}, top_k={gen_kwargs['top_k']})...\")\n",
    "            try:\n",
    "                base_output = config[\"base_backend\"].generate(user_prompt, **gen_kwargs)\n",
    "            except Exception as e:\n",
    "                base_output = f\"Error: {e}\"\n",
    "        \n",
    "        # Generate from tuned model (if configured)\n",
    "        tuned_output = None\n",
    "        if config.get(\"tuned_backend\"):\n",
    "            print(f\"[Tuned Model: {tuned_label}] Generating...\")\n",
    "            try:\n",
    "                tuned_output = config[\"tuned_backend\"].generate(user_prompt, **gen_kwargs)\n",
    "            except Exception as e:\n",
    "                tuned_output = f\"Error: {e}\"\n",
    "        \n",
    "        # Display side-by-side (or single panel if only one model)\n",
    "        display(HTML(\"\"\"\n",
    "        <style>\n",
    "        .comparison-container { display: flex; gap: 20px; }\n",
    "        .model-output { flex: 1; border: 1px solid #ccc; padding: 10px; border-radius: 5px; }\n",
    "        .base-model { background-color: #fff3cd; }\n",
    "        .tuned-model { background-color: #d1ecf1; }\n",
    "        </style>\n",
    "        \"\"\"))\n",
    "        \n",
    "        parts = []\n",
    "        if base_output is not None:\n",
    "            parts.append(f\"\"\"\n",
    "            <div class=\"model-output base-model\">\n",
    "                <h3>Base Model ({base_label})</h3>\n",
    "                <pre style=\"white-space: pre-wrap;\">{base_output}</pre>\n",
    "            </div>\"\"\")\n",
    "        if tuned_output is not None:\n",
    "            parts.append(f\"\"\"\n",
    "            <div class=\"model-output tuned-model\">\n",
    "                <h3>Tuned Model ({tuned_label})</h3>\n",
    "                <pre style=\"white-space: pre-wrap;\">{tuned_output}</pre>\n",
    "            </div>\"\"\")\n",
    "        \n",
    "        if parts:\n",
    "            display(HTML(f'<div class=\"comparison-container\">{\"\".join(parts)}</div>'))\n",
    "        else:\n",
    "            display(HTML(\"<p style='color:orange;'>\\u26a0 No models configured for comparison.</p>\"))\n",
    "        \n",
    "        # Store outputs for analysis (use empty string if model was not run)\n",
    "        global last_base_output, last_tuned_output\n",
    "        last_base_output = base_output or \"\"\n",
    "        last_tuned_output = tuned_output or \"\"\n",
    "\n",
    "generate_button.on_click(generate_comparison)\n",
    "\n",
    "display(VBox([\n",
    "    widgets.HTML(\"<h3>Generation Controls</h3>\"),\n",
    "    example_selector,\n",
    "    example_preview_output,\n",
    "    widgets.HTML(\"<b>Hyperparameters</b> (adjust for your hardware \\u2014 larger tokens = longer output):\"),\n",
    "    max_tokens_slider,\n",
    "    temperature_slider,\n",
    "    top_p_slider,\n",
    "    top_k_slider,\n",
    "    generate_button,\n",
    "    output_area,\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Base Model Output (Highlighted)</h3><pre><span style=\"background-color: #ffeb3b; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Samshaya (Doubt Analysis)\n",
       "</span>**Doubt Type**: Uncertainty about pet ownership\n",
       "**Justification**: Lack of direct information about each person's pet\n",
       "\n",
       "---\n",
       "\n",
       "<span style=\"background-color: #4caf50; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Pramana (Sources of Knowledge)\n",
       "</span>### Pratyaksha (Direct Perception)\n",
       "- Carol does not have the fish, but this is a negative observation only.\n",
       "- Bob has the dog, which is a positive observation.\n",
       "\n",
       "### Anumana (Inference)\n",
       "- From the fact that Alice does not have the cat and Bob has the dog, we can infer that Alice might have the fish or the cat.\n",
       "- However, without more information, we cannot conclude this with certainty.\n",
       "\n",
       "### Upamana (Comparison)\n",
       "- We do not have enough comparative data to make a meaningful inference about the pets of Alice and Carol.\n",
       "\n",
       "### Shabda (Testimony)\n",
       "- No direct testimony is provided about the pets of any individual.\n",
       "\n",
       "---\n",
       "\n",
       "<span style=\"background-color: #2196f3; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Pancha Avayava (5-Member Syllogism)\n",
       "</span>### Syllogism 1: \n",
       "**Pratijna (Thesis)**: Alice does not have the cat.\n",
       "**Hetu (Reason)**: If Bob has the dog, and Alice does not have the cat, then Alice might have the fish or the cat.\n",
       "**Udaharana (Universal + Example)**: This applies to all individuals who do not have a particular pet.\n",
       "**Upanaya (Application)**: We cannot apply this conclusion to Carol without more information.\n",
       "**Nigamana (Conclusion)**: Alice might have the fish or the cat.\n",
       "\n",
       "---\n",
       "\n",
       "<span style=\"background-color: #ff9800; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Tarka (Counterfactual Reasoning)\n",
       "</span>**Hypothesis**: What if we assume that Alice has the cat?\n",
       "**Consequence**: If Alice has the cat, and Bob has the dog, then Carol must have the fish.\n",
       "**Analysis**: This assumption leads to a logical conclusion, but it is not supported by any direct evidence.\n",
       "**Resolution**: The hypothesis is not supported, so we cannot conclude that Alice has the cat.\n",
       "\n",
       "---\n",
       "\n",
       "<span style=\"background-color: #9c27b0; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Hetvabhasa (Fallacy Check)\n",
       "</span>Check for Savyabhichara: \n",
       "- No savyabhichara fallacies are present in this reasoning.\n",
       "\n",
       "Check for Viruddha: \n",
       "- No viruddha fallacies are present in this reasoning.\n",
       "\n",
       "Check for Asiddha: \n",
       "- The conclusion that Alice might have the fish or the cat is not asiddha (not false).\n",
       "\n",
       "Check for Satpratipaksha: \n",
       "- This reasoning does not rely on any single piece of information to an excessive degree.\n",
       "\n",
       "Check for Badhita: \n",
       "- There is no badhita fallacy present in this reasoning.\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "<span style=\"background-color: #f44336; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Nirnaya (Ascertainment)\n",
       "</span>**Final Answer**: Alice might have the fish or the cat.\n",
       "**Justification**: Based on the available evidence, we cannot determine which pet Alice has with certainty. However, it is possible that she has either the fish or the cat.\n",
       "**Confidence**: Moderate confidence in this conclusion</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Tuned Model Output (Highlighted)</h3><pre><span style=\"background-color: #ffeb3b; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Samshaya (Doubt Analysis)\n",
       "</span>\n",
       "**Doubt Type**: Missing information\n",
       "**Justification**: We do not know who has which pet.\n",
       "\n",
       "<span style=\"background-color: #4caf50; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Pramana (Sources of Knowledge)\n",
       "</span>\n",
       "### Pratyaksha (Direct Perception)\n",
       "- None\n",
       "\n",
       "### Anumana (Inference)\n",
       "- From constraint 2: Bob has the dog\n",
       "- From constraint 3: Carol does not have the fish\n",
       "- If Alice does not have the cat and Bob has the dog, then Alice must have the fish.\n",
       "- Therefore, we can infer that Alice has the fish.\n",
       "\n",
       "### Upamana (Comparison)\n",
       "- None\n",
       "\n",
       "### Shabda (Testimony)\n",
       "- None\n",
       "\n",
       "<span style=\"background-color: #2196f3; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Pancha Avayava (5-Member Syllogism)\n",
       "</span>\n",
       "### Syllogism 1: \n",
       "**Pratijna (Thesis)**:\n",
       "Alice has the fish.\n",
       "**Hetu (Reason)**:\n",
       "If Alice does not have the cat and Bob has the dog, then Alice must have the fish.\n",
       "**Udaharana (Universal + Example)**:\n",
       "All people who do not have the cat have the fish.\n",
       "Example: John does not have the cat; therefore, John has the fish.\n",
       "**Upanaya (Application)**:\n",
       "If Alice does not have the cat and Bob has the dog, then Alice must have the fish.\n",
       "**Nigamana (Conclusion)**:\n",
       "Therefore, Alice has the fish.\n",
       "\n",
       "### Syllogism 2: \n",
       "**Pratijna (Thesis)**:\n",
       "Bob has the dog.\n",
       "**Hetu (Reason)**:\n",
       "Constraint 2 states that Bob has the dog.\n",
       "**Udaharana (Universal + Example)**:\n",
       "All people who have the dog are Bob.\n",
       "Example: John has the dog; therefore, John is Bob.\n",
       "**Upanaya (Application)**:\n",
       "Constraint 2 states that Bob has the dog.\n",
       "**Nigamana (Conclusion)**:\n",
       "Therefore, Bob has the dog.\n",
       "\n",
       "### Syllogism 3: \n",
       "**Pratijna (Thesis)**:\n",
       "Carol does not have the fish.\n",
       "**Hetu (Reason)**:\n",
       "Constraint 3 states that Carol does not have the fish.\n",
       "**Udaharana (Universal + Example)**:\n",
       "All people who do not have the fish are Carol.\n",
       "Example: John does not have the fish; therefore, John is Carol.\n",
       "**Upanaya (Application)**:\n",
       "Constraint 3 states that Carol does not have the fish.\n",
       "**Nigamana (Conclusion)**:\n",
       "Therefore, Carol does not have the fish.\n",
       "\n",
       "<span style=\"background-color: #ff9800; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Tarka (Counterfactual Reasoning)\n",
       "</span>\n",
       "**Hypothesis**: What if Alice had the cat? Then Bob would not have the dog. But constraint 2 says Bob has the dog. Therefore, our hypothesis must be false.\n",
       "**Consequence**: If Alice had the cat, then Bob could not have the dog.\n",
       "**Analysis**: Our hypothesis implies that constraint 2 is false: Bob does not have the dog. However, this contradicts constraint 2. Therefore, our hypothesis must be false.\n",
       "**Resolution**: We reject the hypothesis that Alice has the cat.\n",
       "\n",
       "<span style=\"background-color: #9c27b0; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Hetvabhasa (Fallacy Check)\n",
       "</span>\n",
       "Check for Savyabhichara: \n",
       "- No\n",
       "\n",
       "Check for Viruddha: \n",
       "- No\n",
       "\n",
       "Check for Asiddha: \n",
       "- No\n",
       "\n",
       "Check for Satpratipaksha: \n",
       "- No\n",
       "\n",
       "Check for Badhita: \n",
       "- No\n",
       "\n",
       "<span style=\"background-color: #f44336; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">## Nirnaya (Ascertainment)\n",
       "</span>\n",
       "**Final Answer**: Alice has the fish, Bob has the dog, Carol has the cat.\n",
       "**Justification**: We used direct perception to infer that Alice has the fish. We used inference to deduce that Bob has the dog and Carol has the cat. Our reasoning was sound, and we have eliminated all possible fallacies.\n",
       "**Confidence**: High (100%)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Phase highlighting function (double-click to show code)\n",
    "# Phase highlighting function\n",
    "def highlight_phases(text: str) -> str:\n",
    "    \"\"\"Add HTML highlighting to Nyaya phases.\"\"\"\n",
    "    phases = [\n",
    "        (r\"## Samshaya.*?\\n\", \"#ffeb3b\"),\n",
    "        (r\"## Pramana.*?\\n\", \"#4caf50\"),\n",
    "        (r\"## Pancha Avayava.*?\\n\", \"#2196f3\"),\n",
    "        (r\"## Tarka.*?\\n\", \"#ff9800\"),\n",
    "        (r\"## Hetvabhasa.*?\\n\", \"#9c27b0\"),\n",
    "        (r\"## Nirnaya.*?\\n\", \"#f44336\"),\n",
    "    ]\n",
    "    \n",
    "    for pattern, color in phases:\n",
    "        text = re.sub(\n",
    "            pattern,\n",
    "            f'<span style=\"background-color: {color}; padding: 2px 4px; border-radius: 3px; font-weight: bold;\">\\\\g<0></span>',\n",
    "            text,\n",
    "            flags=re.IGNORECASE,\n",
    "        )\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Display highlighted output\n",
    "if 'last_base_output' in globals():\n",
    "    display(HTML(f\"<h3>Base Model Output (Highlighted)</h3><pre>{highlight_phases(last_base_output)}</pre>\"))\n",
    "if 'last_tuned_output' in globals():\n",
    "    display(HTML(f\"<h3>Tuned Model Output (Highlighted)</h3><pre>{highlight_phases(last_tuned_output)}</pre>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Learning Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd0345418ad4be4b203804653eaec05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n    <h4>Exercise 1: Doubt Type Identification</h4>\\n    <p>What type of doubt (Sa…"
      ]
     },
     "metadata": {
      "application/vnd.jupyter.widget-view+json": {
       "colab": {
        "custom_widget_manager": {
         "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
        }
       }
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Exercise 1: Identify the doubt type (double-click to show code)\n",
    "# Exercise 1: Identify the doubt type\n",
    "exercise1_problem = \"\"\"\n",
    "Problem: If it rains, the ground gets wet. The ground is wet. Did it rain?\n",
    "\"\"\"\n",
    "\n",
    "exercise1_question = widgets.HTML(\n",
    "    value=\"\"\"\n",
    "    <h4>Exercise 1: Doubt Type Identification</h4>\n",
    "    <p>What type of doubt (Samshaya) is present in this problem?</p>\n",
    "    <pre>{exercise1_problem}</pre>\n",
    "    \"\"\".format(exercise1_problem=exercise1_problem)\n",
    ")\n",
    "\n",
    "exercise1_answer = widgets.Dropdown(\n",
    "    options=[\n",
    "        (\"Select...\", \"\"),\n",
    "        (\"Samana Dharma Upapatti\", \"samana_dharma_upapatti\"),\n",
    "        (\"Vipratipatti\", \"vipratipatti\"),\n",
    "        (\"Anadhyavasaya\", \"anadhyavasaya\"),\n",
    "    ],\n",
    "    description=\"Answer:\",\n",
    ")\n",
    "\n",
    "exercise1_feedback = Output()\n",
    "\n",
    "def check_exercise1(change):\n",
    "    with exercise1_feedback:\n",
    "        exercise1_feedback.clear_output()\n",
    "        if exercise1_answer.value == \"vipratipatti\":\n",
    "            display(HTML(\"<p style='color: green;'>✓ Correct! This is Vipratipatti (conflicting possibilities) - rain could cause wet ground, but so could other things.</p>\"))\n",
    "        elif exercise1_answer.value:\n",
    "            display(HTML(\"<p style='color: red;'>✗ Not quite. Think about whether there are conflicting possible explanations for the wet ground.</p>\"))\n",
    "\n",
    "exercise1_answer.observe(check_exercise1, names='value')\n",
    "\n",
    "display(VBox([exercise1_question, exercise1_answer, exercise1_feedback]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Exercise 2: Identify Pramana sources (double-click to show code)\n",
    "# Exercise 2: Identify Pramana sources\n",
    "exercise2_text = \"\"\"\n",
    "In a logic puzzle: \"Alice says she has the red ball. Bob says Alice is lying.\"\n",
    "\n",
    "What type of Pramana is \"Alice says she has the red ball\"?\n",
    "\"\"\"\n",
    "\n",
    "exercise2_question = widgets.HTML(\n",
    "    value=f\"\"\"\n",
    "    <h4>Exercise 2: Pramana Source Identification</h4>\n",
    "    <p>{exercise2_text}</p>\n",
    "    \"\"\".format(exercise2_text=exercise2_text)\n",
    ")\n",
    "\n",
    "exercise2_answer = widgets.Dropdown(\n",
    "    options=[\n",
    "        (\"Select...\", \"\"),\n",
    "        (\"Pratyaksha (Direct Perception)\", \"pratyaksha\"),\n",
    "        (\"Anumana (Inference)\", \"anumana\"),\n",
    "        (\"Upamana (Comparison)\", \"upamana\"),\n",
    "        (\"Shabda (Testimony)\", \"shabda\"),\n",
    "    ],\n",
    "    description=\"Answer:\",\n",
    ")\n",
    "\n",
    "exercise2_feedback = Output()\n",
    "\n",
    "def check_exercise2(change):\n",
    "    with exercise2_feedback:\n",
    "        exercise2_feedback.clear_output()\n",
    "        if exercise2_answer.value == \"shabda\":\n",
    "            display(HTML(\"<p style='color: green;'>✓ Correct! This is Shabda (testimony) - we are told what Alice said, not what we directly observed.</p>\"))\n",
    "        elif exercise2_answer.value:\n",
    "            display(HTML(\"<p style='color: red;'>✗ Not quite. Think about whether this is something we directly observed or something we were told.</p>\"))\n",
    "\n",
    "exercise2_answer.observe(check_exercise2, names='value')\n",
    "\n",
    "display(VBox([exercise2_question, exercise2_answer, exercise2_feedback]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Exercise 3: Complete the syllogism (double-click to show code)\n",
    "# Exercise 3: Complete the syllogism\n",
    "exercise3_text = \"\"\"\n",
    "Complete this Pancha Avayava syllogism:\n",
    "\n",
    "Pratijna (Thesis): All birds can fly.\n",
    "Hetu (Reason): Because they have wings.\n",
    "Udaharana (Universal Example): ?\n",
    "Upanaya (Application): ?\n",
    "Nigamana (Conclusion): ?\n",
    "\"\"\"\n",
    "\n",
    "exercise3_question = widgets.HTML(\n",
    "    value=f\"\"\"\n",
    "    <h4>Exercise 3: Complete the Syllogism</h4>\n",
    "    <pre>{exercise3_text}</pre>\n",
    "    \"\"\".format(exercise3_text=exercise3_text)\n",
    ")\n",
    "\n",
    "exercise3_udaharana = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder=\"Enter Udaharana (Universal Example)...\",\n",
    "    description=\"Udaharana:\",\n",
    "    layout=widgets.Layout(width=\"100%\", height=\"80px\"),\n",
    ")\n",
    "\n",
    "exercise3_feedback = Output()\n",
    "\n",
    "def check_exercise3(change):\n",
    "    with exercise3_feedback:\n",
    "        exercise3_feedback.clear_output()\n",
    "        answer = exercise3_udaharana.value.lower()\n",
    "        \n",
    "        # Check for universal rule pattern\n",
    "        has_universal = any(word in answer for word in [\"wherever\", \"whenever\", \"all\", \"any\", \"every\"])\n",
    "        has_example = any(word in answer for word in [\"eagle\", \"sparrow\", \"bird\", \"example\", \"instance\"])\n",
    "        \n",
    "        if has_universal and has_example:\n",
    "            display(HTML(\"<p style='color: green;'>✓ Good! Your Udaharana includes both a universal rule and a concrete example. Example: 'Wherever there is a bird with wings, it can fly. For example, an eagle has wings and can fly.'</p>\"))\n",
    "        elif has_universal:\n",
    "            display(HTML(\"<p style='color: orange;'>⚠ You have a universal rule, but try to include a concrete example too (e.g., 'like an eagle').</p>\"))\n",
    "        else:\n",
    "            display(HTML(\"<p style='color: red;'>✗ Try to include both a universal rule (using words like 'wherever', 'whenever', 'all') and a concrete example.</p>\"))\n",
    "\n",
    "exercise3_udaharana.observe(check_exercise3, names='value')\n",
    "\n",
    "display(VBox([exercise3_question, exercise3_udaharana, exercise3_feedback]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Try Your Own Problem"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form"
   },
   "source": [
    "#@title Try your own problem (double-click to show code)\n",
    "problem_input = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder=\"Enter your logical problem here...\",\n",
    "    description=\"Problem:\",\n",
    "    layout=widgets.Layout(width=\"100%\", height=\"150px\"),\n",
    ")\n",
    "\n",
    "custom_generate_button = widgets.Button(\n",
    "    description=\"Generate Nyaya Reasoning\",\n",
    "    button_style=\"success\",\n",
    "    icon=\"rocket\",\n",
    ")\n",
    "\n",
    "custom_output_area = Output()\n",
    "\n",
    "def generate_custom(button):\n",
    "    \"\"\"Generate reasoning for custom problem using tuned model (fallback: base).\"\"\"\n",
    "    if config is None:\n",
    "        with custom_output_area:\n",
    "            print(\"\\u26a0 Please configure backend and stage first\")\n",
    "        return\n",
    "\n",
    "    problem = problem_input.value.strip()\n",
    "    if not problem:\n",
    "        with custom_output_area:\n",
    "            print(\"\\u26a0 Please enter a problem\")\n",
    "        return\n",
    "\n",
    "    with custom_output_area:\n",
    "        custom_output_area.clear_output()\n",
    "\n",
    "        backend = config.get(\"tuned_backend\") or config.get(\"base_backend\")\n",
    "        if backend is None:\n",
    "            print(\"\\u26a0 No model backend available. Please apply configuration first.\")\n",
    "            return\n",
    "\n",
    "        if config.get(\"tuned_backend\"):\n",
    "            model_label = config.get(\"tuned_label\", config[\"stage_config\"].tuned_model_id)\n",
    "            model_role = \"Tuned\"\n",
    "        else:\n",
    "            model_label = config.get(\"base_label\", config[\"stage_config\"].base_model_id)\n",
    "            model_role = \"Base (tuned model not available)\"\n",
    "\n",
    "        print(f\"Generating reasoning with {model_role} model: {model_label}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        user_prompt = build_user_prompt(problem)\n",
    "        active_system_prompt = config.get(\"system_prompt\", config[\"stage_config\"].system_prompt)\n",
    "\n",
    "        try:\n",
    "            output = backend.generate(\n",
    "                user_prompt,\n",
    "                system_prompt=active_system_prompt,\n",
    "                max_new_tokens=max_tokens_slider.value if 'max_tokens_slider' in globals() else 2048,\n",
    "                temperature=temperature_slider.value if 'temperature_slider' in globals() else 0.5,\n",
    "                top_p=top_p_slider.value if 'top_p_slider' in globals() else 0.75,\n",
    "                top_k=top_k_slider.value if 'top_k_slider' in globals() else 5,\n",
    "            )\n",
    "            display(Markdown(f\"## Generated Reasoning ({model_role}: {model_label})\\n\\n{output}\"))\n",
    "        except Exception as e:\n",
    "            display(HTML(f\"<p style='color: red;'>Error: {e}</p>\"))\n",
    "\n",
    "custom_generate_button.on_click(generate_custom)\n",
    "\n",
    "display(VBox([\n",
    "    widgets.HTML(\"<h3>Try Your Own Problem</h3>\"),\n",
    "    problem_input,\n",
    "    custom_generate_button,\n",
    "    custom_output_area,\n",
    "]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **Structured Reasoning**: The 6-phase Navya-Nyaya methodology\n",
    "2. **Model Comparison**: Side-by-side comparison of base vs tuned models\n",
    "3. **Interactive Learning**: Exercises to understand Nyaya concepts\n",
    "4. **Custom Problems**: Generate reasoning for your own logical problems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}