% Bibliography for Pramana paper
% Comprehensive reference list organized by category

% ============================================================================
% 1. NAVYA-NYAYA PRIMARY AND COMPUTATIONAL WORKS
% ============================================================================

@book{nyaya-sutras,
  title={Nyāya Sūtras},
  author={Gautama, Akṣapāda},
  year={c. 500 BCE},
  note={Foundational text of Nyaya philosophy}
}

@book{tattvacintamani,
  title={Tattvacintāmaṇi},
  author={Gaṅgeśa, Upādhyāya},
  year={1325},
  publisher={Various editions},
  note={Foundational text of Navya-Nyaya logic}
}

@book{matilal1985logic,
  title={Logic, language, and reality: Indian philosophy and contemporary issues},
  author={Matilal, Bimal Krishna},
  year={1985},
  publisher={Motilal Banarsidass},
  address={Delhi}
}

@incollection{kulkarni2018later,
  title={Later Nyāya Logic: Computational Aspects},
  author={Kulkarni, Amba},
  booktitle={Handbook of Logical Thought in India},
  editor={Ganeri, Jonardon},
  publisher={Springer},
  year={2018},
  pages={1--30},
  doi={10.1007/978-81-322-1812-8_12-1}
}

@article{burton2020diagrams,
  title={Diagrams for Navya-Nyāya},
  author={Burton, Jim},
  journal={Journal of Indian Philosophy},
  volume={48},
  number={5},
  pages={747--801},
  year={2020},
  doi={10.1007/s10781-020-09419-0}
}

@article{sarma1994survey,
  title={A survey of Indian logic from the point of view of computer science},
  author={Sarma, V. V. S.},
  journal={Sadhana},
  volume={19},
  number={6},
  pages={971--983},
  year={1994},
  publisher={Indian Academy of Sciences}
}

@article{ganeri2001philosophy,
  title={Philosophy in classical India: The proper work of reason},
  author={Ganeri, Jonardon},
  journal={Philosophy East and West},
  year={2001},
  publisher={Routledge}
}

@inproceedings{indian-logic-ai,
  title={Indian Logic and AI System Design},
  author={Ganeri, Jonardon},
  booktitle={Proceedings of the International Conference on Knowledge Engineering},
  year={2000},
  note={Available at academia.edu}
}

% ============================================================================
% 2. LLM REASONING AND CHAIN-OF-THOUGHT
% ============================================================================

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc V. and Zhou, Denny},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022},
  url={https://proceedings.neurips.cc/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract.html}
}

@inproceedings{wang2023understanding,
  title={Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters},
  author={Wang, Boshi and Min, Sewon and Deng, Xiang and Shen, Jiaming and Wu, You and Zettlemoyer, Luke and Sun, Huan},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics},
  pages={2717--2739},
  year={2023},
  doi={10.18653/v1/2023.acl-long.153}
}

@article{deepseek-r1-2025,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={DeepSeek-AI},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025},
  url={https://arxiv.org/abs/2501.12948}
}

@article{reasonflux-prm-2025,
  title={ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs},
  author={Zhang, Yuxiang and others},
  journal={arXiv preprint arXiv:2506.18896},
  year={2025},
  url={https://arxiv.org/abs/2506.18896}
}

@article{flow-dpo-2024,
  title={Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning},
  author={Deng, Yihe and Mineiro, Paul},
  journal={arXiv preprint arXiv:2412.16145},
  year={2024},
  url={https://arxiv.org/abs/2412.16145}
}

@article{lightman2023prm,
  title={Let's Verify Step by Step},
  author={Lightman, Hunter and Cobbe, Vineet and Schulman, John},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023},
  url={https://arxiv.org/abs/2305.20050}
}

@article{grpo-2024,
  title={Group Relative Policy Optimization},
  author={DeepSeek-AI},
  journal={arXiv preprint arXiv:2505.22257},
  year={2024},
  note={Used in DeepSeek-R1 training}
}

@inproceedings{logicbench-2024,
  title={LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models},
  author={Mihir3009 and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics},
  year={2024},
  url={https://aclanthology.org/2024.acl-long.739/}
}

@article{prontoqa-2023,
  title={ProntoQA: Proof and Ontology-Generated Question-Answering},
  author={Saparov, Abulhair and others},
  journal={arXiv preprint arXiv:2306.14077},
  year={2023},
  url={https://arxiv.org/abs/2306.14077}
}

@inproceedings{ruletaker-2020,
  title={Transformers as Soft Reasoners over Language},
  author={Clark, Peter and Tafjord, Oyvind and Richardson, Kyle},
  booktitle={Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
  pages={3882--3890},
  year={2020},
  doi={10.24963/ijcai.2020/538}
}

% ============================================================================
% 3. HALLUCINATION AND VERIFICATION
% ============================================================================

@article{halluclean-2025,
  title={HalluClean: A Unified Framework for Detecting and Correcting Hallucinations in Large Language Models},
  author={Zhang, Yuxiang and others},
  journal={arXiv preprint arXiv:2511.08916},
  year={2025},
  url={https://arxiv.org/abs/2511.08916}
}

@inproceedings{cove-2024,
  title={Chain-of-Verification Reduces Hallucination in Large Language Models},
  author={Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2024},
  pages={2693--2708},
  year={2024},
  doi={10.18653/v1/2024.findings-acl.212}
}

@article{apple-gsm-symbolic-2024,
  title={GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models},
  author={Apple Machine Learning Research},
  journal={arXiv preprint arXiv:2410.05229},
  year={2024},
  month={October},
  url={https://arxiv.org/abs/2410.05229}
}

@article{cognitive-foundations-2025,
  title={Cognitive Foundations for Reasoning and Their Manifestation in LLMs},
  author={Various},
  journal={arXiv preprint arXiv:2511.16660},
  year={2025},
  url={https://arxiv.org/abs/2511.16660}
}

@article{illusion-thinking-2024,
  title={The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity},
  author={Apple Machine Learning Research},
  journal={Apple Machine Learning Research},
  year={2024},
  month={October},
  url={https://machinelearning.apple.com/research/illusion-of-thinking}
}

% ============================================================================
% 4. NEURO-SYMBOLIC AI
% ============================================================================

@article{garcez2019neural,
  title={Neural-symbolic computing: An effective methodology for principled integration of machine learning and reasoning},
  author={Garcez, Artur d'Avila and Gori, Marco and Lamb, Luis C. and Serafini, Luciano and Spranger, Michael and Tran, Son N.},
  journal={Journal of Applied Logics},
  volume={6},
  number={4},
  pages={611--632},
  year={2019}
}

@article{proofnet-plus-2025,
  title={ProofNet++: A Neuro-Symbolic System for Formal Proof Verification with Self-Correction},
  author={Fedin, Oleg and others},
  journal={arXiv preprint arXiv:2505.24230},
  year={2025},
  url={https://arxiv.org/abs/2505.24230}
}

@article{verge-2024,
  title={VERGE: Verification-Guided Reasoning for Large Language Models},
  author={Various},
  journal={arXiv preprint arXiv:2601.20055},
  year={2024},
  url={https://arxiv.org/abs/2601.20055}
}

@article{proof-of-thought-2024,
  title={Proof of Thought: Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning},
  author={Various},
  journal={arXiv preprint arXiv:2409.17270},
  year={2024},
  url={https://arxiv.org/abs/2409.17270}
}

@article{improving-rule-based-2025,
  title={Improving Rule-based Reasoning in LLMs via Neurosymbolic Representations},
  author={Various},
  journal={arXiv preprint arXiv:2502.01657},
  year={2025},
  url={https://arxiv.org/abs/2502.01657}
}

@article{vericot-2025,
  title={VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks},
  author={Various},
  journal={arXiv preprint arXiv:2511.04662},
  year={2025},
  url={https://arxiv.org/abs/2511.04662}
}

% ============================================================================
% 5. FINE-TUNING FRAMEWORKS
% ============================================================================

@inproceedings{qlora-2023,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023},
  url={https://arxiv.org/abs/2305.14314}
}

@misc{unsloth-2024,
  title={Unsloth: Fast and Memory-Efficient Fine-Tuning of Large Language Models},
  author={Han, Daniel and Han, Michael},
  year={2024},
  url={https://docs.unsloth.ai/},
  note={Open-source library for efficient LLM fine-tuning}
}

@article{lora-2021,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021},
  url={https://arxiv.org/abs/2106.09685}
}

% ============================================================================
% 6. SMT SOLVERS AND FORMAL VERIFICATION
% ============================================================================

@inproceedings{z3-2008,
  title={Z3: An Efficient SMT Solver},
  author={de Moura, Leonardo and Bj{\o}rner, Nikolaj},
  booktitle={Tools and Algorithms for the Construction and Analysis of Systems},
  pages={337--340},
  year={2008},
  publisher={Springer},
  doi={10.1007/978-3-540-78800-3_24}
}

@misc{z3-manual,
  title={The Z3 Theorem Prover},
  author={de Moura, Leonardo and Bj{\o}rner, Nikolaj},
  year={2024},
  url={https://github.com/Z3Prover/z3},
  note={Microsoft Research}
}

% ============================================================================
% 7. ADDITIONAL RELEVANT WORKS
% ============================================================================

@article{supercorrect-2024,
  title={SuperCorrect: Advancing Small LLM Reasoning with Thought Template Distillation and Self-Correction},
  author={Various},
  journal={OpenReview},
  year={2024},
  url={https://openreview.net/forum?id=PyjZO7oSw2}
}

@article{gsm8k-2021,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021},
  url={https://arxiv.org/abs/2110.14168}
}

@article{openai-reasoning-2024,
  title={Learning to Reason with LLMs},
  author={OpenAI},
  journal={OpenAI Research},
  year={2024},
  url={https://openai.com/index/learning-to-reason-with-llms/}
}

@article{ancient-indian-logic-case-based,
  title={Ancient Indian Logic as a Theory of Case-Based Reasoning},
  author={Ganeri, Jonardon},
  journal={ResearchGate},
  year={2012},
  url={https://www.researchgate.net/publication/251381865}
}

@inproceedings{logical-fallacy-2024,
  title={Large Language Models are Few-Shot Training Example Generators: A Case Study in Fallacy Recognition},
  author={Various},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2024},
  year={2024},
  url={https://aclanthology.org/2024.findings-acl.732/}
}
