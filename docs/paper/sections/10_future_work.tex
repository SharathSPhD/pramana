\section{Future Work}
\label{sec:future}

This section outlines planned improvements across three time horizons: near-term synthetic scaling (Stage 2), medium-term reinforcement learning enhancement (Stage 3), and long-term vision for domain expansion and frontier integration. Each stage builds on demonstrated capabilities while addressing identified limitations.

\subsection{Near-Term: Synthetic Scaling (Stage 2)}
\label{subsec:stage2}

The immediate priority is scaling the training dataset from 55 manually-created examples to 200-500 high-quality synthetic examples while maintaining rigorous quality control. Stage 1 demonstrated that models can achieve 100\% semantic correctness even with partial format adherence, validating the core Nyaya methodology. However, format adherence remains at 40\%, indicating that structural discipline requires stronger reinforcement.

\subsubsection{Synthetic Data Generation Pipeline}

We plan to generate synthetic examples using frontier models (GPT-4o, Claude) following seed patterns from Stage 0 and Stage 1. The generation process will employ a three-tier quality control system:

\textbf{Tier 1: Automated Structural Filters} (applied to 100\% of generated examples) perform fast validation checks including: valid YAML frontmatter, presence of all six phases in correct order, completeness of Pramana components (all four types), five-member syllogism structure validation, and Z3 SMT solver verification for formal logic problems where applicable. Examples failing any structural check are immediately rejected, targeting a 70-80\% pass rate at this tier.

\textbf{Tier 2: LLM-as-Judge Quality Scoring} evaluates all Tier 1 passes using GPT-4 with explicit Nyaya rubrics. Each example receives scores (0-10 scale) for: Samshaya appropriateness, Pratyaksha validity (only observables), Anumana correctness, Upamana relevance, Shabda correctness, Pancha Avayava quality (universal rules in Udaharana), Tarka meaningfulness, Hetvabhasa thoroughness, and Nirnaya definitiveness. Examples scoring $\geq$0.85 are auto-accepted, 0.70-0.84 require manual review, and $<$0.70 are rejected. Expected distribution: 40-60\% auto-accept, 20-30\% manual review, 10-20\% reject.

\textbf{Tier 3: Strategic Manual Review} focuses on boundary cases (scores 0.68-0.72 for calibration), high-scoring validation (ensuring deserved scores), specific phase failures, and problem type coverage. This targeted approach maximizes quality while minimizing manual effort to 15-20 hours for strategic sampling.

\subsubsection{Z3 Verification Integration}

For formal logic problems (constraint satisfaction, Boolean SAT), we will implement runtime verification using the Z3 SMT solver. The pipeline will: (1) parse Pratijna, Hetu, and Udaharana from model outputs, (2) autoformalize to Z3 SMT-LIB format, (3) execute Z3 to verify logical validity, and (4) inject error feedback for self-correction when verification fails. This neuro-symbolic integration provides ground truth validation for approximately 30\% of the dataset while enabling iterative improvement through automated error detection.

\subsubsection{Format Enforcement Strategies}

To address the 40\% format adherence rate observed in Stage 1, we will explore three complementary approaches:

\textbf{Rejection Sampling}: During training data generation, regenerate examples until format validation passes. This ensures the training distribution contains only structurally valid examples, teaching the model that format compliance is non-negotiable.

\textbf{Constrained Decoding}: Implement grammar-based generation using formal grammars that enforce Nyaya structure. This guides generation at inference time, preventing format violations before they occur.

\textbf{Format Reward in RL Fine-Tuning}: In Stage 3, incorporate structural adherence as an explicit reward component (30\% weight), incentivizing format compliance alongside semantic correctness.

\subsubsection{Expected Outcomes}

Stage 2 targets: 60-70\% format adherence (improvement from 40\%), maintained semantic correctness (preserving 100\% where achievable), and expanded problem diversity (200-500 examples across constraint satisfaction, Boolean SAT, and multi-step deduction). The three-tier quality control ensures synthetic data maintains gold-standard quality while enabling scalable dataset expansion.

\subsection{Medium-Term: Reinforcement Learning Enhancement (Stage 3)}
\label{subsec:stage3}

Building on Stage 2's expanded dataset, Stage 3 will implement Group Relative Policy Optimization (GRPO)~\cite{grpo-2024} with composite reward functions designed specifically for Nyaya-structured reasoning. This addresses the observation that supervised fine-tuning alone may not sufficiently enforce structural discipline, requiring reward-based optimization.

\subsubsection{GRPO Training Configuration}

GRPO will optimize a composite reward function with the following components:

\textbf{Format Reward} (30\% weight): Structural adherence to all six phases, correct phase ordering, and completeness of phase components. This directly addresses the format adherence gap identified in Stage 1.

\textbf{Validity Reward} (25\% weight): Combines Process Reward Model (PRM) scores with ground truth matching. The PRM will be trained on Nyaya-specific metrics, learning to evaluate reasoning quality beyond simple answer correctness. Trajectory-aware supervision captures the quality of intermediate reasoning steps, not just final conclusions.

\textbf{Consistency Reward} (20\% weight): Tarka counterfactual verification via Z3~\cite{z3-2008} for formal logic problems. This rewards models that generate logically consistent reasoning traces, where counterfactual tests genuinely verify conclusions rather than providing tautological checks.

\textbf{Pramana Appropriateness} (15\% weight): Correct application of knowledge sources—ensuring Pratyaksha contains only observables, Anumana represents genuine inferences, Upamana provides relevant analogies, and Shabda cites valid principles.

\textbf{Answer Correctness} (10\% weight): Final answer matches ground truth, weighted lower than reasoning quality to emphasize process over outcome.

\subsubsection{Process Reward Model}

Rather than using GPT-4 as a judge (which would cost \$5,000-10,000 for continuous RL scoring), we will train a specialized Process Reward Model on Stage 2 examples with quality scores. This PRM will be a small model (Llama 3.2 1B) trained to score Nyaya phases, providing calibrated rewards aligned with Nyaya methodology. One-time training cost on DGX Spark (4 hours) enables free inference during GRPO, making RL training cost-effective.

\subsubsection{Multi-Agent Debate Protocols}

Beyond single-model optimization, Stage 3 will explore multi-agent debate protocols inspired by Nyaya dialectical traditions:

\textbf{Vada (Cooperative Dialectic)}: Multiple agents refine reasoning collaboratively, with each agent specializing in different Nyaya phases. Agents exchange intermediate reasoning states, allowing cross-validation and error detection before final conclusions.

\textbf{Jalpa (Competitive Debate)}: Agents challenge each other's conclusions, forcing rigorous justification. The adversarial dynamic surfaces weaknesses in reasoning chains, improving robustness through stress-testing.

These protocols enable consensus formation and uncertainty quantification—when agents disagree, the model can explicitly state insufficient evidence (Nirnaya phase) rather than hallucinating confidence.

\subsubsection{Expected Outcomes}

Stage 3 targets: $\geq$90\% format adherence (substantial improvement from Stage 2's 60-70\%), robust reasoning across domains (maintaining high accuracy on LogicBench, ProntoQA, RuleTaker), and genuine epistemic improvements (not just performance gains, but interpretability and self-correction capability). The composite reward function ensures improvements are holistic, preventing reward hacking where models optimize single metrics at the expense of reasoning quality.

\subsection{Long-Term Vision}
\label{subsec:longterm}

Beyond formal logic domains, the long-term vision extends Nyaya-structured reasoning to broader problem types, cross-lingual applications, and hybrid architectures that balance interpretability with efficiency.

\subsubsection{Domain Expansion}

Current training focuses on constraint satisfaction, Boolean SAT, and multi-step deduction—domains with clear ground truth. Future work will explore:

\textbf{Causal Reasoning}: Applying Nyaya methodology to causal inference problems where correlation must be distinguished from causation (Savyabhichara fallacy detection becomes critical). The explicit Pramana separation helps models avoid conflating observed correlations with causal mechanisms.

\textbf{Legal Reasoning}: Using Shabda (testimony) for precedent-based reasoning, where authoritative legal principles serve as knowledge sources. The Pancha Avayava structure provides auditable argument chains suitable for legal justification.

\textbf{Medical Diagnosis}: Applying epistemic humility (Nirnaya phase) to distinguish definitive diagnoses from hypotheses requiring verification. The Tarka counterfactual testing helps rule out alternative diagnoses systematically.

\textbf{Open-Ended Questions}: Extending beyond problems with ground truth to questions where "insufficient evidence" is a valid conclusion. This tests the model's ability to express epistemic humility rather than hallucinating answers.

\subsubsection{Cross-Lingual Nyaya}

The Nyaya tradition originated in Sanskrit, and future work will integrate original terminology and extend to multilingual reasoning:

\textbf{Sanskrit Terminology Integration}: Incorporating original Nyaya terms (Vyapti, Drishtanta) alongside English translations, preserving cultural epistemology while maintaining accessibility.

\textbf{Multilingual Reasoning}: Training models that can reason in Hindi, Bengali, Tamil, and other Indian languages, ensuring the epistemological framework transfers across linguistic boundaries.

\textbf{Cultural Epistemology Preservation}: Ensuring that Western formal logic assumptions don't overwrite Nyaya's unique contributions—particularly the emphasis on concrete examples (Drishtanta) and universal rules (Vyapti) grounded in observation rather than abstract axioms.

\subsubsection{Hybrid Architecture}

The 3-6x token overhead of full Nyaya structure may be unnecessary for simple problems. Future work will explore adaptive architectures:

\textbf{Fast-Path for Simple Problems}: Standard inference for trivial cases (single-step deductions, direct lookups), bypassing full Nyaya structure to reduce latency and cost.

\textbf{Rigorous Path for Complex Reasoning}: Full 6-phase Nyaya for problems requiring justification, multi-step deduction, or high-stakes decisions. Dynamic routing based on problem complexity estimates.

\textbf{Adaptive Token Budgets}: Models that self-assess problem difficulty and allocate reasoning resources accordingly. Simple problems receive abbreviated Nyaya (2-3 phases), while complex problems receive full structure.

This hybrid approach balances interpretability benefits with practical efficiency, making Nyaya-structured reasoning viable for production deployment.

\subsubsection{Frontier Integration and Benchmarking}

To validate Nyaya methodology against state-of-the-art reasoning systems, future work will conduct comprehensive benchmarking:

\textbf{Standard Benchmarks}: Evaluate on LogicBench (multi-step deduction), ProntoQA (ontological reasoning), RuleTaker (rule-based reasoning), and GSM8K subset (mathematical word problems). Target: competitive accuracy with superior interpretability.

\textbf{Frontier Model Comparison}: Compare to o1-preview (internal reasoning), Claude extended thinking, and GPT-4 on reasoning tasks. DeepSeek-R1~\cite{deepseek-r1-2025} demonstrates that GRPO can improve reasoning quality, providing a baseline for comparison. The hypothesis: Nyaya-structured models achieve comparable accuracy while providing explicit reasoning traces that frontier models lack.

\textbf{Nyaya-Specific Benchmarks}: Develop custom evaluation suites for fallacy detection (Hetvabhasa), knowledge source validation (Pramana), and epistemic humility (Nirnaya). These benchmarks measure capabilities unique to Nyaya methodology, providing competitive moat beyond raw performance.

The goal is not merely matching frontier model performance, but demonstrating that explicit epistemological structure enables trustworthiness, auditability, and error detection that black-box reasoning cannot provide.
