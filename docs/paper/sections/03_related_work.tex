\section{Related Work}
\label{sec:related}

This work sits at the intersection of several research areas: computational formalization of Indian logic, LLM reasoning enhancement, hallucination mitigation, neuro-symbolic AI, and efficient fine-tuning. We review each area and position our contributions relative to existing approaches.

\subsection{Navya-Nyaya Logic and Computational Formalization}

Navya-Nyaya represents a sophisticated development of Indian logic that emphasizes concrete examples (\textit{dṛṣṭānta}) and universal rules (\textit{vyāpti}), distinguishing it from Western formal logic which separates logical validity from epistemic grounding~\cite{matilal1985logic}. The tradition originates from Gautama's Nyaya Sutras (c. 500 BCE) and reached its peak with Gangesa's Tattvacintamani (1325 CE), which systematized inference patterns into unambiguous terminology suitable for computational formalization~\cite{tattvacintamani}.

Modern computational work on Navya-Nyaya has focused on formalizing inference patterns and developing diagrammatic representations. Matilal~\cite{matilal1985logic} established the philosophical foundations for computational approaches, demonstrating how Nyaya's emphasis on concrete grounding makes it suitable for formal systems. Burton~\cite{burton2020diagrams} developed diagrammatic methods for representing Navya-Nyaya reasoning, showing how the framework's structured approach translates to computational representations. Sarma~\cite{sarma1994survey} surveyed Indian logic from a computer science perspective, identifying formalization opportunities and computational challenges.

Ganeri~\cite{ganeri2001philosophy,indian-logic-ai,ancient-indian-logic-case-based} has extensively explored the computational aspects of Indian logic, particularly case-based reasoning patterns and their application to AI system design. Kulkarni~\cite{kulkarni2018later} reviewed later Nyaya logic with explicit focus on computational aspects, identifying inference patterns that can be formalized algorithmically.

However, prior work has focused on symbolic formalization rather than neural learning. Our contribution is the first to demonstrate that Navya-Nyaya structures can be learned by language models through fine-tuning, producing systematic reasoning traces without requiring explicit symbolic representations.

\subsection{LLM Reasoning and Chain-of-Thought}

Chain-of-thought (CoT) prompting~\cite{wei2022chain} demonstrated that asking language models to generate step-by-step reasoning traces improves performance on complex reasoning tasks. However, CoT relies on implicit reasoning patterns learned during pre-training rather than enforcing explicit logical structures. Wang et al.~\cite{wang2023understanding} conducted empirical studies of what makes CoT effective, finding that structure and reasoning steps matter, but the approach remains fundamentally pattern-matching rather than systematic reasoning.

Recent work has attempted to improve reasoning through process supervision and verification. Lightman et al.~\cite{lightman2023prm} introduced process reward models (PRMs) that provide step-by-step verification, training models to prefer correct reasoning processes over just correct answers. ReasonFlux-PRM~\cite{reasonflux-prm-2025} extends this to trajectory-aware PRMs for long chain-of-thought reasoning, while Flow-DPO~\cite{flow-dpo-2024} uses multi-agent learning to improve mathematical reasoning.

DeepSeek-R1~\cite{deepseek-r1-2025} applies Group Relative Policy Optimization (GRPO)~\cite{grpo-2024} to incentivize reasoning capability, demonstrating that reinforcement learning can improve reasoning quality. However, these approaches still rely on opaque neural patterns rather than explicit epistemological structures.

Our approach differs by enforcing a formal 6-phase structure that requires explicit knowledge source identification, universal rule statements, and systematic verification. This provides interpretability advantages over black-box reasoning processes.

Evaluation benchmarks for logical reasoning include LogicBench~\cite{logicbench-2024} for multi-step deduction, ProntoQA~\cite{prontoqa-2023} for ontological reasoning, and RuleTaker~\cite{ruletaker-2020} for rule-based logical reasoning. Our evaluation uses constraint satisfaction and Boolean satisfiability problems, which test systematic reasoning without requiring domain-specific knowledge.

\subsection{Hallucination and Grounding}

LLM hallucination---the generation of confident falsehoods---represents a fundamental epistemic failure. Recent work has attempted to mitigate hallucinations through verification and grounding mechanisms. HalluClean~\cite{halluclean-2025} provides a unified framework for detecting and correcting hallucinations, using reasoning-enhanced approaches to identify and fix errors. Chain-of-Verification (CoVe)~\cite{cove-2024} reduces hallucination by having models generate verification questions and answer them before finalizing responses.

Apple Machine Learning Research's work on mathematical reasoning fragility~\cite{apple-gsm-symbolic-2024} demonstrates that adding irrelevant context causes up to 65\% performance degradation, revealing that apparent reasoning is often sophisticated pattern-matching. This finding motivates our focus on systematic reasoning frameworks that enforce explicit knowledge grounding.

Recent surveys on cognitive foundations~\cite{cognitive-foundations-2025} and the ``illusion of thinking''~\cite{illusion-thinking-2024} have explored the strengths and limitations of reasoning models, identifying problem complexity as a key factor in reasoning quality. Our approach addresses these limitations by enforcing explicit epistemological structures that prevent conflation of correlation with causation and require traceable justification for all claims.

Unlike verification-based approaches that check outputs after generation, our framework structures the reasoning process itself, requiring models to identify knowledge sources, construct explicit arguments, and verify conclusions before reaching final answers. This proactive approach prevents errors rather than detecting them post-hoc.

\subsection{Neuro-Symbolic AI and Formal Verification}

Neuro-symbolic AI combines neural networks with symbolic reasoning engines, aiming to leverage the strengths of both approaches~\cite{garcez2019neural}. Recent work has focused on integrating formal verification with LLM reasoning. ProofNet++~\cite{proofnet-plus-2025} provides a neuro-symbolic system for formal proof verification with self-correction, using theorem provers to verify model outputs. VERGE~\cite{verge-2024} uses verification-guided reasoning, decomposing claims and verifying them via SMT solvers.

Proof of Thought~\cite{proof-of-thought-2024} demonstrates that neurosymbolic program synthesis allows robust and interpretable reasoning, while VeriCoT~\cite{vericot-2025} validates chain-of-thought outputs via logical consistency checks. These approaches use external verifiers (typically Z3 SMT solver~\cite{z3-2008}) to check model outputs, providing guarantees for formal logic problems.

Our approach differs by structuring the reasoning process itself rather than verifying outputs post-hoc. The Nyaya framework requires explicit universal rules in syllogisms (Udaharana), which can be formalized to SMT-LIB format for Z3 verification, but the primary contribution is the epistemological structure that guides reasoning rather than external verification alone.

For formal logic problems (constraint satisfaction, Boolean SAT), we implement optional Z3 verification to validate logical consistency. However, the Nyaya structure provides value even without formal verification, by enforcing systematic reasoning patterns that prevent logical leaps and require explicit justification.

\subsection{Efficient Fine-Tuning and Reasoning Models}

Efficient fine-tuning enables training specialized models without full parameter updates. LoRA~\cite{lora-2021} introduced low-rank adaptation, allowing efficient fine-tuning with minimal parameter overhead. QLoRA~\cite{qlora-2023} extended this to quantized models, enabling 4-bit quantization with minimal accuracy loss. Unsloth~\cite{unsloth-2024} provides fast and memory-efficient fine-tuning, achieving 2x speedup and 40\% memory reduction through optimized implementations.

Our training pipeline uses Unsloth with QLoRA (4-bit quantization) and high LoRA rank (64-128) to target all attention and feedforward layers. This approach balances efficiency with capacity needed to learn complex reasoning paradigms. We train on relatively small datasets (20-55 examples) to prove the learnability hypothesis before scaling, demonstrating that structured reasoning can be learned with minimal data when format enforcement is strong.

DeepSeek-R1~\cite{deepseek-r1-2025} uses GRPO training methodology and distillation to create reasoning-capable models, demonstrating that reinforcement learning can improve reasoning quality. Our Stage 1 uses DeepSeek-R1-Distill-Llama-8B as the base model, leveraging its pre-trained reasoning traces while fine-tuning on Nyaya structure.

Recent work on structured thought organization includes models that use scratch/conclusion blocks to separate reasoning from final answers. Our Nyaya framework provides a more comprehensive structure, with six distinct phases that enforce epistemological rigor throughout the reasoning process, not just separation of reasoning from conclusions.

Our contribution demonstrates that efficient fine-tuning (QLoRA) can teach complex epistemological structures to language models, producing interpretable reasoning traces without requiring expensive full fine-tuning or reinforcement learning. This makes the approach accessible for community research and reproduction.