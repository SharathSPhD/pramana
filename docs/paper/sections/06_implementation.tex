\section{Implementation}
\label{sec:implementation}

This section documents the technical implementation of the Pramana reasoning engine, covering the technology stack, infrastructure setup, and code architecture that enables reproducible Nyaya-structured reasoning.

\subsection{Tech Stack}
\label{subsec:tech_stack}

The Pramana implementation leverages a carefully selected technology stack optimized for efficient fine-tuning, verification, and deployment. Table~\ref{tab:tech_stack} summarizes the core components.

\begin{table}[h]
\centering
\caption{Technology stack for the Pramana implementation.}
\label{tab:tech_stack}
\begin{tabular}{ll}
\toprule
Component & Technology \\
\midrule
Fine-tuning & Unsloth + TRL (SFTTrainer) \\
Quantization & QLoRA (4-bit, bitsandbytes) \\
PEFT & LoRA adapters \\
Verification & Z3 SMT Solver \\
LLM APIs & OpenAI, Anthropic (evaluation) \\
Experiment Tracking & Weights \& Biases, TensorBoard \\
CLI & Typer + Rich \\
Configuration & Pydantic + YAML inheritance \\
Containerization & Docker (NVIDIA PyTorch base) \\
Deployment & HuggingFace Hub, Ollama, Gradio Spaces \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Fine-tuning Framework:} Unsloth provides optimized implementations of FastLanguageModel and FastModel classes, enabling efficient QLoRA training with 4-bit quantization via bitsandbytes. The TRL library's SFTTrainer orchestrates the supervised fine-tuning loop with gradient accumulation, evaluation callbacks, and checkpoint management.

\textbf{Parameter-Efficient Fine-Tuning:} LoRA (Low-Rank Adaptation) adapters target all linear layers (query, key, value, output projections, and feed-forward networks) with rank 64 and alpha 64, providing sufficient capacity for learning the Nyaya reasoning paradigm while maintaining memory efficiency.

\textbf{Verification Infrastructure:} The Z3 SMT solver enables formal verification of logical constraints extracted from model outputs. For problems marked as \texttt{z3\_verifiable} in the dataset metadata, the evaluation pipeline extracts SMT-LIB constraints and verifies satisfiability.

\textbf{Experiment Tracking:} Weights \& Biases integration logs training metrics (loss, format adherence, phase counts) and sample generations during evaluation steps. TensorBoard provides complementary visualization for loss curves and training dynamics.

\textbf{CLI and Configuration:} The Typer framework provides a type-safe command-line interface with Rich terminal formatting for improved readability. Pydantic models enforce type safety for configuration loading, with YAML-based stage configurations supporting inheritance from a base configuration.

\subsection{Infrastructure Setup}
\label{subsec:infrastructure}

\textbf{Docker Environment:} The training environment uses NVIDIA's PyTorch container (\texttt{nvcr.io/nvidia/pytorch:25.11-py3}) as the base image, providing CUDA 12.x support and optimized PyTorch builds. The container includes the \texttt{uv} package manager for fast dependency installation and caching.

The Dockerfile installs system dependencies (git, curl, build tools) and sets up the Python environment with all ML dependencies. Volume mounts expose the source code, data directories, and model checkpoints for persistent storage across container restarts.

\textbf{Compute Platform:} Training runs execute on NVIDIA DGX Spark infrastructure with A100 GPUs (40GB or 80GB variants). The container runtime includes NVIDIA Container Toolkit for GPU passthrough, enabling direct CUDA access from within Docker.

\textbf{Memory and GPU Utilization:} QLoRA with 4-bit quantization reduces memory requirements significantly compared to full fine-tuning. Stage 0 (Llama 3.2-3B) trains comfortably on a single A100 40GB, while Stage 1 (DeepSeek-R1-Distill-Llama-8B) requires careful batch size and gradient accumulation tuning to fit within GPU memory limits.

\textbf{GGUF Conversion:} For local deployment via Ollama, merged models undergo GGUF conversion using \texttt{llama.cpp}. The conversion process transforms HuggingFace safetensors format to GGUF, followed by quantization to Q4\_K\_M format for efficient CPU inference. Modelfile templates configure system prompts and generation parameters for consistent Nyaya-structured outputs.

\textbf{Reproducibility Considerations:} All training scripts accept environment variables for hyperparameters (LoRA rank, learning rate, batch size, epochs), enabling exact reproduction of training runs. Random seeds are set for PyTorch, NumPy, and Python's random module. Checkpoint metadata includes git commit hashes, training configuration, and timestamp information for full traceability.

\subsection{Code Architecture}
\label{subsec:code_architecture}

The Pramana codebase follows a layered architecture with clear separation of concerns, enabling testability and extensibility. The structure organizes code into four primary layers: CLI, Application, Domain, and Infrastructure.

\textbf{Design Patterns:} The implementation employs several key design patterns:

\begin{itemize}
    \item \textbf{Template Method Pattern:} The \texttt{BaseTrainer} abstract class defines the training workflow skeleton (\texttt{setup}, \texttt{prepare\_data}, \texttt{train}, \texttt{cleanup}), with concrete implementations (\texttt{SupervisedFineTuningTrainer}) providing stage-specific behavior.
    
    \item \textbf{Chain of Responsibility:} The \texttt{EvaluationPipeline} chains evaluation handlers (Tier 1 structural validation, Tier 2 LLM judge, Tier 3 Z3 verification), stopping at the first failure tier and aggregating results.
    
    \item \textbf{Adapter Pattern:} Infrastructure adapters (\texttt{UnslothAdapter}, \texttt{Z3Verifier}, \texttt{OpenAILLMClient}, \texttt{AnthropicLLMClient}) wrap external libraries, providing clean interfaces to the application layer.
    
    \item \textbf{Repository Pattern:} The \texttt{CheckpointRepository} manages checkpoint persistence, metadata serialization, and HuggingFace Hub uploads, abstracting storage concerns from training logic.
\end{itemize}

\textbf{Module Structure:} The codebase organizes functionality into five primary modules:

\begin{itemize}
    \item \texttt{application/}: Contains training orchestration (\texttt{training/}), evaluation pipeline (\texttt{evaluation/}), and data processing (\texttt{data/}). This layer coordinates domain logic with infrastructure services.
    
    \item \texttt{cli/}: Implements command-line interface using Typer, with separate command modules for training, evaluation, validation, and data management.
    
    \item \texttt{config/}: Provides configuration loading with YAML inheritance (\texttt{loader.py}) and environment-based settings via Pydantic (\texttt{settings.py}).
    
    \item \texttt{domain/}: Contains core domain models (\texttt{models/nyaya\_example.py}), validators (\texttt{validators/structure.py}), and reward components (\texttt{rewards/}). This layer is infrastructure-agnostic and highly testable.
    
    \item \texttt{infrastructure/}: Wraps external dependencies including ML frameworks (\texttt{ml/unsloth\_adapter.py}), verification tools (\texttt{verification/z3\_verifier.py}), LLM APIs (\texttt{llm/client.py}), and storage (\texttt{storage/checkpoint\_repository.py}, \texttt{storage/hf\_uploader.py}).
\end{itemize}

\textbf{Key Classes and Responsibilities:}

The \texttt{MarkdownParser} (\texttt{application/data/parser.py}) transforms structured markdown files with YAML frontmatter into \texttt{NyayaExample} domain objects, extracting each of the six Nyaya phases through regex-based section parsing.

The \texttt{NyayaStructureValidator} (\texttt{domain/validators/structure.py}) enforces structural correctness: verifying all six phases are present, checking Pramana knowledge sources are valid, and ensuring syllogisms contain all five required members.

The \texttt{EvaluationPipeline} (\texttt{application/evaluation/pipeline.py}) orchestrates multi-tier evaluation, executing handlers sequentially and collecting tier-specific results (pass/fail, scores, error details).

The \texttt{SupervisedFineTuningTrainer} (\texttt{application/training/sft.py}) implements the training workflow: loading models via Unsloth, applying LoRA adapters, formatting data with Nyaya prompt templates, and executing training with TRL's SFTTrainer.

\textbf{Separation of Concerns:} The layered architecture ensures that domain logic (Nyaya structure validation, reward computation) remains independent of infrastructure choices (Unsloth vs. standard transformers, OpenAI vs. Anthropic APIs). This separation enables unit testing of domain logic without requiring GPU resources or external API calls, while integration tests verify infrastructure adapters function correctly.

\textbf{Testability:} The codebase includes comprehensive test suites organized by layer (\texttt{tests/unit/application/}, \texttt{tests/unit/domain/}, \texttt{tests/unit/infrastructure/}). Pytest markers (\texttt{@pytest.mark.slow}, \texttt{@pytest.mark.gpu}, \texttt{@pytest.mark.integration}) enable selective test execution during development. The infrastructure layer is excluded from coverage requirements, acknowledging that adapter tests require actual external dependencies.
