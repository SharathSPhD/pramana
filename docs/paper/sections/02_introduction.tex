\section{Introduction}

\subsection{The Epistemic Gap in LLMs}

Large language models have achieved remarkable success in pattern recognition and text generation, yet they fundamentally struggle with systematic reasoning. When presented with logical problems requiring structured, step-by-step analysis, LLMs often produce outputs that appear coherent but lack traceable justification. This limitation becomes particularly apparent when models encounter irrelevant context or ambiguous problems, leading to what we term the ``epistemic gap''---the inability to systematically distinguish valid knowledge from probabilistic pattern-matching.

Recent research by Apple Machine Learning Research demonstrates this fragility empirically~\cite{apple-gsm-symbolic-2024}. In their October 2024 study, adding irrelevant context to mathematical reasoning problems caused up to 65\% performance degradation, revealing that apparent ``reasoning'' is often sophisticated pattern-matching rather than genuine logical deduction. This finding aligns with broader concerns about LLM hallucination and unreliable reasoning, where models confidently produce falsehoods without internal mechanisms to verify claims or distinguish belief from knowledge.

The epistemic gap manifests in several ways: (1) models cannot trace the justification for their conclusions, (2) they conflate correlation with causation, (3) they lack mechanisms to detect and correct reasoning errors, and (4) they cannot distinguish definitive knowledge from reasonable hypotheses. These limitations prevent reliable deployment in high-stakes applications requiring systematic reasoning, such as medical diagnosis, legal argumentation, or safety-critical systems.

\subsection{Motivation: Why Epistemology Matters}

The distinction between belief and knowledge is fundamental to reliable reasoning systems. Knowledge requires not just true belief, but justified true belief---claims must be grounded in valid evidence sources and traceable through explicit reasoning chains. Current LLMs produce outputs without such traceable justification, making it impossible to audit reasoning processes or identify failure modes.

Unlike opaque neural patterns that emerge from training data, systematic reasoning frameworks provide interpretable structures where each step can be verified, challenged, and corrected. This interpretability is crucial for building trust in AI systems, especially as they are deployed in domains requiring accountability and transparency.

The need for systematic reasoning frameworks becomes clear when considering the limitations of current approaches. Chain-of-thought prompting~\cite{wei2022chain} improves performance but relies on implicit patterns learned during pre-training rather than enforcing explicit logical structures. Process reward models~\cite{lightman2023prm,reasonflux-prm-2025} provide step-by-step verification but do not enforce epistemological rigor in how knowledge is acquired and justified. What is needed is a framework that explicitly structures the reasoning process from evidence acquisition through conclusion, with built-in mechanisms for verification and error detection.

\subsection{Navya-Nyaya as Solution}

Navya-Nyaya, a 2,500-year-old formal epistemological framework from Indian philosophy, provides precisely such a structure. Developed from Gautama's Nyaya Sutras (c. 500 BCE) and refined by Gangesa's Tattvacintamani (1325 CE), Navya-Nyaya integrates logic with explicit knowledge sources (\textit{pramanas}), requiring grounding in concrete examples (\textit{dṛṣṭānta}) and universal rules (\textit{vyāpti}) rather than abstract symbolic manipulation~\cite{matilal1985logic,tattvacintamani}.

Unlike Western formal logic, which separates logical validity from epistemic grounding, Navya-Nyaya requires that all reasoning be traceable to valid knowledge sources: direct perception (\textit{pratyaksha}), inference (\textit{anumana}), comparison (\textit{upamana}), and testimony (\textit{shabda}). This integration of logic and epistemology makes Navya-Nyaya particularly suitable for computational formalization, as demonstrated by recent work on diagrammatic representations~\cite{burton2020diagrams} and computational aspects of Indian logic~\cite{sarma1994survey}.

The Nyaya framework enforces systematic reasoning through a structured 6-phase methodology: (1) \samshaya{} classifies the type of uncertainty requiring investigation, (2) \pramana{} identifies valid knowledge sources grounding all claims, (3) \pancha{} constructs formal arguments with explicit universal rules, (4) \tarka{} verifies conclusions via counterfactual testing, (5) \hetvabhasa{} detects reasoning fallacies, and (6) \nirnaya{} distinguishes definitive knowledge from hypotheses requiring verification. This structure provides cognitive scaffolding that prevents logical leaps and enforces epistemic humility.

\subsection{Research Hypothesis}

We hypothesize that fine-tuning LLMs on structured Nyaya methodology creates interpretable, verifiable reasoning superior to opaque chain-of-thought approaches. By teaching models to follow explicit epistemological structures, we can produce reasoning traces where each step is traceable, each claim is grounded in valid knowledge sources, and each conclusion is verified through systematic testing. This approach should yield reasoning comparable to frontier models like o1-preview or Claude extended thinking, but based on explicit methodology rather than opaque reinforcement learning.

Our hypothesis rests on three premises: (1) the Nyaya structure is computationally formalizable and learnable by neural architectures, (2) explicit epistemological scaffolding improves reasoning quality beyond pattern-matching, and (3) structured reasoning traces provide interpretability advantages over black-box approaches. We test this hypothesis through empirical evaluation across two training stages with different model sizes and datasets, measuring both format adherence (structural correctness) and semantic correctness (answer accuracy).

\subsection{Contributions}

This paper makes the following contributions:

\begin{itemize}
    \item \textbf{First LLM fine-tuned on explicit 6-phase Nyaya methodology}: We demonstrate that language models can learn systematic reasoning patterns from Navya-Nyaya epistemology, producing structured outputs with all six phases present and properly ordered.
    
    \item \textbf{Empirical analysis across two training stages}: We compare Stage 0 (Llama 3.2-3B, 20 examples) and Stage 1 (DeepSeek-R1-Distill-Llama-8B, 55 examples), showing how model size and dataset scale affect format adherence and semantic correctness. Both stages achieve 40\% format adherence (4/10 examples), indicating structural enforcement requires additional techniques. Stage 1 achieves 100\% semantic answer correctness (10/10 examples), demonstrating that models can internalize reasoning content even when strict schema compliance remains challenging.
    
    \item \textbf{Open-source release}: We publish models, datasets, and training infrastructure on Hugging Face for community research and reproduction. This includes fine-tuned models (\texttt{qbz506/nyaya-llama-3b-stage0-full}, \texttt{qbz506/nyaya-deepseek-8b-stage1-full}), training datasets (\texttt{qbz506/pramana-nyaya-stage0}, \texttt{qbz506/pramana-nyaya-stage1}), and an interactive demo Space (\texttt{qbz506/pramana-nyaya-demo}).
\end{itemize}

The remainder of this paper is organized as follows: Section~\ref{sec:related} reviews related work on reasoning, hallucination, and neuro-symbolic AI. Section~\ref{sec:nyaya} introduces the Nyaya framework in detail. Section~\ref{sec:methodology} describes our training methodology. Section~\ref{sec:implementation} details implementation specifics. Section~\ref{sec:results} presents evaluation results. Section~\ref{sec:discussion} discusses implications and limitations. Section~\ref{sec:open-source} describes open-source artifacts. Section~\ref{sec:future} outlines future work, and Section~\ref{sec:conclusion} concludes.