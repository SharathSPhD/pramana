\section{Open-Source Artifacts and Reproducibility}
\label{sec:open-source}

To enable reproducibility and community research, we release all components of the Pramana project under open-source licenses. This section documents the released models, datasets, codebase, and deployment artifacts.

\subsection{HuggingFace Models}
\label{subsec:hf_models}

We publish both LoRA adapter weights and full merged models for each training stage. Table~\ref{tab:hf_models} lists all released model artifacts.

\begin{table}[h]
\centering
\caption{Released models on HuggingFace Hub.}
\label{tab:hf_models}
\begin{tabular}{lll}
\toprule
Artifact & Repository & Description \\
\midrule
Stage 0 Adapter & \texttt{qbz506/nyaya-llama-3b-stage0} & LoRA adapter \\
Stage 0 Full & \texttt{qbz506/nyaya-llama-3b-stage0-full} & Merged + GGUF \\
Stage 1 Adapter & \texttt{qbz506/nyaya-deepseek-8b-stage1} & LoRA adapter \\
Stage 1 Full & \texttt{qbz506/nyaya-deepseek-8b-stage1-full} & Merged + GGUF \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Adapter Models:} LoRA adapters require loading alongside the base model. Stage 0 adapters target \texttt{unsloth/Llama-3.2-3B-Instruct}, while Stage 1 adapters target \texttt{unsloth/DeepSeek-R1-Distill-Llama-8B}. Adapter repositories include tokenizer configurations, chat templates, and adapter weights in safetensors format.

\textbf{Merged Models:} Full merged models combine base weights with LoRA adapters via \texttt{PeftModel.merge\_and\_unload()}, enabling standalone inference without base model loading. Merged repositories include safetensors weights, tokenizer files, and GGUF quantized versions (Q4\_K\_M) for Ollama deployment.

All model repositories include comprehensive model cards documenting training hyperparameters, evaluation metrics, usage instructions, and known limitations.

\subsection{Datasets}
\label{subsec:datasets}

Training and validation datasets are published on HuggingFace Hub in JSONL format. Table~\ref{tab:datasets} summarizes the released datasets.

\begin{table}[h]
\centering
\caption{Training and validation datasets.}
\label{tab:datasets}
\begin{tabular}{lll}
\toprule
Dataset & Repository & Examples \\
\midrule
Stage 0 & \texttt{qbz506/pramana-nyaya-stage0} & 20 \\
Stage 1 & \texttt{qbz506/pramana-nyaya-stage1} & 55 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Dataset Format:} Each dataset repository contains:
\begin{itemize}
    \item \texttt{train.jsonl}: Training examples in JSONL format with \texttt{instruction} (problem statement) and \texttt{output} (complete Nyaya reasoning trace) fields.
    \item \texttt{seed\_examples/}: Original markdown files with YAML frontmatter, preserving human-readable format and metadata.
    \item \texttt{README.md}: Dataset card documenting problem types, difficulty distribution, and usage guidelines.
\end{itemize}

Stage 0 includes 20 manually created seed examples across five problem types (constraint satisfaction, Boolean SAT, transitive reasoning, set membership, multi-step deduction). Stage 1 expands to 55 examples by combining Stage 0 seeds with 35 additional Stage 1 examples, including 5 negative examples designed to enforce structural quality.

\subsection{Demo Space}
\label{subsec:demo_space}

An interactive demonstration is available at \url{https://huggingface.co/spaces/qbz506/pramana-nyaya-demo}. The Gradio-based interface enables:

\begin{itemize}
    \item \textbf{Stage Selection:} Radio buttons switch between Stage 0 (Llama 3.2-3B) and Stage 1 (DeepSeek-R1-Distill-Llama-8B) models.
    \item \textbf{Base vs. Tuned Comparison:} Side-by-side outputs from base and fine-tuned models for the same prompt, enabling direct comparison of reasoning quality.
    \item \textbf{Example Dropdown:} Pre-populated examples from training datasets, allowing users to explore model behavior on known problems.
    \item \textbf{Custom Prompts:} Text input for testing arbitrary logical problems with Nyaya-structured reasoning.
\end{itemize}

The Space runs on CPU (free tier) or ZeroGPU (Pro tier), with runtime optimizations including model caching, reduced token limits for Stage 1 (256 tokens), and split GPU tasks to avoid memory limits when comparing base and tuned models simultaneously.

\subsection{Local Deployment}
\label{subsec:local_deployment}

\textbf{Ollama Integration:} Merged models converted to GGUF format (Q4\_K\_M quantization) can be imported into Ollama for local inference. Modelfile templates configure system prompts and generation parameters:

\begin{verbatim}
FROM /path/to/nyaya-model-q4.gguf
SYSTEM "You are a Nyaya reasoning engine. 
       Follow the exact output format provided."
PARAMETER temperature 0
PARAMETER top_p 1
PARAMETER num_ctx 4096
\end{verbatim}

\textbf{OpenWebUI Compatibility:} Models imported into Ollama are automatically available in OpenWebUI interfaces, enabling chat-based interaction with Nyaya-structured reasoning. The system prompt enforces format adherence, while temperature 0 ensures deterministic outputs for reproducibility.

\textbf{Modelfile Templates:} The repository includes Modelfile templates (\texttt{Modelfile}, \texttt{Modelfile.q4}) in model upload directories, documenting recommended parameters for Nyaya reasoning tasks. Users can customize these templates for different use cases (e.g., higher temperature for exploratory reasoning).

\subsection{Experiment Tracking}
\label{subsec:experiment_tracking}

\textbf{Weights \& Biases:} Training runs for Stage 0 and Stage 1 are logged to W\&B projects, enabling comparison of hyperparameters, loss curves, format adherence metrics, and sample generations across experiments. W\&B run links are included in model cards for full traceability.

\textbf{TensorBoard Logs:} Local TensorBoard logs are available in checkpoint directories, providing detailed loss curves, learning rate schedules, and evaluation metrics. Logs can be visualized with \texttt{tensorboard --logdir=models/stage\_*/checkpoint-*/logs}.

\textbf{Reproducibility:} All training scripts document exact hyperparameters, random seeds, and environment variables required for reproduction. Checkpoint metadata includes git commit hashes, ensuring code version traceability. Environment variable overrides enable exact reproduction: \texttt{LORA\_RANK=64 NUM\_TRAIN\_EPOCHS=10 python scripts/train\_stage1.py}.

\subsection{GitHub Repository}
\label{subsec:github_repo}

The complete codebase is available on GitHub under the MIT License, enabling academic and commercial use. The repository includes:

\begin{itemize}
    \item \textbf{Training Scripts:} Stage-specific training scripts (\texttt{scripts/train\_stage0.py}, \texttt{scripts/train\_stage1.py}) with comprehensive hyperparameter documentation.
    \item \textbf{Evaluation Tools:} Evaluation pipelines (\texttt{scripts/evaluate\_stage0.py}) and custom metrics computation for format adherence, semantic correctness, and Z3 verification.
    \item \textbf{Docker Setup:} Dockerfile and docker-compose.yml for reproducible containerized training environments.
    \item \textbf{Configuration Files:} YAML-based stage configurations (\texttt{configs/stage\_0.yaml}, \texttt{configs/stage\_1.yaml}) with inheritance from base configuration.
    \item \textbf{Documentation:} Comprehensive technical inventory (\texttt{docs/TECHNICAL\_INVENTORY.md}), stage reports, and architecture documentation.
\end{itemize}

\textbf{License:} The MIT License enables unrestricted use, modification, and distribution for both academic research and commercial applications. Contributors are welcome, with contribution guidelines documented in the repository.

\textbf{Reproducibility Checklist:} To reproduce training runs:
\begin{enumerate}
    \item Clone the repository and install dependencies via \texttt{uv sync --dev}.
    \item Set environment variables (\texttt{HF\_TOKEN}, \texttt{WANDB\_API\_KEY}) in \texttt{.env} file.
    \item Build Docker container: \texttt{docker compose build}.
    \item Run training: \texttt{docker compose run --rm pramana python scripts/train\_stage1.py}.
    \item Evaluate: \texttt{docker compose run --rm pramana python scripts/evaluate\_stage0.py}.
\end{enumerate}

All artifacts (models, datasets, code) are version-controlled and publicly accessible, ensuring full reproducibility of reported results.
