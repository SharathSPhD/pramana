% Abstract section
\begin{abstract}
Large language models (LLMs) demonstrate impressive pattern-matching capabilities but suffer from systematic reasoning fragility, particularly when irrelevant context is introduced. Recent research by Apple Machine Learning demonstrates that adding irrelevant context to mathematical reasoning problems causes up to 65\% performance degradation, revealing that apparent ``reasoning'' is often sophisticated pattern-matching rather than genuine logical deduction~\cite{apple-gsm-symbolic-2024}. This epistemic gap---the inability to distinguish valid knowledge from probabilistic associations---limits the reliability of LLMs in high-stakes applications requiring traceable justification.

We address this limitation by teaching LLMs a formal epistemological framework from Navya-Nyaya, a 2,500-year-old Indian logical tradition that integrates logic with explicit knowledge sources. Unlike standard chain-of-thought reasoning, our approach enforces a structured 6-phase methodology: \samshaya{} (doubt analysis), \pramana{} (evidence sources), \pancha{} (5-member syllogism), \tarka{} (counterfactual testing), \hetvabhasa{} (fallacy detection), and \nirnaya{} (ascertainment). We fine-tune language models on Nyaya-structured reasoning traces for constraint satisfaction and Boolean satisfiability problems.

Our empirical analysis across two training stages (Llama 3.2-3B and DeepSeek-R1-Distill-Llama-8B) demonstrates that models can learn systematic reasoning patterns while maintaining high semantic correctness. Stage 0 achieves 40\% format adherence (4/10 examples), while Stage 1 maintains 40\% format adherence but dramatically improves semantic correctness to 100\% (10/10 examples). This demonstrates that models internalize reasoning content even when strict structural compliance remains challenging, highlighting both the learnability of Nyaya methodology and areas for future format enforcement.

Our contributions include: (1) the first LLM fine-tuned on explicit 6-phase Nyaya methodology, (2) empirical analysis comparing two training stages with different model sizes and datasets, (3) open-source release of models, datasets, and training infrastructure on Hugging Face for community research and reproduction. This work demonstrates that ancient epistemological frameworks can structure modern neural reasoning, providing interpretable, verifiable reasoning traces superior to opaque chain-of-thought approaches.
\end{abstract}