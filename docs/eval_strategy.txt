Looking at your Pramana project's unique characteristics and the staged implementation plan, the answer is clearly **D) Staged benchmarking** - but with a critical insight about what "format-first" actually means for a Nyaya reasoning system.

## Why Staged Benchmarking is Essential

Your project isn't trying to beat GPT-4 on GSM8K in Stage Zero. You're validating a fundamentally different hypothesis: **can an LLM learn to reason through a systematic six-phase epistemological framework?** This requires evaluation metrics that evolve as your understanding of what's possible deepens through each stage.

## The Evaluation Strategy

### Stage Zero: Structural Validity (Format-First, But Meaningful)

**Primary Metric: Nyaya Phase Completeness**
- Does the model generate all six phases? (Binary: yes/no)
- Are phases in correct order? (Samshaya → Pramana → Pancha Avayava → Tarka → Hetvabhasa → Nirnaya)
- Are phase boundaries clearly marked?

**Secondary Metric: Phase Content Appropriateness**
This is where "format-first" gets sophisticated. You're not just checking for JSON keys. For each phase:

- **Samshaya**: Does it identify what type of doubt exists? (Not just "there is doubt" but "this is Samana Dharma Upapatti because...")
- **Pramana**: Are the four knowledge sources explicitly invoked with correct terminology? (Pratyaksha, Anumana, Upamana, Shabda)
- **Pancha Avayava**: Does it contain all five components - Pratijna, Hetu, Udaharana, Upanaya, Nigamana?
- **Tarka**: Is there actual reductio ad absurdum reasoning, not just filler text?
- **Hetvabhasa**: Are the five fallacy types explicitly checked?
- **Nirnaya**: Does it reach a definitive conclusion?

**Why accuracy is secondary here**: With only 5-10 examples, the model will massively overfit. If it gets 100% accuracy on constraint satisfaction puzzles, that's interesting but not the point. The critical question is: *when it makes mistakes, does it make them through the Nyaya structure or by abandoning it?*

**Success Criteria**: 80%+ structural completeness on held-out examples (even if answers are wrong). This proves the format is learnable.

### Stage One: Structure + Correctness Correlation

Now with 50 examples, you can meaningfully measure both:

**Format Adherence**: Should reach 90%+ (you've proven it's learnable, now optimize)

**Accuracy Tracking by Phase Quality**:
- Problems where all 6 phases present: __% correct
- Problems missing Tarka phase: __% correct  
- Problems with incomplete Pramana: __% correct
- Problems with detected Hetvabhasa: __% correct

This tells you whether the Nyaya structure is actually *helping* reasoning or just decorative overhead. If accuracy is independent of format adherence, you have a problem.

**Benchmark Introduction**: Now test on standard reasoning benchmarks:
- **ProntoQA** (logical inference): Can Nyaya methodology handle formal logical rules?
- **Simple RuleTaker examples** (rule-based reasoning): Does Pramana application work?

Don't expect to beat baselines yet. You're measuring whether the model can *apply* Nyaya structure to problems it hasn't seen training examples for.

### Stage Two: Custom Nyaya Scoring + Neuro-Symbolic Validation

Here's where you build the sophisticated evaluation that actually validates epistemic quality:

**Pramana Validity Scoring** (0-1 scale):
```python
def score_pramana_application(reasoning_trace):
    score = 0.0
    
    # Pratyaksha: Only observable facts from problem statement
    pratyaksha_facts = extract_pratyaksha(reasoning_trace)
    if all(fact in problem_statement for fact in pratyaksha_facts):
        score += 0.25
    
    # Anumana: Valid logical inferences
    inferences = extract_anumana(reasoning_trace)
    if all(is_valid_inference(inf) for inf in inferences):
        score += 0.25
    
    # Upamana: Appropriate analogies to similar solved cases
    analogies = extract_upamana(reasoning_trace)
    if analogies and are_relevant_analogies(analogies):
        score += 0.25
    
    # Shabda: Correct invocation of logical principles
    principles = extract_shabda(reasoning_trace)
    if principles and are_valid_principles(principles):
        score += 0.25
    
    return score
```

**Z3 Validation for Formal Logic Subset**:
For constraint satisfaction and Boolean SAT problems:
- Extract logical constraints from Hetu and Upanaya
- Convert to SMT-LIB format
- Run Z3 solver
- Compare Z3 solution to model's Nirnaya

**Metric**: Z3 Consistency Rate - percentage of problems where model's conclusion matches Z3's solution when both are expressible in first-order logic.

**Hetvabhasa Detection Accuracy**:
Create adversarial examples with intentional fallacies:
- Savyabhichara (inconsistent application)
- Viruddha (contradictory reasoning)
- Prakaranasama (irrelevant middle term)
- Sadhyasama (circular reasoning)
- Kalaatita (temporally invalid)

**Metric**: Fallacy Detection Precision/Recall - can the model identify which of the five fallacy types is present?

**Standard Benchmark Performance**:
Now you can meaningfully compare to baselines:
- LogicBench (multi-step deduction)
- Full RuleTaker (complex rule sets)
- GSM8K (mathematical reasoning - testing if Nyaya transfers to new domains)

### Stage Three (GRPO): Reward-Aligned Evaluation

Your GRPO reward function weights become your evaluation weights:
- 30% Structure completeness
- 25% Logical consistency (Z3)
- 20% Hetvabhasa detection
- 15% Pramana appropriateness
- 10% Answer correctness

**Key Insight**: If the model learns to maximize these rewards through RL, your evaluation metrics directly measure training objective alignment. No reward hacking should be possible if you've designed the rewards correctly.

**New Metric: Reasoning Efficiency**:
- Average tokens per correct solution
- Nyaya reasoning overhead vs. standard CoT
- Does structured reasoning actually reduce trial-and-error?

### Stage Four: Competitive Benchmarking

Only now do you seriously compare to frontier models:
- o1-preview on reasoning benchmarks
- DeepSeek-R1 on mathematical reasoning
- Claude 3.5 Sonnet on logical puzzles

**But you also introduce Nyaya-specific benchmarks that other models can't touch:**
- Nyaya Fallacy Detection Suite (your custom test set)
- Pramana Source Validation (which knowledge source was appropriately used?)
- Multi-step Inference with Explicit Warrants (can it show its work the Nyaya way?)

## Critical Implementation Details

### Format Validation Code (Stage Zero)

```python
def validate_nyaya_structure(output_json):
    """Binary validation: does output follow six-phase structure?"""
    required_phases = [
        "samshaya", "pramana", "pancha_avayava", 
        "tarka", "hetvabhasa", "nirnaya"
    ]
    
    results = {
        "has_all_phases": all(phase in output_json for phase in required_phases),
        "correct_order": check_phase_order(output_json),
        "phase_completeness": {}
    }
    
    # Check each phase has required components
    if "pramana" in output_json:
        pramana = output_json["pramana"]
        results["phase_completeness"]["pramana"] = {
            "has_pratyaksha": "pratyaksha" in pramana,
            "has_anumana": "anumana" in pramana,
            "has_upamana": "upamana" in pramana,
            "has_shabda": "shabda" in pramana
        }
    
    if "pancha_avayava" in output_json:
        avayava = output_json["pancha_avayava"]
        results["phase_completeness"]["pancha_avayava"] = {
            "has_pratijna": "pratijna" in avayava,
            "has_hetu": "hetu" in avayava,
            "has_udaharana": "udaharana" in avayava,
            "has_upanaya": "upanaya" in avayava,
            "has_nigamana": "nigamana" in avayava
        }
    
    if "hetvabhasa" in output_json:
        hetvabhasa = output_json["hetvabhasa"]
        results["phase_completeness"]["hetvabhasa"] = {
            "checks_performed": len(hetvabhasa.get("checks", [])),
            "fallacies_detected": hetvabhasa.get("detected", [])
        }
    
    return results
```

### Accuracy Tracking (Stage One)

```python
def evaluate_with_structure_correlation(model, test_set):
    """Track accuracy vs. structural quality"""
    results = []
    
    for example in test_set:
        output = model.generate(example["problem"])
        
        structure_score = validate_nyaya_structure(output)
        is_correct = (output["nirnaya"]["answer"] == example["ground_truth"])
        
        results.append({
            "problem_id": example["id"],
            "structural_completeness": structure_score["has_all_phases"],
            "accuracy": is_correct,
            "phase_quality": structure_score["phase_completeness"]
        })
    
    # Correlation analysis
    high_structure = [r for r in results if r["structural_completeness"]]
    low_structure = [r for r in results if not r["structural_completeness"]]
    
    return {
        "overall_accuracy": sum(r["accuracy"] for r in results) / len(results),
        "accuracy_with_full_structure": sum(r["accuracy"] for r in high_structure) / len(high_structure),
        "accuracy_without_full_structure": sum(r["accuracy"] for r in low_structure) / len(low_structure) if low_structure else 0,
        "structure_adherence_rate": len(high_structure) / len(results)
    }
```

## Why This Staged Approach is Critical

**Stage Zero would fail with accuracy-first evaluation**: With 5 examples, any accuracy measurement is just noise. But 80% structural adherence on held-out examples proves the hypothesis that LLMs can learn systematic reasoning frameworks.

**Stage One accuracy tracking validates the hypothesis**: If Nyaya structure correlates with better reasoning, you're onto something. If it doesn't, you need to revise how you're teaching the structure.

**Stage Two custom metrics are your competitive moat**: No other system can be evaluated on Pramana validity or Hetvabhasa detection. This is where you prove Nyaya methodology provides genuine epistemic advantages.

**Stage Three reward alignment proves you can optimize for it**: If GRPO can improve Nyaya-specific metrics while maintaining accuracy, you've shown the framework is trainable via RL.

**Stage Four competitive benchmarks prove generalization**: Can this approach actually compete with frontier models on standard tasks while providing better interpretability?

## The Spec Should Specify

For each stage, document:
1. **Primary success metric** (gate for next stage)
2. **Secondary tracking metrics** (inform iteration)
3. **Failure criteria** (when to pivot)
4. **Benchmark suite** (what tests to run)
5. **Evaluation frequency** (every N steps, every epoch, etc.)

The key philosophical point: **Format-first doesn't mean format-only**. It means validating that the structural foundation is solid before worrying about performance optimization. Nyaya Darshan itself teaches this - you establish valid Pramanas before reaching Nirnaya.

Your evaluation strategy should mirror your reasoning methodology.